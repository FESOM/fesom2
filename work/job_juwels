#!/bin/bash
#SBATCH --job-name=fes_core
#SBATCH --ntasks-per-node=48
#SBATCH --ntasks=288
#SBATCH --time=00:30:00
#SBATCH -o slurm-out.out
#SBATCH -e slurm-err.out
#SBATCH --partition=batch
#SBATCH -A hhb19

module --force purge
module use /gpfs/software/juwels/otherstages
module load Stages/2019a
module load StdEnv
module load Intel/2019.3.199-GCC-8.3.0  IntelMPI/2018.5.288 imkl/2019.3.199
# if you use ParaStation MPI
#module load Intel/2019.3.199-GCC-8.3.0 ParaStationMPI/5.4 imkl/2019.5.281
module load netCDF/4.6.3
module load netCDF-Fortran/4.4.5

set -x

ulimit -s unlimited

# determine JOBID
JOBID=`echo $SLURM_JOB_ID |cut -d"." -f1`

ln -s ../bin/fesom.x .
cp -n ../config/namelist.config  .
cp -n ../config/namelist.forcing .
cp -n ../config/namelist.oce     .
cp -n ../config/namelist.ice     .

date
srun --mpi=pmi2 ./fesom.x > "fesom2.0.out"
date

#qstat -f $PBS_JOBID
#export EXITSTATUS=$?
#if [ ${EXITSTATUS} -eq 0 ] || [ ${EXITSTATUS} -eq 127 ] ; then
#sbatch job_ollie
#fi
