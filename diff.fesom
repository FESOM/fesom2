diff --git a/src/MOD_DYN.F90 b/src/MOD_DYN.F90
index 76b62d06..3efc7ff3 100644
--- a/src/MOD_DYN.F90
+++ b/src/MOD_DYN.F90
@@ -1,7 +1,7 @@
 !==========================================================
 MODULE MOD_DYN
 USE O_PARAM
-USE, intrinsic :: ISO_FORTRAN_ENV, only : int32
+USE, intrinsic :: ISO_FORTRAN_ENV
 USE MOD_WRITE_BINARY_ARRAYS
 USE MOD_READ_BINARY_ARRAYS
 IMPLICIT NONE
@@ -27,8 +27,13 @@ TYPE T_SOLVERINFO
 !!!
     real(kind=WP), allocatable   :: rr(:), zz(:), pp(:), App(:)
     contains
+#if defined(__PGI)
+    private
+#endif            
     procedure WRITE_T_SOLVERINFO
     procedure READ_T_SOLVERINFO
+    generic :: write(unformatted) => WRITE_T_SOLVERINFO
+    generic :: read(unformatted)  => READ_T_SOLVERINFO
 END TYPE T_SOLVERINFO
 !
 !
@@ -36,30 +41,34 @@ END TYPE T_SOLVERINFO
 TYPE T_DYN_WORK
     real(kind=WP), allocatable, dimension(:,:,:) :: uvnode_rhs
     real(kind=WP), allocatable, dimension(:,:)   :: u_c, v_c
-
+    
     ! easy backscatter contribution
     real(kind=WP), allocatable, dimension(:,:)   :: u_b, v_b
     contains
+#if defined(__PGI)
+    private
+#endif            
     procedure WRITE_T_DYN_WORK
     procedure READ_T_DYN_WORK
+    generic :: write(unformatted) => WRITE_T_DYN_WORK
+    generic :: read(unformatted)  => READ_T_DYN_WORK
 END TYPE T_DYN_WORK
 !
 !
 !_______________________________________________________________________________
-! set main structure for dynamicss, contains viscosity options and parameters +
-! option for momentum advection
+! set main structure for dynamicss, contains viscosity options and parameters + 
+! option for momentum advection 
 TYPE T_DYN
 !___________________________________________________________________________
     ! instant zonal merdional velocity & Adams-Bashfort rhs
-    real(kind=WP), allocatable, dimension(:,:,:)   :: uv, uv_rhs, fer_uv
-    real(kind=WP), allocatable, dimension(:,:,:,:) :: uv_rhsAB
-    integer                                        :: AB_order=2
+    real(kind=WP), allocatable, dimension(:,:,:):: uv, uv_rhs, uv_rhsAB, fer_uv  
+
     ! horizontal velocities at nodes
     real(kind=WP), allocatable, dimension(:,:,:):: uvnode
-
+    
     ! instant vertical vel arrays
     real(kind=WP), allocatable, dimension(:,:)  :: w, w_e, w_i, w_old, cfl_z, fer_w
-
+    
     ! sea surface height arrays
     real(kind=WP), allocatable, dimension(:)    :: eta_n, d_eta, ssh_rhs, ssh_rhs_old
     
@@ -70,21 +79,20 @@ TYPE T_DYN
     !___________________________________________________________________________
     ! summarizes solver input parameter
     type(t_solverinfo)                          :: solverinfo
-
+    
     !___________________________________________________________________________
     ! put dynmiacs working arrays
     type(t_dyn_work)                            :: work
-
+    
     !___________________________________________________________________________
     ! opt_visc=...
     ! 5=Kinematic (easy) Backscatter
     ! 6=Biharmonic flow aware (viscosity depends on velocity Laplacian)
     ! 7=Biharmonic flow aware (viscosity depends on velocity differences)
     ! 8=Dynamic Backscatter
-    logical                                     :: check_opt_visc= .true.
-    integer                                     :: opt_visc      = 5
+    integer                                     :: opt_visc      = 5      
 
-    ! gamma0 [m/s],   backgroung viscosity= gamma0*len, it should be as small
+    ! gamma0 [m/s],   backgroung viscosity= gamma0*len, it should be as small 
     !                 as possible (keep it < 0.01 m/s).
     ! gamma1 [nodim], for computation of the flow aware viscosity
     ! gamma2 [s/m],   is only used in easy backscatter option
@@ -92,19 +100,19 @@ TYPE T_DYN
     real(kind=WP)                               :: visc_gamma1   = 0.1
     real(kind=WP)                               :: visc_gamma2   = 0.285
 
-    ! coefficient for returned sub-gridscale energy, to be used with opt_visc=5
+    ! coefficient for returned sub-gridscale energy, to be used with opt_visc=5 
     ! (easy backscatter)
-    real(kind=WP)                               :: visc_easybsreturn = 1.5
+    real(kind=WP)                               :: visc_easybsreturn = 1.5    
 
     logical                                     :: use_ivertvisc = .true.
     integer                                     :: momadv_opt    = 2
-
+    
     ! Switch on free slip
-    logical                                     :: use_freeslip  = .false.
-
+    logical                                     :: use_freeslip  = .false. 
+    
     ! do implicite, explicite spliting of vertical velocity
     logical                                     :: use_wsplit    = .false.
-    ! maximum allowed CFL criteria in vertical (0.5 < w_max_cfl < 1.)
+    ! maximum allowed CFL criteria in vertical (0.5 < w_max_cfl < 1.) 
     ! in older FESOM it used to be w_exp_max=1.e-3
     real(kind=WP)                               :: wsplit_maxcfl= 1.0
     ! energy diagnostic part: will be computed inside the model ("hard integration"):
@@ -113,24 +121,21 @@ TYPE T_DYN
     real(kind=WP), allocatable, dimension(:,:,:) :: ke_adv, ke_cor, ke_pre, ke_hvis, ke_vvis, ke_du2, ke_umean, ke_u2mean
     real(kind=WP), allocatable, dimension(:,:)   :: ke_wind, ke_drag
     ! same as above but multiplied by velocity. we need both for later computation of turbulent fluxes
-    real(kind=WP), allocatable, dimension(:,:,:)   :: ke_adv_xVEL, ke_cor_xVEL, ke_pre_xVEL, ke_hvis_xVEL, ke_vvis_xVEL
-    real(kind=WP), allocatable, dimension(:,:)     :: ke_wind_xVEL, ke_drag_xVEL
-    real(kind=WP), allocatable, dimension(:,:)     :: ke_wrho         !we use pressure to compute (W*dens) as it appeares much easier to compute (P*dW) instead of (dP*w)
-    real(kind=WP), allocatable, dimension(:,:)     :: ke_dW, ke_Pfull !for later computation of turbulent fluxes from the term above
-    real(kind=WP), allocatable, dimension(:,:,:,:) :: ke_adv_AB, ke_cor_AB
-    real(kind=WP), allocatable, dimension(:,:,:)   :: ke_rhs_bak
+    real(kind=WP), allocatable, dimension(:,:,:) :: ke_adv_xVEL, ke_cor_xVEL, ke_pre_xVEL, ke_hvis_xVEL, ke_vvis_xVEL
+    real(kind=WP), allocatable, dimension(:,:)   :: ke_wind_xVEL, ke_drag_xVEL
+    real(kind=WP), allocatable, dimension(:,:)   :: ke_wrho         !we use pressure to compute (W*dens) as it appeares much easier to compute (P*dW) instead of (dP*w)
+    real(kind=WP), allocatable, dimension(:,:)   :: ke_dW, ke_Pfull !for later computation of turbulent fluxes from the term above
+    real(kind=WP), allocatable, dimension(:,:,:) :: ke_adv_AB, ke_cor_AB
+    real(kind=WP), allocatable, dimension(:,:,:) :: ke_rhs_bak
     ! surface fields to compute APE generation
     real(kind=WP), allocatable, dimension(:)     :: ke_J, ke_D, ke_G, ke_D2, ke_n0, ke_JD, ke_GD, ke_swA, ke_swB
-
     !___________________________________________________________________________
     contains
 #if defined(__PGI)
-     procedure, private WRITE_T_DYN
-     procedure, private READ_T_DYN
-#else
+     private
+#endif            
      procedure WRITE_T_DYN
      procedure READ_T_DYN
-#endif
      generic :: write(unformatted) => WRITE_T_DYN
      generic :: read(unformatted)  => READ_T_DYN
 END TYPE T_DYN
@@ -141,12 +146,12 @@ contains
 !
 !_______________________________________________________________________________
 ! set unformatted writing and reading for T_DYN_WORK
-subroutine WRITE_T_SOLVERINFO(tsolverinfo, unit)
+subroutine WRITE_T_SOLVERINFO(tsolverinfo, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_SOLVERINFO),  intent(in)     :: tsolverinfo
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
     !___________________________________________________________________________
     write(unit, iostat=iostat, iomsg=iomsg) tsolverinfo%ident
     write(unit, iostat=iostat, iomsg=iomsg) tsolverinfo%maxiter
@@ -161,12 +166,12 @@ subroutine WRITE_T_SOLVERINFO(tsolverinfo, unit)
     call write_bin_array(tsolverinfo%App, unit, iostat, iomsg)
 end subroutine WRITE_T_SOLVERINFO
 
-subroutine READ_T_SOLVERINFO(tsolverinfo, unit)
+subroutine READ_T_SOLVERINFO(tsolverinfo, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_SOLVERINFO),  intent(inout)     :: tsolverinfo
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
     read(unit, iostat=iostat, iomsg=iomsg) tsolverinfo%ident
     read(unit, iostat=iostat, iomsg=iomsg) tsolverinfo%maxiter
     read(unit, iostat=iostat, iomsg=iomsg) tsolverinfo%restart
@@ -184,12 +189,12 @@ end subroutine READ_T_SOLVERINFO
 !
 !_______________________________________________________________________________
 ! set unformatted writing and reading for T_DYN_WORK
-subroutine WRITE_T_DYN_WORK(twork, unit)
+subroutine WRITE_T_DYN_WORK(twork, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_DYN_WORK), intent(in)        :: twork
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
     call write_bin_array(twork%uvnode_rhs, unit, iostat, iomsg)
     call write_bin_array(twork%u_c,        unit, iostat, iomsg)
     call write_bin_array(twork%v_c,        unit, iostat, iomsg)
@@ -197,12 +202,12 @@ subroutine WRITE_T_DYN_WORK(twork, unit)
     call write_bin_array(twork%v_b,        unit, iostat, iomsg)
 end subroutine WRITE_T_DYN_WORK
 
-subroutine READ_T_DYN_WORK(twork, unit)
+subroutine READ_T_DYN_WORK(twork, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_DYN_WORK), intent(inout)        :: twork
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
     call read_bin_array(twork%uvnode_rhs, unit, iostat, iomsg)
     call read_bin_array(twork%u_c,        unit, iostat, iomsg)
     call read_bin_array(twork%v_c,        unit, iostat, iomsg)
@@ -220,29 +225,29 @@ subroutine WRITE_T_DYN(dynamics, unit, iostat, iomsg)
     integer,              intent(in)     :: unit
     integer,              intent(out)    :: iostat
     character(*),         intent(inout)  :: iomsg
-
+    
     !___________________________________________________________________________
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%opt_visc
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_gamma0
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_gamma1
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_gamma2
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_easybsreturn
-
+    
     !___________________________________________________________________________
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%use_ivertvisc
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%momadv_opt
-
+    
     !___________________________________________________________________________
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%use_freeslip
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%use_wsplit
     write(unit, iostat=iostat, iomsg=iomsg) dynamics%wsplit_maxcfl
-
+    
     !___________________________________________________________________________
-    call dynamics%solverinfo%WRITE_T_SOLVERINFO(unit)
-
+    write(unit, iostat=iostat, iomsg=iomsg) dynamics%solverinfo
+    
     !___________________________________________________________________________
-    call dynamics%work%WRITE_T_DYN_WORK(unit)
-
+    write(unit, iostat=iostat, iomsg=iomsg) dynamics%work
+    
     !___________________________________________________________________________
     call write_bin_array(dynamics%uv        , unit, iostat, iomsg)
     call write_bin_array(dynamics%uv_rhs    , unit, iostat, iomsg)
@@ -255,9 +260,9 @@ subroutine WRITE_T_DYN(dynamics, unit, iostat, iomsg)
     if (Fer_GM) then
         call write_bin_array(dynamics%fer_w , unit, iostat, iomsg)
         call write_bin_array(dynamics%fer_uv, unit, iostat, iomsg)
-    end if
-
-
+    end if 
+    
+    
 end subroutine WRITE_T_DYN
 
 subroutine READ_T_DYN(dynamics, unit, iostat, iomsg)
@@ -266,29 +271,29 @@ subroutine READ_T_DYN(dynamics, unit, iostat, iomsg)
     integer,              intent(in)     :: unit
     integer,              intent(out)    :: iostat
     character(*),         intent(inout)  :: iomsg
-
+    
     !___________________________________________________________________________
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%opt_visc
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_gamma0
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_gamma1
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_gamma2
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%visc_easybsreturn
-
+    
     !___________________________________________________________________________
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%use_ivertvisc
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%momadv_opt
-
+    
     !___________________________________________________________________________
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%use_freeslip
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%use_wsplit
     read(unit, iostat=iostat, iomsg=iomsg) dynamics%wsplit_maxcfl
-
+    
     !___________________________________________________________________________
-    call dynamics%solverinfo%READ_T_SOLVERINFO(unit)
-
+    read(unit, iostat=iostat, iomsg=iomsg) dynamics%solverinfo
+    
     !___________________________________________________________________________
-    call dynamics%work%READ_T_DYN_WORK(unit)
-
+    read(unit, iostat=iostat, iomsg=iomsg) dynamics%work
+    
     !___________________________________________________________________________
     call read_bin_array(dynamics%uv        , unit, iostat, iomsg)
     call read_bin_array(dynamics%uv_rhs    , unit, iostat, iomsg)
@@ -302,7 +307,7 @@ subroutine READ_T_DYN(dynamics, unit, iostat, iomsg)
         call read_bin_array(dynamics%fer_w     , unit, iostat, iomsg)
         call read_bin_array(dynamics%fer_uv    , unit, iostat, iomsg)
     end if
-
+    
 end subroutine READ_T_DYN
 
 END MODULE MOD_DYN
diff --git a/src/MOD_ICE.F90 b/src/MOD_ICE.F90
index 5d0606d4..64c1bd1e 100644
--- a/src/MOD_ICE.F90
+++ b/src/MOD_ICE.F90
@@ -1,28 +1,33 @@
 MODULE MOD_ICE
 USE o_PARAM, only: WP
-USE, intrinsic :: ISO_FORTRAN_ENV, only : int32
+USE, intrinsic :: ISO_FORTRAN_ENV
 USE MOD_WRITE_BINARY_ARRAYS
 USE MOD_READ_BINARY_ARRAYS
 IMPLICIT NONE
 SAVE
 
 !
-!
+! 
 !_______________________________________________________________________________
-! set data array derived type for ice-tracers (area, mice, msnow) more tracer
+! set data array derived type for ice-tracers (area, mice, msnow) more tracer 
 ! are theretical possible
 TYPE T_ICE_DATA
     !___________________________________________________________________________
-    real(kind=WP), allocatable, dimension(:)    :: values, values_old, values_rhs, &
+    real(kind=WP), allocatable, dimension(:)    :: values, values_old, values_rhs, & 
                                                    values_div_rhs, dvalues, valuesl
     integer                                     :: ID
     !___________________________________________________________________________
     contains
+#if defined(__PGI)
+    private
+#endif            
         procedure WRITE_T_ICE_DATA
         procedure READ_T_ICE_DATA
+        generic :: write(unformatted) => WRITE_T_ICE_DATA
+        generic :: read(unformatted)  => READ_T_ICE_DATA
 END TYPE T_ICE_DATA
 !
-!
+! 
 !_______________________________________________________________________________
 ! set work array derived type for ice
 TYPE T_ICE_WORK
@@ -36,11 +41,16 @@ TYPE T_ICE_WORK
     real(kind=WP), allocatable, dimension(:)    :: ice_strength, inv_areamass, inv_mass
     !___________________________________________________________________________
     contains
+#if defined(__PGI)
+    private
+#endif            
         procedure WRITE_T_ICE_WORK
         procedure READ_T_ICE_WORK
+        generic :: write(unformatted) => WRITE_T_ICE_WORK
+        generic :: read(unformatted)  => READ_T_ICE_WORK
 END TYPE T_ICE_WORK
 !
-!
+! 
 !_______________________________________________________________________________
 ! set work array derived type for ice
 TYPE T_ICE_THERMO
@@ -52,26 +62,26 @@ TYPE T_ICE_THERMO
     real(kind=WP) :: rhofwt=1000., inv_rhofwt=1./1000.! Freshwter density & inverse
     real(kind=WP) :: rhoice=910. , inv_rhoice=1./910. ! Ice density & inverse, AOMIP
     real(kind=WP) :: rhosno=290. , inv_rhosno=1./290. ! Snow density & inverse, AOMIP
-    ! Specific heat of air, ice, snow [J/(kg * K)]
-    real(kind=WP) :: cpair=1005., cpice=2106., cpsno=2090.
+    ! Specific heat of air, ice, snow [J/(kg * K)] 
+    real(kind=WP) :: cpair=1005., cpice=2106., cpsno=2090. 
 !     real(kind=WP) :: cc=rhowat*4190.0  ! Volumetr. heat cap. of water [J/m**3/K](cc = rhowat*cp_water)
 !     real(kind=WP) :: cl=rhoice*3.34e5  ! Volumetr. latent heat of ice fusion [J/m**3](cl=rhoice*Lf)
 ! --> cl and cc are setted in subroutine ice_init(...)
     real(kind=WP) :: cc=1025.*4190.0  ! Volumetr. heat cap. of water [J/m**3/K](cc = rhowat*cp_water)
-    real(kind=WP) :: cl=910.*3.34e5  ! Volumetr. latent heat of ice fusion [J/m**3](cl=rhoice*Lf)
+    real(kind=WP) :: cl=910.*3.34e5  ! Volumetr. latent heat of ice fusion [J/m**3](cl=rhoice*Lf) 
     real(kind=WP) :: clhw=2.501e6      ! Specific latent heat [J/kg]: water	-> water vapor
     real(kind=WP) :: clhi=2.835e6      !                              sea ice-> water vapor
-    real(kind=WP) :: tmelt=273.15      ! 0 deg C expressed in K
+    real(kind=WP) :: tmelt=273.15      ! 0 deg C expressed in K 
     real(kind=WP) :: boltzmann=5.67E-8 ! S. Boltzmann const.*longw. emissivity
     integer       :: iclasses=7        ! Number of ice thickness gradations for ice growth calcs.
     real(kind=WP) :: hmin= 0.01        ! Cut-off ice thickness     !!
     real(kind=WP) :: Armin=0.01        ! Minimum ice concentration !!
-
+    
     ! --- namelist parameter /ice_therm/
     real(kind=WP) :: con= 2.1656, consn = 0.31 ! Thermal conductivities: ice & snow; W/m/K
     real(kind=WP) :: Sice = 4.0        ! Ice salinity 3.2--5.0 ppt.
     real(kind=WP) :: h0=1.0	           ! Lead closing parameter [m] ! 0.5
-    real(kind=WP) :: emiss_ice=0.97    ! Emissivity of Snow/Ice,
+    real(kind=WP) :: emiss_ice=0.97    ! Emissivity of Snow/Ice, 
     real(kind=WP) :: emiss_wat=0.97    ! Emissivity of open water
     real(kind=WP) :: albsn = 0.81      ! Albedo: frozen snow
     real(kind=WP) :: albsnm= 0.77      !         melting snow
@@ -79,11 +89,16 @@ TYPE T_ICE_THERMO
     real(kind=WP) :: albim = 0.68      !         melting ice
     real(kind=WP) :: albw  = 0.066     !         open water, LY2004
     contains
+#if defined(__PGI)
+    private
+#endif            
         procedure WRITE_T_ICE_THERMO
         procedure READ_T_ICE_THERMO
+        generic :: write(unformatted) => WRITE_T_ICE_THERMO
+        generic :: read(unformatted)  => READ_T_ICE_THERMO
 END TYPE T_ICE_THERMO
 !
-!
+! 
 !_______________________________________________________________________________
 ! set work array derived type for ice
 #if defined (__oasis) || defined (__ifsinterface)
@@ -94,18 +109,23 @@ TYPE T_ICE_ATMCOUPL
 #if defined (__oifs) || defined (__ifsinterface)
     !___________________________________________________________________________
     real(kind=WP), allocatable, dimension(:)    :: ice_alb, enthalpyoffuse
-    ! !!! DONT FORGET ice_temp rhs_tempdiv rhs_temp is advected for oifs !!! --> becomes additional ice
+    ! !!! DONT FORGET ice_temp rhs_tempdiv rhs_temp is advected for oifs !!! --> becomes additional ice 
     ! tracer in ice%data(4)%values
 #endif /* (__oifs)  */
     !___________________________________________________________________________
     contains
+#if defined(__PGI)
+    private
+#endif            
         procedure WRITE_T_ICE_ATMCOUPL
         procedure READ_T_ICE_ATMCOUPL
+        generic :: write(unformatted) => WRITE_T_ICE_ATMCOUPL
+        generic :: read(unformatted)  => READ_T_ICE_ATMCOUPL
 END TYPE T_ICE_ATMCOUPL
 #endif /* (__oasis) */
 
 !
-!
+! 
 !_______________________________________________________________________________
 ! set main ice derived type contains parameters, data array, work array, u_ice, vice
 TYPE T_ICE
@@ -118,18 +138,18 @@ TYPE T_ICE
     ! surface stess atm<-->ice, oce<-->ice
     real(kind=WP), allocatable, dimension(:)    :: stress_atmice_x, stress_iceoce_x
     real(kind=WP), allocatable, dimension(:)    :: stress_atmice_y, stress_iceoce_y
-
+    
     ! oce temp, salt, ssh, and uv at surface
     real(kind=WP), allocatable, dimension(:)    :: srfoce_temp, srfoce_salt, srfoce_ssh
 !     real(kind=WP), allocatable, dimension(:,:)  :: srfoce_uv
     real(kind=WP), allocatable, dimension(:)    :: srfoce_u, srfoce_v
-
+    
     ! freshwater & heatflux
     real(kind=WP), allocatable, dimension(:)    :: flx_fw, flx_h
-
+    
     ! maEVP variables
     real(kind=WP), allocatable, dimension(:)    :: alpha_evp_array, beta_evp_array
-
+    
     !___________________________________________________________________________
     ! total number of ice tracers (default=3, 1=area, 2=mice, 3=msnow, (4=ice_temp)
 #if defined (__oifs) || defined (__ifsinterface)
@@ -145,22 +165,23 @@ TYPE T_ICE
 #endif 
     !------------------------------
 #endif 
+    
 
     ! put ice tracers data arrays
     type(t_ice_data), allocatable, dimension(:) :: data
-
+    
     !___________________________________________________________________________
     ! put ice working arrays
     type(t_ice_work)                            :: work
-
+    
     ! put thermodynamics arrays
     type(t_ice_thermo)                          :: thermo
-
+    
 #if defined (__oasis) || defined (__ifsinterface)
     !___________________________________________________________________________
     ! put ice arrays for coupled model
     type(t_ice_atmcoupl)                        :: atmcoupl
-#endif /* (__oasis) */
+#endif /* (__oasis) */ 
 
     !___________________________________________________________________________
     ! set ice model parameters:
@@ -180,39 +201,37 @@ TYPE T_ICE
     real(kind=WP)             :: c_aevp=0.15               ! 0.1--0.2, but should be adjusted experimentally
     ! --- Ice forcing averaging ---
     integer                   :: ice_ave_steps=1           !ice step=ice_ave_steps*oce_step
-    real(kind=WP)             :: cd_oce_ice = 5.5e-3       ! drag coef. oce - ice
+    real(kind=WP)             :: cd_oce_ice = 5.5e-3       ! drag coef. oce - ice      
     logical                   :: ice_free_slip=.false.
     integer                   :: whichEVP=0                ! 0=standart; 1=mEVP; 2=aEVP
-
+    
     real(kind=WP)             :: ice_dt                    ! ice step=ice_ave_steps*oce_step
-    real(kind=WP)             :: Tevp_inv
-
+    real(kind=WP)             :: Tevp_inv   
+    
     integer                   :: ice_steps_since_upd=0
     logical                   :: ice_update = .true.
     !___________________________________________________________________________
     contains
 #if defined(__PGI)
-        procedure, private WRITE_T_ICE
-        procedure, private READ_T_ICE
-#else
+    private
+#endif            
         procedure WRITE_T_ICE
         procedure READ_T_ICE
-#endif
         generic :: write(unformatted) => WRITE_T_ICE
         generic :: read(unformatted)  => READ_T_ICE
 END TYPE T_ICE
 
-contains
+contains 
 !
 !
 !_______________________________________________________________________________
 ! Unformatted writing for T_ICE_DATA
-subroutine WRITE_T_ICE_DATA(tdata, unit)
+subroutine WRITE_T_ICE_DATA(tdata, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_DATA),      intent(in)     :: tdata
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call write_bin_array(tdata%values,         unit, iostat, iomsg)
     call write_bin_array(tdata%values_old,     unit, iostat, iomsg)
     call write_bin_array(tdata%values_rhs,     unit, iostat, iomsg)
@@ -220,15 +239,15 @@ subroutine WRITE_T_ICE_DATA(tdata, unit)
     call write_bin_array(tdata%dvalues,        unit, iostat, iomsg)
     call write_bin_array(tdata%valuesl,        unit, iostat, iomsg)
     write(unit, iostat=iostat, iomsg=iomsg) tdata%ID
-end subroutine WRITE_T_ICE_DATA
+end subroutine WRITE_T_ICE_DATA  
 
 ! Unformatted reading for T_ICE_DATA
-subroutine READ_T_ICE_DATA(tdata, unit)
+subroutine READ_T_ICE_DATA(tdata, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_DATA),      intent(inout)  :: tdata
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call read_bin_array(tdata%values,         unit, iostat, iomsg)
     call read_bin_array(tdata%values_old,     unit, iostat, iomsg)
     call read_bin_array(tdata%values_rhs,     unit, iostat, iomsg)
@@ -236,17 +255,17 @@ subroutine READ_T_ICE_DATA(tdata, unit)
     call read_bin_array(tdata%dvalues,        unit, iostat, iomsg)
     call read_bin_array(tdata%valuesl,        unit, iostat, iomsg)
     read(unit, iostat=iostat, iomsg=iomsg) tdata%ID
-end subroutine READ_T_ICE_DATA
+end subroutine READ_T_ICE_DATA                                   
 !
 !
 !_______________________________________________________________________________
 ! Unformatted writing for T_ICE_WORK
-subroutine WRITE_T_ICE_WORK(twork, unit)
+subroutine WRITE_T_ICE_WORK(twork, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_WORK),      intent(in)     :: twork
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call write_bin_array(twork%fct_tmax,     unit, iostat, iomsg)
     call write_bin_array(twork%fct_tmin,     unit, iostat, iomsg)
     call write_bin_array(twork%fct_plus,     unit, iostat, iomsg)
@@ -262,15 +281,15 @@ subroutine WRITE_T_ICE_WORK(twork, unit)
     call write_bin_array(twork%ice_strength, unit, iostat, iomsg)
     call write_bin_array(twork%inv_areamass, unit, iostat, iomsg)
     call write_bin_array(twork%inv_mass,     unit, iostat, iomsg)
-end subroutine WRITE_T_ICE_WORK
+end subroutine WRITE_T_ICE_WORK    
 
 ! Unformatted reading for T_ICE_WORK
-subroutine READ_T_ICE_WORK(twork, unit)
+subroutine READ_T_ICE_WORK(twork, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_WORK),      intent(inout)  :: twork
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call read_bin_array(twork%fct_tmax,     unit, iostat, iomsg)
     call read_bin_array(twork%fct_tmin,     unit, iostat, iomsg)
     call read_bin_array(twork%fct_plus,     unit, iostat, iomsg)
@@ -291,26 +310,26 @@ end subroutine READ_T_ICE_WORK
 !
 !_______________________________________________________________________________
 ! Unformatted writing for T_ICE_WORK
-subroutine WRITE_T_ICE_THERMO(ttherm, unit)
+subroutine WRITE_T_ICE_THERMO(ttherm, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_THERMO),    intent(in)     :: ttherm
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call write_bin_array(ttherm%t_skin,       unit, iostat, iomsg)
     call write_bin_array(ttherm%thdgr,        unit, iostat, iomsg)
     call write_bin_array(ttherm%thdgrsn,      unit, iostat, iomsg)
     call write_bin_array(ttherm%thdgr_old,    unit, iostat, iomsg)
     call write_bin_array(ttherm%ustar,        unit, iostat, iomsg)
-end subroutine WRITE_T_ICE_THERMO
+end subroutine WRITE_T_ICE_THERMO    
 
 ! Unformatted reading for T_ICE_WORK
-subroutine READ_T_ICE_THERMO(ttherm, unit)
+subroutine READ_T_ICE_THERMO(ttherm, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_THERMO),    intent(inout)  :: ttherm
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call read_bin_array(ttherm%t_skin,       unit, iostat, iomsg)
     call read_bin_array(ttherm%thdgr,        unit, iostat, iomsg)
     call read_bin_array(ttherm%thdgrsn,      unit, iostat, iomsg)
@@ -321,42 +340,42 @@ end subroutine READ_T_ICE_THERMO
 !
 !_______________________________________________________________________________
 ! Unformatted writing for T_ICE_ATMCOUPL
-#if defined (__oasis) || defined (__ifsinterface)
-subroutine WRITE_T_ICE_ATMCOUPL(tcoupl, unit)
+#if defined (__oasis) || defined (__ifsinterface)    
+subroutine WRITE_T_ICE_ATMCOUPL(tcoupl, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_ATMCOUPL),  intent(in)     :: tcoupl
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call write_bin_array(tcoupl%oce_flx_h,      unit, iostat, iomsg)
     call write_bin_array(tcoupl%ice_flx_h,      unit, iostat, iomsg)
     call write_bin_array(tcoupl%tmpoce_flx_h,   unit, iostat, iomsg)
     call write_bin_array(tcoupl%tmpice_flx_h,   unit, iostat, iomsg)
-#if defined (__oifs) || defined (__ifsinterface)
+#if defined (__oifs) || defined (__ifsinterface)  
     call write_bin_array(tcoupl%ice_alb,        unit, iostat, iomsg)
     call write_bin_array(tcoupl%enthalpyoffuse, unit, iostat, iomsg)
 #endif /* (__oifs) */
-end subroutine WRITE_T_ICE_ATMCOUPL
-#endif /* (__oasis) */
+end subroutine WRITE_T_ICE_ATMCOUPL  
+#endif /* (__oasis) */    
 
 ! Unformatted reading for T_ICE_ATMCOUPL
 #if defined (__oasis) || defined (__ifsinterface)
-subroutine READ_T_ICE_ATMCOUPL(tcoupl, unit)
+subroutine READ_T_ICE_ATMCOUPL(tcoupl, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_ICE_ATMCOUPL),  intent(inout)  :: tcoupl
     integer,                intent(in)     :: unit
-    integer                                :: iostat
-    character(len=1024)                    :: iomsg
+    integer,                intent(out)    :: iostat
+    character(*),           intent(inout)  :: iomsg
     call read_bin_array(tcoupl%oce_flx_h, unit, iostat, iomsg)
     call read_bin_array(tcoupl%ice_flx_h, unit, iostat, iomsg)
     call read_bin_array(tcoupl%tmpoce_flx_h, unit, iostat, iomsg)
     call read_bin_array(tcoupl%tmpice_flx_h, unit, iostat, iomsg)
-#if defined (__oifs) || defined (__ifsinterface)
+#if defined (__oifs) || defined (__ifsinterface)  
     call read_bin_array(tcoupl%ice_alb, unit, iostat, iomsg)
     call read_bin_array(tcoupl%enthalpyoffuse, unit, iostat, iomsg)
 #endif /* (__oifs) */
 end subroutine READ_T_ICE_ATMCOUPL
-#endif /* (__oasis) */
+#endif /* (__oasis) */   
 !
 !
 !_______________________________________________________________________________
@@ -371,14 +390,14 @@ subroutine WRITE_T_ICE(ice, unit, iostat, iomsg)
     !___________________________________________________________________________
     write(unit, iostat=iostat, iomsg=iomsg) ice%num_itracers
     do i=1, ice%num_itracers
-       call ice%data(i)%WRITE_T_ICE_DATA(unit)
+       write(unit, iostat=iostat, iomsg=iomsg) ice%data(i)
     end do
     !___________________________________________________________________________
-    call ice%thermo%WRITE_T_ICE_THERMO(unit)
-    call ice%work%WRITE_T_ICE_WORK(unit)
+    write(unit, iostat=iostat, iomsg=iomsg) ice%thermo
+    write(unit, iostat=iostat, iomsg=iomsg) ice%work
 #if defined (__oasis) || defined (__ifsinterface)
-    call ice%atmcoupl%WRITE_T_ICE_ATMCOUPL(unit)
-#endif /* (__oasis) */
+    write(unit, iostat=iostat, iomsg=iomsg) ice%atmcoupl
+#endif /* (__oasis) */       
 
     !___________________________________________________________________________
     write(unit, iostat=iostat, iomsg=iomsg) ice%pstar
@@ -403,7 +422,7 @@ subroutine WRITE_T_ICE(ice, unit, iostat, iomsg)
     write(unit, iostat=iostat, iomsg=iomsg) ice%Tevp_inv
     write(unit, iostat=iostat, iomsg=iomsg) ice%ice_steps_since_upd
     write(unit, iostat=iostat, iomsg=iomsg) ice%ice_update
-
+    
     !___________________________________________________________________________
     call write_bin_array(ice%uice           , unit, iostat, iomsg)
     call write_bin_array(ice%uice_rhs       , unit, iostat, iomsg)
@@ -427,8 +446,8 @@ subroutine WRITE_T_ICE(ice, unit, iostat, iomsg)
     if (ice%whichEVP > 0) then
         call write_bin_array(ice%alpha_evp_array        , unit, iostat, iomsg)
         call write_bin_array(ice%beta_evp_array         , unit, iostat, iomsg)
-    end if
-
+    end if     
+    
 end subroutine WRITE_T_ICE
 
 ! Unformatted reading for T_ICE
@@ -439,19 +458,19 @@ subroutine READ_T_ICE(ice, unit, iostat, iomsg)
     integer,                intent(out)    :: iostat
     character(*),           intent(inout)  :: iomsg
     integer                                :: i
-
+    
     !___________________________________________________________________________
     read(unit, iostat=iostat, iomsg=iomsg) ice%num_itracers
     if (.not. allocated(ice%data)) allocate(ice%data(ice%num_itracers))
     do i=1, ice%num_itracers
-       call ice%data(i)%READ_T_ICE_DATA(unit)
+       read(unit, iostat=iostat, iomsg=iomsg) ice%data(i)
     end do
     !___________________________________________________________________________
-    call ice%thermo%READ_T_ICE_THERMO(unit)
-    call ice%work%READ_T_ICE_WORK(unit)
+    read(unit, iostat=iostat, iomsg=iomsg) ice%thermo
+    read(unit, iostat=iostat, iomsg=iomsg) ice%work
 #if defined (__oasis) || defined (__ifsinterface)
-    call ice%atmcoupl%READ_T_ICE_ATMCOUPL(unit)
-#endif /* (__oasis) */
+    read(unit, iostat=iostat, iomsg=iomsg) ice%atmcoupl
+#endif /* (__oasis) */       
 
     !___________________________________________________________________________
     read(unit, iostat=iostat, iomsg=iomsg) ice%pstar
@@ -476,7 +495,7 @@ subroutine READ_T_ICE(ice, unit, iostat, iomsg)
     read(unit, iostat=iostat, iomsg=iomsg) ice%Tevp_inv
     read(unit, iostat=iostat, iomsg=iomsg) ice%ice_steps_since_upd
     read(unit, iostat=iostat, iomsg=iomsg) ice%ice_update
-
+    
     !___________________________________________________________________________
     call read_bin_array(ice%uice            , unit, iostat, iomsg)
     call read_bin_array(ice%uice_rhs        , unit, iostat, iomsg)
@@ -500,14 +519,14 @@ subroutine READ_T_ICE(ice, unit, iostat, iomsg)
     if (ice%whichEVP > 0) then
         call read_bin_array(ice%alpha_evp_array     , unit, iostat, iomsg)
         call read_bin_array(ice%beta_evp_array      , unit, iostat, iomsg)
-    end if
-
+    end if     
+    
 end subroutine READ_T_ICE
 END MODULE MOD_ICE
 !
 !
 !_______________________________________________________________________________
-! interface to initialise derived type for sea ice
+! interface to initialise derived type for sea ice 
 module ice_init_interface
     interface
         subroutine ice_init(ice, partit, mesh)
@@ -524,7 +543,7 @@ end module
 !
 !
 !_______________________________________________________________________________
-! initialise derived type for sea ice
+! initialise derived type for sea ice 
 subroutine ice_init(ice, partit, mesh)
     USE MOD_ICE
     USE MOD_PARTIT
@@ -547,9 +566,9 @@ subroutine ice_init(ice, partit, mesh)
     namelist /ice_dyn/ whichEVP, Pstar, ellipse, c_pressure, delta_min, evp_rheol_steps, &
                        Cd_oce_ice, ice_gamma_fct, ice_diff, theta_io, ice_ave_steps, &
                        alpha_evp, beta_evp, c_aevp
-
+                       
     real(kind=WP)  :: Sice, h0, emiss_ice, emiss_wat, albsn, albsnm, albi, &
-                      albim, albw, con, consn
+                      albim, albw, con, consn                   
     namelist /ice_therm/ Sice, h0, emiss_ice, emiss_wat, albsn, albsnm, albi, &
                          albim, albw, con, consn
     !___________________________________________________________________________
@@ -557,7 +576,7 @@ subroutine ice_init(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h"   
 
     !___________________________________________________________________________
     ! open and read namelist.ice for I/O
@@ -572,10 +591,10 @@ subroutine ice_init(ice, partit, mesh)
     read(nm_unit, nml=ice_dyn  , iostat=iost)
     read(nm_unit, nml=ice_therm, iostat=iost)
     close(nm_unit)
-
+    
     !___________________________________________________________________________
-    ! set parameters in ice derived type from namelist.ice --> namelist /ice_dyn/
-    ice%whichEVP        = whichEVP
+    ! set parameters in ice derived type from namelist.ice --> namelist /ice_dyn/ 
+    ice%whichEVP        = whichEVP 
     ice%pstar           = Pstar
     ice%ellipse         = ellipse
     ice%c_pressure      = c_pressure
@@ -586,34 +605,34 @@ subroutine ice_init(ice, partit, mesh)
     ice%ice_diff        = ice_diff
     ice%theta_io        = theta_io
     ice%ice_ave_steps   = ice_ave_steps
-    ice%alpha_evp       = alpha_evp
+    ice%alpha_evp       = alpha_evp       
     ice%beta_evp        = beta_evp
     ice%c_aevp          = c_aevp
-
-    ! set parameters in ice derived type from namelist.ice --> namelist /ice_therm/
+    
+    ! set parameters in ice derived type from namelist.ice --> namelist /ice_therm/ 
     ice%thermo%con      = con
     ice%thermo%consn    = consn
     ice%thermo%Sice     = Sice
     ice%thermo%h0       = h0
-    ice%thermo%emiss_ice= emiss_ice
+    ice%thermo%emiss_ice= emiss_ice    
     ice%thermo%emiss_wat= emiss_wat
-    ice%thermo%albsn    = albsn
+    ice%thermo%albsn    = albsn 
     ice%thermo%albsnm   = albsnm
-    ice%thermo%albi     = albi
+    ice%thermo%albi     = albi    
     ice%thermo%albim    = albim
     ice%thermo%albw     = albw
-
+    
     ice%thermo%cc=ice%thermo%rhowat*4190.0  ! Volumetr. heat cap. of water [J/m**3/K](cc = rhowat*cp_water)
     ice%thermo%cl=ice%thermo%rhoice*3.34e5  ! Volumetr. latent heat of ice fusion [J/m**3](cl=rhoice*Lf)
-
+    
     !___________________________________________________________________________
     ! define local vertice & elem array size
     elem_size=myDim_elem2D+eDim_elem2D
     node_size=myDim_nod2D +eDim_nod2D
-
+    
     !___________________________________________________________________________
     ! allocate/initialise arrays in ice derived type
-    ! initialise velocity and stress related arrays in ice derived type
+    ! initialise velocity and stress related arrays in ice derived type 
     allocate(ice%uice(                 node_size))
     allocate(ice%uice_rhs(             node_size))
     allocate(ice%uice_old(             node_size))
@@ -646,9 +665,9 @@ subroutine ice_init(ice, partit, mesh)
         ice%alpha_evp_array = ice%alpha_evp
         ice%beta_evp_array  = ice%alpha_evp
     end if
-
+    
     !___________________________________________________________________________
-    ! initialise surface ocean arrays in ice derived type
+    ! initialise surface ocean arrays in ice derived type 
     allocate(ice%srfoce_u(             node_size))
     allocate(ice%srfoce_v(             node_size))
     allocate(ice%srfoce_temp(          node_size))
@@ -659,15 +678,15 @@ subroutine ice_init(ice, partit, mesh)
     ice%srfoce_temp      = 0.0_WP
     ice%srfoce_salt      = 0.0_WP
     ice%srfoce_ssh       = 0.0_WP
-
+    
     allocate(ice%flx_fw(node_size))
     allocate(ice%flx_h( node_size))
     ice%flx_fw           = 0.0_WP
     ice%flx_h            = 0.0_WP
-
+    
     !___________________________________________________________________________
     ! initialse data array of ice derived type containing "ice tracer" that have
-    ! to be advected: a_ice (index=1), m_ice (index=2), m_snow (index=3),
+    ! to be advected: a_ice (index=1), m_ice (index=2), m_snow (index=3), 
     ! ice_temp (index=4, only when coupled)
     allocate(ice%data(ice%num_itracers))
     do n = 1, ice%num_itracers
@@ -686,9 +705,9 @@ subroutine ice_init(ice, partit, mesh)
         ice%data(n)%valuesl        = 0.0_WP
         if (n==4) ice%data(n)%values = 265.15_WP
     end do
-
+    
     !___________________________________________________________________________
-    ! initialse work array of ice derived type
+    ! initialse work array of ice derived type 
     allocate(ice%work%fct_tmax(        node_size))
     allocate(ice%work%fct_tmin(        node_size))
     allocate(ice%work%fct_plus(        node_size))
@@ -699,10 +718,10 @@ subroutine ice_init(ice, partit, mesh)
     ice%work%fct_plus    = 0.0_WP
     ice%work%fct_minus   = 0.0_WP
     ice%work%fct_fluxes  = 0.0_WP
-
+    
     allocate(ice%work%fct_massmatrix(sum(nn_num(1:myDim_nod2D))))
     ice%work%fct_massmatrix = 0.0_WP
-
+    
     allocate(ice%work%sigma11(         elem_size))
     allocate(ice%work%sigma12(         elem_size))
     allocate(ice%work%sigma22(         elem_size))
@@ -715,16 +734,16 @@ subroutine ice_init(ice, partit, mesh)
     ice%work%eps11       = 0.0_WP
     ice%work%eps12       = 0.0_WP
     ice%work%eps22       = 0.0_WP
-
+    
     allocate(ice%work%ice_strength(    elem_size))
     allocate(ice%work%inv_areamass(    node_size))
     allocate(ice%work%inv_mass(        node_size))
     ice%work%ice_strength= 0.0_WP
     ice%work%inv_areamass= 0.0_WP
     ice%work%inv_mass    = 0.0_WP
-
+    
     !___________________________________________________________________________
-    ! initialse thermo array of ice derived type
+    ! initialse thermo array of ice derived type 
     allocate(ice%thermo%ustar(         node_size))
     allocate(ice%thermo%t_skin(        node_size))
     allocate(ice%thermo%thdgr(         node_size))
@@ -735,10 +754,10 @@ subroutine ice_init(ice, partit, mesh)
     ice%thermo%thdgr     = 0.0_WP
     ice%thermo%thdgrsn   = 0.0_WP
     ice%thermo%thdgr_old = 0.0_WP
-
+    
     !___________________________________________________________________________
-    ! initialse coupling array of ice derived type
-#if defined (__oasis) || defined (__ifsinterface)
+    ! initialse coupling array of ice derived type 
+#if defined (__oasis) || defined (__ifsinterface)    
     allocate(ice%atmcoupl%oce_flx_h(     node_size))
     allocate(ice%atmcoupl%ice_flx_h(     node_size))
     allocate(ice%atmcoupl%tmpoce_flx_h(  node_size))
@@ -747,13 +766,13 @@ subroutine ice_init(ice, partit, mesh)
     ice%atmcoupl%ice_flx_h     = 0.0_WP
     ice%atmcoupl%tmpoce_flx_h  = 0.0_WP
     ice%atmcoupl%tmpice_flx_h  = 0.0_WP
-#if defined (__oifs) || defined (__ifsinterface)
+#if defined (__oifs) || defined (__ifsinterface)  
     allocate(ice%atmcoupl%ice_alb(       node_size))
     allocate(ice%atmcoupl%enthalpyoffuse(node_size))
     ice%atmcoupl%ice_alb       = 0.6_WP
     ice%atmcoupl%enthalpyoffuse= 0.0_WP
 #endif /* (__oifs) */
-#endif /* (__oasis) */
+#endif /* (__oasis) */       
 
     !___________________________________________________________________________
     ! --> took from oce_mesh.F90 --> subroutine mesh_auxiliary_arrays(partit, mesh)
@@ -779,7 +798,7 @@ end subroutine ice_init
 !
 !
 !_______________________________________________________________________________
-! initialise derived type for sea ice
+! initialise derived type for sea ice 
 subroutine ice_init_toyocean_dummy(ice, partit, mesh)
     USE MOD_ICE
     USE MOD_PARTIT
@@ -797,15 +816,15 @@ subroutine ice_init_toyocean_dummy(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h"   
 
     !___________________________________________________________________________
     ! define local vertice & elem array size
     node_size=myDim_nod2D+eDim_nod2D
-
+    
     !___________________________________________________________________________
     ! allocate/initialise arrays in ice derived type
-    ! initialise velocity and stress related arrays in ice derived type
+    ! initialise velocity and stress related arrays in ice derived type 
     allocate(ice%uice(                  node_size))
     allocate(ice%vice(                  node_size))
     ice%uice               = 0.0_WP
@@ -816,4 +835,4 @@ subroutine ice_init_toyocean_dummy(ice, partit, mesh)
         ice%data(n)%ID      = n
         ice%data(n)%values  = 0.0_WP
     end do
-end subroutine ice_init_toyocean_dummy
+end subroutine ice_init_toyocean_dummy    
diff --git a/src/MOD_MESH.F90 b/src/MOD_MESH.F90
index 04cff77f..8726f42d 100644
--- a/src/MOD_MESH.F90
+++ b/src/MOD_MESH.F90
@@ -3,12 +3,12 @@ MODULE MOD_MESH
 USE O_PARAM
 USE MOD_WRITE_BINARY_ARRAYS
 USE MOD_READ_BINARY_ARRAYS
-USE,     intrinsic    :: ISO_FORTRAN_ENV, only : int32
+USE,     intrinsic    :: ISO_FORTRAN_ENV
 IMPLICIT NONE
 SAVE
 integer, parameter    :: MAX_ADJACENT=32 ! Max allowed number of adjacent nodes
 
-TYPE SPARSE_MATRIX
+TYPE SPARSE_MATRIX 
      integer :: nza
      integer :: dim
      real(kind=WP),  allocatable,   dimension(:) :: values
@@ -21,77 +21,62 @@ END TYPE SPARSE_MATRIX
 
 TYPE T_MESH
 integer                                     :: nod2D           ! the number of 2D nodes
-real(kind=WP)                               :: ocean_area
-real(kind=WP)                               :: ocean_areawithcav
-real(kind=WP), allocatable, dimension(:,:)  :: coord_nod2D
-real(kind=WP), allocatable, dimension(:,:)  :: geo_coord_nod2D
+real(kind=WP)                               :: ocean_area, ocean_areawithcav
+real(kind=WP), allocatable, dimension(:,:)  :: coord_nod2D, geo_coord_nod2D
 integer                                     :: edge2D       ! the number of 2D edges
 integer                                     :: edge2D_in    ! the number of internal 2D edges
 integer                                     :: elem2D       ! the number of 2D elements
-integer, allocatable, dimension(:,:)        :: elem2D_nodes ! elem2D_nodes(:,n) lists; 3 nodes of element n
+integer, allocatable, dimension(:,:)        :: elem2D_nodes ! elem2D_nodes(:,n) lists; 3 nodes of element n   
 integer, allocatable, dimension(:,:)        :: edges        ! edge(:,n) lists 2 nodes; edge n
-integer, allocatable, dimension(:,:)        :: edge_tri     ! edge_tri(:,n) lists 2
-                                                            ! elements containing edge n: the first one is to left
+integer, allocatable, dimension(:,:)        :: edge_tri     ! edge_tri(:,n) lists 2 
+                                                            ! elements containing edge n: the first one is to left 
                                                             ! of the line directed to the second node
-integer,       allocatable, dimension(:,:)  :: elem_edges   ! elem_edges(:,n) are edges of element n.
+integer,       allocatable, dimension(:,:)  :: elem_edges   ! elem_edges(:,n) are edges of element n.  
 real(kind=WP), allocatable, dimension(:)    :: elem_area
-real(kind=WP), allocatable, dimension(:,:)  :: edge_dxdy
-real(kind=WP), allocatable, dimension(:,:)  :: edge_cross_dxdy
-real(kind=WP), allocatable, dimension(:)    :: elem_cos
-real(kind=WP), allocatable, dimension(:)    :: metric_factor
+real(kind=WP), allocatable, dimension(:,:)  :: edge_dxdy, edge_cross_dxdy
+real(kind=WP), allocatable, dimension(:)    :: elem_cos, metric_factor
 integer,       allocatable, dimension(:,:)  :: elem_neighbors
 integer,       allocatable, dimension(:,:)  :: nod_in_elem2D
-real(kind=WP), allocatable, dimension(:,:)  :: x_corners
-real(kind=WP), allocatable, dimension(:,:)  :: y_corners ! cornes for the scalar points
+real(kind=WP), allocatable, dimension(:,:)  :: x_corners, y_corners ! cornes for the scalar points
 integer,       allocatable, dimension(:)    :: nod_in_elem2D_num
-real(kind=WP), allocatable, dimension(:)    :: depth                ! depth(n) is the depths at node n
-real(kind=WP), allocatable, dimension(:,:)  :: gradient_vec
+real(kind=WP), allocatable, dimension(:)    :: depth                ! depth(n) is the depths at node n 
+real(kind=WP), allocatable, dimension(:,:)  :: gradient_vec 
                                                            ! coefficients of linear reconstruction
                                                            ! of velocities on elements
 real(kind=WP), allocatable, dimension(:,:)  :: gradient_sca ! Coefficients to compute gradient of scalars
                                                            ! on elements
 INTEGER,       ALLOCATABLE, DIMENSION(:)    :: bc_index_nod2D(:)
-                                                           ! vertical structure
+                                                           ! vertical structure     
 !
 !
-!___vertical mesh info__________________________________________________________
+!___vertical mesh info__________________________________________________________                                                           
 ! total number of layers
 integer                                     :: nl
 
 ! initial layer, mid-depth layer and element depth
-real(kind=WP), allocatable, dimension(:)    :: zbar
-real(kind=WP), allocatable, dimension(:)    :: Z
-real(kind=WP), allocatable, dimension(:)    :: elem_depth
+real(kind=WP), allocatable, dimension(:)    :: zbar, Z,elem_depth
 
-! upper boudnary index of all vertical vertice/element loops, default==1 but when
+! upper boudnary index of all vertical vertice/element loops, default==1 but when 
 ! cavity is used becomes index of cavity-ocean boundary at vertices and elements
-integer,       allocatable, dimension(:)    :: ulevels
-integer,       allocatable, dimension(:)    :: ulevels_nod2D
-integer,       allocatable, dimension(:)    :: ulevels_nod2D_max
+integer,       allocatable, dimension(:)    :: ulevels, ulevels_nod2D, ulevels_nod2D_max
 
 ! number of levels at elem and vertices considering bottom topography
-integer,       allocatable, dimension(:)    :: nlevels
-integer,       allocatable, dimension(:)    :: nlevels_nod2D
-integer,       allocatable, dimension(:)    :: nlevels_nod2D_min
+integer,       allocatable, dimension(:)    :: nlevels, nlevels_nod2D, nlevels_nod2D_min
 
 !
 !
 !___horizontal mesh info________________________________________________________
-real(kind=WP), allocatable, dimension(:,:)  :: area
-real(kind=WP), allocatable, dimension(:,:)  :: area_inv
-real(kind=WP), allocatable, dimension(:,:)  :: areasvol
-real(kind=WP), allocatable, dimension(:,:)  :: areasvol_inv
+real(kind=WP), allocatable, dimension(:,:)  :: area, area_inv, areasvol, areasvol_inv
 real(kind=WP), allocatable, dimension(:)    :: mesh_resolution
 
 !
 !
 !___cavity mesh info____________________________________________________________
 ! level index of cavity-ocean boundary at vertices and elements
-! --> see: ulevels, ulevels_nod2D (fvom_main)
+! --> see: ulevels, ulevels_nod2D (fvom_main) 
 
 ! vertice/element yes=1/no=0 flag if cavity exists
-integer,       allocatable, dimension(:)    :: cavity_flag_n
-integer,       allocatable, dimension(:)    :: cavity_flag_e
+integer,       allocatable, dimension(:)    :: cavity_flag_n, cavity_flag_e
 
 ! depth of cavity-ocean interface
 real(kind=WP), allocatable, dimension(:)    :: cavity_depth
@@ -99,16 +84,19 @@ real(kind=WP), allocatable, dimension(:)    :: cavity_depth
 
 real(kind=WP), allocatable, dimension(:,:)  :: cavity_nrst_cavlpnt_xyz
 
+!
+!
+!___coriolis force______________________________________________________________
+real(kind=WP), allocatable, dimension(:)    :: coriolis_node, coriolis
+
 !
 !
 !___Elevation stiffness matrix__________________________________________________
 type(sparse_matrix)                         :: ssh_stiff
 
 !#if defined (__oasis)
-real(kind=WP), allocatable, dimension(:)    :: lump2d_south
-real(kind=WP), allocatable, dimension(:)    :: lump2d_north
-integer,       allocatable, dimension(:)    :: ind_south
-integer,       allocatable, dimension(:)    :: ind_north
+real(kind=WP), allocatable, dimension(:)    :: lump2d_south, lump2d_north  
+integer,       allocatable, dimension(:)    :: ind_south, ind_north    
 !#endif
 
 integer                                       :: nn_size
@@ -118,33 +106,30 @@ integer, allocatable, dimension(:,:)          :: nn_pos
 !_______________________________________________________________________________
 ! Arrays added for ALE implementation:
 ! --> layer thinkness at node and depthlayer for t=n and t=n+1
+real(kind=WP), allocatable,dimension(:,:)   :: hnode, hnode_new, zbar_3d_n, Z_3d_n
 !------------------------------
-real(kind=WP), allocatable,dimension(:,:)   :: hnode
-real(kind=WP), allocatable,dimension(:,:)   :: hnode_new
-real(kind=WP), allocatable,dimension(:,:)   :: zbar_3d_n
-real(kind=WP), allocatable,dimension(:,:)   :: Z_3d_n
 ! LA 2023-01-31 add icebergs
-real(kind=WP), allocatable,dimension(:,:)   :: Z_3d_n_ib
+!#if defined(__async_icebergs)
+        real(kind=WP), allocatable,dimension(:,:)   :: Z_3d_n_ib
+!#endif 
 
 !------------------------------
 ! --> layer thinkness at elements, interpolated from hnode
 real(kind=WP), allocatable,dimension(:,:)   :: helem
 
 ! --> thinkness of bottom elem (important for partial cells)
-real(kind=WP), allocatable,dimension(:)     :: bottom_elem_thickness
-real(kind=WP), allocatable,dimension(:)     :: bottom_node_thickness
+real(kind=WP), allocatable,dimension(:)     :: bottom_elem_thickness 
+real(kind=WP), allocatable,dimension(:)     :: bottom_node_thickness 
 
 ! --> The increment of total fluid depth on elements. It is used to update the matrix
 real(kind=WP), allocatable,dimension(:)     :: dhe
 
 ! --> hbar, hbar_old: correspond to the elevation, but on semi-integer time steps.
-real(kind=WP), allocatable,dimension(:)     :: hbar
-real(kind=WP), allocatable,dimension(:)     :: hbar_old
+real(kind=WP), allocatable,dimension(:)     :: hbar, hbar_old
 
-! --> auxiliary array to store depth of layers and depth of mid level due to changing
+! --> auxiliary array to store depth of layers and depth of mid level due to changing 
 !     layer thinkness at every node
-!real(kind=WP), allocatable,dimension(:)     :: zbar_n
-!real(kind=WP), allocatable,dimension(:)     :: Z_n
+!real(kind=WP), allocatable,dimension(:)     :: zbar_n, Z_n
 
 ! new bottom depth at node and element due to partial cells
 real(kind=WP), allocatable,dimension(:)     :: zbar_n_bot
@@ -156,20 +141,13 @@ real(kind=WP), allocatable,dimension(:)     :: zbar_e_srf
 
 character(:), allocatable :: representative_checksum
 
-!
-!
-!___coriolis force______________________________________________________________
-real(kind=WP), allocatable, dimension(:)    :: coriolis
-real(kind=WP), allocatable, dimension(:)    :: coriolis_node
 
 contains
 #if defined(__PGI)
-  procedure, private write_t_mesh
-  procedure, private read_t_mesh
-#else
+  private
+#endif        
   procedure write_t_mesh
   procedure read_t_mesh
-#endif
   generic :: write(unformatted) => write_t_mesh
   generic :: read(unformatted)  => read_t_mesh
 END TYPE T_MESH
@@ -236,7 +214,7 @@ subroutine write_t_mesh(mesh, unit, iostat, iomsg)
 
     write(unit, iostat=iostat, iomsg=iomsg) mesh%ssh_stiff%dim
     write(unit, iostat=iostat, iomsg=iomsg) mesh%ssh_stiff%nza
-
+ 
     call write_bin_array(mesh%ssh_stiff%rowptr,     unit, iostat, iomsg)
     call write_bin_array(mesh%ssh_stiff%colind,     unit, iostat, iomsg)
     call write_bin_array(mesh%ssh_stiff%values,     unit, iostat, iomsg)
@@ -254,7 +232,10 @@ subroutine write_t_mesh(mesh, unit, iostat, iomsg)
     call write_bin_array(mesh%hnode_new,               unit, iostat, iomsg)
     call write_bin_array(mesh%zbar_3d_n,               unit, iostat, iomsg)
     call write_bin_array(mesh%Z_3d_n,                  unit, iostat, iomsg)
+! LA 2023-01-31 add icebergs
+!#if defined(__async_icebergs)
     call write_bin_array(mesh%Z_3d_n_ib,               unit, iostat, iomsg)
+!#endif 
     call write_bin_array(mesh%helem,                   unit, iostat, iomsg)
     call write_bin_array(mesh%bottom_elem_thickness,   unit, iostat, iomsg)
     call write_bin_array(mesh%bottom_node_thickness,   unit, iostat, iomsg)
@@ -280,7 +261,8 @@ subroutine read_t_mesh(mesh, unit, iostat, iomsg)
     integer,       intent(in)          :: unit
     integer,       intent(out)         :: iostat
     character(*),  intent(inout)       :: iomsg
-
+    integer                            :: i, j, k
+    integer                            :: s1, s2, s3
     ! write records (giving sizes for the allocation for arrays)
     read(unit, iostat=iostat, iomsg=iomsg) mesh%nod2D
     read(unit, iostat=iostat, iomsg=iomsg) mesh%ocean_area
@@ -351,7 +333,10 @@ subroutine read_t_mesh(mesh, unit, iostat, iomsg)
     call read_bin_array(mesh%hnode_new,               unit, iostat, iomsg)
     call read_bin_array(mesh%zbar_3d_n,               unit, iostat, iomsg)
     call read_bin_array(mesh%Z_3d_n,                  unit, iostat, iomsg)
+! LA 2023-01-31 add icebergs
+!#if defined(__async_icebergs)
     call read_bin_array(mesh%Z_3d_n_ib,               unit, iostat, iomsg)
+!#endif 
     call read_bin_array(mesh%helem,                   unit, iostat, iomsg)
     call read_bin_array(mesh%bottom_elem_thickness,   unit, iostat, iomsg)
     call read_bin_array(mesh%bottom_node_thickness,   unit, iostat, iomsg)
@@ -367,7 +352,8 @@ subroutine read_t_mesh(mesh, unit, iostat, iomsg)
 !   call read_bin_array(mesh%representative_checksum, unit, iostat, iomsg)
     call read_bin_array(mesh%coriolis,                unit, iostat, iomsg)
     call read_bin_array(mesh%coriolis_node,           unit, iostat, iomsg)
-
+    
 end subroutine read_t_mesh
 end module MOD_MESH
 !==========================================================
+
diff --git a/src/MOD_PARTIT.F90 b/src/MOD_PARTIT.F90
index 6603b892..d4f2a6a5 100644
--- a/src/MOD_PARTIT.F90
+++ b/src/MOD_PARTIT.F90
@@ -1,8 +1,8 @@
 !==========================================================
-! Variables to organize parallel work
+! Variables to organize parallel work  
 module MOD_PARTIT
 USE O_PARAM
-USE, intrinsic :: ISO_FORTRAN_ENV, only : int32
+USE, intrinsic :: ISO_FORTRAN_ENV
 USE MOD_WRITE_BINARY_ARRAYS
 USE MOD_READ_BINARY_ARRAYS
 #if defined(_OPENMP)
@@ -16,11 +16,11 @@ integer, parameter   :: MAX_NEIGHBOR_PARTITIONS=32
 
 
 type com_struct
-     integer                                       :: rPEnum ! the number of PE I receive info from
+     integer                                       :: rPEnum ! the number of PE I receive info from 
      integer, dimension(MAX_NEIGHBOR_PARTITIONS)   :: rPE    ! their list
      integer, dimension(MAX_NEIGHBOR_PARTITIONS+1) :: rptr   ! allocatables to the list of nodes
      integer, dimension(:), allocatable            :: rlist  ! the list of nodes
-     integer                                       :: sPEnum ! send part
+     integer                                       :: sPEnum ! send part 
      integer, dimension(MAX_NEIGHBOR_PARTITIONS)   :: sPE
      integer, dimension(MAX_NEIGHBOR_PARTITIONS)   :: sptr
      integer, dimension(:), allocatable            :: slist
@@ -28,11 +28,17 @@ type com_struct
      integer                                       :: nreq   ! number of requests for MPI_Wait
                                                              ! (to combine halo exchange of several fields)
      contains
-     procedure READ_T_COM_STRUCT
+#if defined(__PGI)
+     private
+#endif
      procedure WRITE_T_COM_STRUCT
+     procedure READ_T_COM_STRUCT
+     generic :: write(unformatted) => WRITE_T_COM_STRUCT
+     generic :: read(unformatted)  =>  READ_T_COM_STRUCT
 end type com_struct
 
 TYPE T_PARTIT
+  integer              :: MPI_COMM_FESOM ! FESOM communicator (for ocean only runs if often a copy of MPI_COMM_WORLD)
   
   !---------------------------------------------------
   !LA 2023-01-31 add asynchronous icebergs
@@ -43,51 +49,44 @@ TYPE T_PARTIT
   type(com_struct) :: com_nod2D
   type(com_struct) :: com_elem2D
   type(com_struct) :: com_elem2D_full
+ 
+  ! MPI Datatypes for interface exchange
+  ! Element fields (2D; 2D integer; 3D with nl-1 or nl levels, 1 - 4 values)
+  !                 small halo and / or full halo
+  !!! s(r)_mpitype_* are constructed during the runtime ans shall not be dumped!!!
+  integer, allocatable :: s_mpitype_elem2D(:,:),       r_mpitype_elem2D(:,:) 
+  integer, allocatable :: s_mpitype_elem2D_full_i(:),  r_mpitype_elem2D_full_i(:) 
+  integer, allocatable :: s_mpitype_elem2D_full(:,:),  r_mpitype_elem2D_full(:,:) 
+  integer, allocatable :: s_mpitype_elem3D(:,:,:),     r_mpitype_elem3D(:,:,:) 
+  integer, allocatable :: s_mpitype_elem3D_full(:,:,:),r_mpitype_elem3D_full(:,:,:) 
 
+  ! Nodal fields (2D; 2D integer; 3D with nl-1 or nl levels, one, two, or three values)
+  integer, allocatable       :: s_mpitype_nod2D(:),     r_mpitype_nod2D(:) 
+  integer, allocatable       :: s_mpitype_nod2D_i(:),   r_mpitype_nod2D_i(:)
+  integer, allocatable       :: s_mpitype_nod3D(:,:,:), r_mpitype_nod3D(:,:,:) 
+
+  ! general MPI part
+  integer            :: MPIERR
   !---------------------------------------------------
   !LA 2023-01-31 add asynchronous icebergs
   ! kh 11.02.21
   integer            :: MPIERR_IB
   !---------------------------------------------------
-  integer              :: npes
-  integer              :: mype
-  integer              :: maxPEnum=100
+  integer            :: npes
+  integer            :: mype
+  integer            :: maxPEnum=100
+!PS   logical            :: flag_debug=.false.
   integer, allocatable, dimension(:)  :: part
 
-  integer                             :: myDim_nod2D
-  integer                             :: eDim_nod2D
+  ! Mesh partition
+  integer                             :: myDim_nod2D, eDim_nod2D
   integer, allocatable, dimension(:)  :: myList_nod2D
-
-  integer                             :: myDim_elem2D, myDim_elem2D_shrinked
-  integer                             :: eDim_elem2D
-  integer                             :: eXDim_elem2D
-  integer, allocatable, dimension(:)  :: myList_elem2D, myInd_elem2D_shrinked
-
-  integer                             :: myDim_edge2D
-  integer                             :: eDim_edge2D
+  integer                             :: myDim_elem2D, eDim_elem2D, eXDim_elem2D
+  integer, allocatable, dimension(:)  :: myList_elem2D
+  integer                             :: myDim_edge2D, eDim_edge2D
   integer, allocatable, dimension(:)  :: myList_edge2D
-  integer :: pe_status = 0 ! if /=0 then something is wrong
-
-  integer              :: MPI_COMM_FESOM ! FESOM communicator (for ocean only runs if often a copy of MPI_COMM_WORLD)
-  integer              :: MPI_COMM_WORLD ! FESOM communicator (for ocean only runs if often a copy of MPI_COMM_WORLD)
 
-  ! MPI Datatypes for interface exchange
-  ! Element fields (2D; 2D integer; 3D with nl-1 or nl levels, 1 - 4 values)
-  !                 small halo and / or full halo
-  !!! s(r)_mpitype_* are constructed during the runtime ans shall not be dumped!!!
-  integer, allocatable :: s_mpitype_elem2D(:,:),       r_mpitype_elem2D(:,:)
-  integer, allocatable :: s_mpitype_elem2D_full_i(:),  r_mpitype_elem2D_full_i(:)
-  integer, allocatable :: s_mpitype_elem2D_full(:,:),  r_mpitype_elem2D_full(:,:)
-  integer, allocatable :: s_mpitype_elem3D(:,:,:),     r_mpitype_elem3D(:,:,:)
-  integer, allocatable :: s_mpitype_elem3D_full(:,:,:),r_mpitype_elem3D_full(:,:,:)
-
-  ! Nodal fields (2D; 2D integer; 3D with nl-1 or nl levels, one, two, or three values)
-  integer, allocatable       :: s_mpitype_nod2D(:),     r_mpitype_nod2D(:)
-  integer, allocatable       :: s_mpitype_nod2D_i(:),   r_mpitype_nod2D_i(:)
-  integer, allocatable       :: s_mpitype_nod3D(:,:,:), r_mpitype_nod3D(:,:,:)
-
-  integer            :: MPIERR
-  
+  integer :: pe_status = 0 ! if /=0 then something is wrong 
   !!! remPtr_* are constructed during the runtime and shall not be dumped!!!
   integer, allocatable ::  remPtr_nod2D(:),  remList_nod2D(:)
   integer, allocatable ::  remPtr_elem2D(:), remList_elem2D(:)
@@ -97,27 +96,24 @@ TYPE T_PARTIT
   !!! plock is constructed during the runtime and shall not be dumped!!!
     integer(omp_lock_kind), allocatable :: plock(:)
 #endif
-
   contains
 #if defined(__PGI)
-  procedure,private  READ_T_PARTIT
-  procedure,private  WRITE_T_PARTIT
-#else
-  procedure READ_T_PARTIT
+  private
+#endif          
   procedure WRITE_T_PARTIT
-#endif
+  procedure  READ_T_PARTIT
+  generic :: write(unformatted) => WRITE_T_PARTIT
   generic :: read(unformatted)  =>  READ_T_PARTIT
-  generic :: write(unformatted) =>  WRITE_T_PARTIT
 END TYPE T_PARTIT
 contains
 
 ! Unformatted writing for COM_STRUCT TYPE
-subroutine WRITE_T_COM_STRUCT(tstruct, unit)
+subroutine WRITE_T_COM_STRUCT(tstruct, unit, iostat, iomsg)
     IMPLICIT NONE
     class(COM_STRUCT),    intent(in)     :: tstruct
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
 
     write(unit, iostat=iostat, iomsg=iomsg) tstruct%rPEnum
     call write1d_int_static(tstruct%rPE,   unit, iostat, iomsg)
@@ -127,17 +123,17 @@ subroutine WRITE_T_COM_STRUCT(tstruct, unit)
     call write1d_int_static(tstruct%sPE,   unit, iostat, iomsg)
     call write1d_int_static(tstruct%sptr,  unit, iostat, iomsg)
     call write_bin_array(tstruct%slist, unit, iostat, iomsg)
-    ! req is constructed during the runtime
+    ! req is constructed during the runtime 
     ! call write_bin_array(tstruct%req,   unit, iostat, iomsg)
     write(unit, iostat=iostat, iomsg=iomsg) tstruct%nreq
 end subroutine WRITE_T_COM_STRUCT
 
-subroutine READ_T_COM_STRUCT(tstruct, unit)
+subroutine READ_T_COM_STRUCT(tstruct, unit, iostat, iomsg)
     IMPLICIT NONE
     class(COM_STRUCT),    intent(inout)  :: tstruct
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
 
     read(unit, iostat=iostat, iomsg=iomsg) tstruct%rPEnum
     call read1d_int_static(tstruct%rPE,   unit, iostat, iomsg)
@@ -147,7 +143,7 @@ subroutine READ_T_COM_STRUCT(tstruct, unit)
     call read1d_int_static(tstruct%sPE,   unit, iostat, iomsg)
     call read1d_int_static(tstruct%sptr,  unit, iostat, iomsg)
     call read_bin_array(tstruct%slist, unit, iostat, iomsg)
-!   req is constructed during the runtime
+!   req is constructed during the runtime 
 !   call read_bin_array(tstruct%req,   unit, iostat, iomsg)
     read(unit, iostat=iostat, iomsg=iomsg) tstruct%nreq
 end subroutine READ_T_COM_STRUCT
@@ -160,9 +156,9 @@ subroutine WRITE_T_PARTIT(partit, unit, iostat, iomsg)
     integer,              intent(out)    :: iostat
     character(*),         intent(inout)  :: iomsg
 
-    call partit%com_nod2D%WRITE_T_COM_STRUCT(unit)
-    call partit%com_elem2D%WRITE_T_COM_STRUCT(unit)
-    call partit%com_elem2D_full%WRITE_T_COM_STRUCT(unit)
+    write(unit, iostat=iostat, iomsg=iomsg) partit%com_nod2D
+    write(unit, iostat=iostat, iomsg=iomsg) partit%com_elem2D
+    write(unit, iostat=iostat, iomsg=iomsg) partit%com_elem2D_full
 
     write(unit, iostat=iostat, iomsg=iomsg) partit%npes
     write(unit, iostat=iostat, iomsg=iomsg) partit%mype
@@ -192,9 +188,9 @@ subroutine READ_T_PARTIT(partit, unit, iostat, iomsg)
     integer,              intent(out)    :: iostat
     character(*),         intent(inout)  :: iomsg
 
-    call partit%com_nod2D%READ_T_COM_STRUCT(unit)
-    call partit%com_elem2D%READ_T_COM_STRUCT(unit)
-    call partit%com_elem2D_full%READ_T_COM_STRUCT(unit)
+    read(unit, iostat=iostat, iomsg=iomsg) partit%com_nod2D
+    read(unit, iostat=iostat, iomsg=iomsg) partit%com_elem2D
+    read(unit, iostat=iostat, iomsg=iomsg) partit%com_elem2D_full
 
     read(unit, iostat=iostat, iomsg=iomsg) partit%npes
     read(unit, iostat=iostat, iomsg=iomsg) partit%mype
diff --git a/src/MOD_READ_BINARY_ARRAYS.F90 b/src/MOD_READ_BINARY_ARRAYS.F90
index 60de1167..84b883c4 100644
--- a/src/MOD_READ_BINARY_ARRAYS.F90
+++ b/src/MOD_READ_BINARY_ARRAYS.F90
@@ -7,7 +7,7 @@ use o_PARAM
 private
 public :: read_bin_array, read1d_int_static
 INTERFACE read_bin_array
-          MODULE PROCEDURE read1d_real, read1d_int, read1d_char, read2d_real, read2d_int, read3d_real, read3d_int, read4d_real, read4d_int
+          MODULE PROCEDURE read1d_real, read1d_int, read1d_char, read2d_real, read2d_int, read3d_real, read3d_int
 END INTERFACE
 contains
 subroutine read1d_real(arr, unit, iostat, iomsg)
@@ -113,31 +113,6 @@ subroutine read3d_int(arr, unit, iostat, iomsg)
     if (.not. allocated(arr)) allocate(arr(s1,s2,s3))
     read(unit, iostat=iostat, iomsg=iomsg) arr(1:s1, 1:s2, 1:s3)
 end subroutine read3d_int
-
-subroutine read4d_real(arr, unit, iostat, iomsg)
-    real(kind=WP), intent(inout), allocatable :: arr(:,:,:,:)
-    integer,       intent(in)                 :: unit
-    integer,       intent(out)                :: iostat
-    character(*),  intent(inout)              :: iomsg
-    integer                                   :: s1, s2, s3, s4
-
-    read(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3, s4
-    if ((s1==0) .or. (s2==0) .or. (s3==0) .or. (s4==0)) return
-    if (.not. allocated(arr)) allocate(arr(s1,s2,s3,s4))
-    read(unit, iostat=iostat, iomsg=iomsg) arr(1:s1, 1:s2, 1:s3, 1:s4)
-end subroutine read4d_real
-
-subroutine read4d_int(arr, unit, iostat, iomsg)
-    integer, intent(inout), allocatable :: arr(:,:,:,:)
-    integer,       intent(in)           :: unit
-    integer,       intent(out)          :: iostat
-    character(*),  intent(inout)        :: iomsg
-    integer                             :: s1, s2, s3, s4
-
-    read(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3, s4
-    if ((s1==0) .or. (s2==0) .or. (s3==0) .or. (s4==0)) return
-    if (.not. allocated(arr)) allocate(arr(s1,s2,s3,s4))
-    read(unit, iostat=iostat, iomsg=iomsg) arr(1:s1, 1:s2, 1:s3, 1:s4)
-end subroutine read4d_int
 end module MOD_READ_BINARY_ARRAYS
 !==========================================================
+
diff --git a/src/MOD_TRACER.F90 b/src/MOD_TRACER.F90
index 5ac7ac8a..6f1912d5 100644
--- a/src/MOD_TRACER.F90
+++ b/src/MOD_TRACER.F90
@@ -1,29 +1,30 @@
 !==========================================================
 MODULE MOD_TRACER
 USE O_PARAM
-USE, intrinsic :: ISO_FORTRAN_ENV, only : int32
+USE, intrinsic :: ISO_FORTRAN_ENV
 USE MOD_WRITE_BINARY_ARRAYS
 USE MOD_READ_BINARY_ARRAYS
 IMPLICIT NONE
 SAVE
 
 TYPE T_TRACER_DATA
-real(kind=WP), allocatable, dimension(:,:)    :: values    ! instant values
-real(kind=WP), allocatable, dimension(:,:)    :: valuesAB  ! Adams-Bashfort interpolation
-real(kind=WP), allocatable, dimension(:,:,:)  :: valuesold ! previous timesteps
-
-logical                                       :: smooth_bh_tra=.false.
-real(kind=WP)                                 :: gamma0_tra, gamma1_tra, gamma2_tra
-logical                                       :: i_vert_diff =.false.
-character(20)                                 :: tra_adv_hor, tra_adv_ver, tra_adv_lim ! type of the advection scheme for this tracer
-real(kind=WP)                                 :: tra_adv_ph  = 1.  ! a parameter to be used in horizontal advection (for MUSCL it is the fraction of fourth-order contribution in the solution)
-real(kind=WP)                                 :: tra_adv_pv  = 1.  ! a parameter to be used in horizontal advection (for QR4C  it is the fraction of fourth-order contribution in the solution)
-integer                                       :: AB_order=2
-integer                                       :: ID
+real(kind=WP), allocatable, dimension(:,:)  :: values, valuesAB  ! instant values & Adams-Bashfort interpolation
+logical                                     :: smooth_bh_tra=.false.
+real(kind=WP)                               :: gamma0_tra, gamma1_tra, gamma2_tra
+logical                                     :: i_vert_diff =.false.
+character(20)                               :: tra_adv_hor, tra_adv_ver, tra_adv_lim ! type of the advection scheme for this tracer
+real(kind=WP)                               :: tra_adv_ph  = 1.  ! a parameter to be used in horizontal advection (for MUSCL it is the fraction of fourth-order contribution in the solution)
+real(kind=WP)                               :: tra_adv_pv  = 1.  ! a parameter to be used in horizontal advection (for QR4C  it is the fraction of fourth-order contribution in the solution)
+integer                                     :: ID
 
 contains
+#if defined(__PGI)
+private
+#endif        
 procedure WRITE_T_TRACER_DATA
 procedure READ_T_TRACER_DATA
+generic :: write(unformatted) => WRITE_T_TRACER_DATA
+generic :: read(unformatted)  => READ_T_TRACER_DATA
 END TYPE T_TRACER_DATA
 
 
@@ -47,8 +48,13 @@ integer,allocatable,dimension(:,:)            :: edge_up_dn_tri
 real(kind=WP),allocatable,dimension(:,:,:)    :: edge_up_dn_grad
 
 contains
+#if defined(__PGI)
+private
+#endif        
 procedure WRITE_T_TRACER_WORK
 procedure READ_T_TRACER_WORK
+generic :: write(unformatted) => WRITE_T_TRACER_WORK
+generic :: read(unformatted)  => READ_T_TRACER_WORK
 END TYPE T_TRACER_WORK
 
 ! auxury type for reading namelist.tra
@@ -78,12 +84,10 @@ type(t_tracer_work)                         :: work
 
 contains
 #if defined(__PGI)
-procedure, private WRITE_T_TRACER
-procedure, private READ_T_TRACER
-#else
+private
+#endif        
 procedure WRITE_T_TRACER
 procedure READ_T_TRACER
-#endif
 generic :: write(unformatted) => WRITE_T_TRACER
 generic :: read(unformatted)  => READ_T_TRACER
 END TYPE T_TRACER
@@ -91,15 +95,14 @@ END TYPE T_TRACER
 contains
 
 ! Unformatted writing for T_TRACER_DATA
-subroutine WRITE_T_TRACER_DATA(tdata, unit)
+subroutine WRITE_T_TRACER_DATA(tdata, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_TRACER_DATA), intent(in)     :: tdata
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
 
     call write_bin_array(tdata%values,   unit, iostat, iomsg)
-    call write_bin_array(tdata%valuesold,unit, iostat, iomsg)
     call write_bin_array(tdata%valuesAB, unit, iostat, iomsg)
     write(unit, iostat=iostat, iomsg=iomsg) tdata%smooth_bh_tra
     write(unit, iostat=iostat, iomsg=iomsg) tdata%gamma0_tra
@@ -115,16 +118,15 @@ subroutine WRITE_T_TRACER_DATA(tdata, unit)
 end subroutine WRITE_T_TRACER_DATA
 
 ! Unformatted reading for T_TRACER_DATA
-subroutine READ_T_TRACER_DATA(tdata, unit)
+subroutine READ_T_TRACER_DATA(tdata, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_TRACER_DATA), intent(inout)  :: tdata
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
 
-    call read_bin_array(tdata%values,    unit, iostat, iomsg)
-    call read_bin_array(tdata%valuesold,unit, iostat, iomsg)
-    call read_bin_array(tdata%valuesAB,  unit, iostat, iomsg)
+    call read_bin_array(tdata%values,   unit, iostat, iomsg)
+    call read_bin_array(tdata%valuesAB, unit, iostat, iomsg)
     read(unit, iostat=iostat, iomsg=iomsg) tdata%smooth_bh_tra
     read(unit, iostat=iostat, iomsg=iomsg) tdata%gamma0_tra
     read(unit, iostat=iostat, iomsg=iomsg) tdata%gamma1_tra
@@ -139,12 +141,12 @@ subroutine READ_T_TRACER_DATA(tdata, unit)
 end subroutine READ_T_TRACER_DATA
 
 ! Unformatted writing for T_TRACER_WORK
-subroutine WRITE_T_TRACER_WORK(twork, unit)
+subroutine WRITE_T_TRACER_WORK(twork, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_TRACER_WORK), intent(in)     :: twork
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
 
     call write_bin_array(twork%del_ttf,          unit, iostat, iomsg)
     call write_bin_array(twork%del_ttf_advhoriz, unit, iostat, iomsg)
@@ -164,12 +166,12 @@ subroutine WRITE_T_TRACER_WORK(twork, unit)
 end subroutine WRITE_T_TRACER_WORK
 
 ! Unformatted reading for T_TRACER_WORK
-subroutine READ_T_TRACER_WORK(twork, unit)
+subroutine READ_T_TRACER_WORK(twork, unit, iostat, iomsg)
     IMPLICIT NONE
     class(T_TRACER_WORK), intent(inout)  :: twork
     integer,              intent(in)     :: unit
-    integer                              :: iostat
-    character(len=1024)                  :: iomsg
+    integer,              intent(out)    :: iostat
+    character(*),         intent(inout)  :: iomsg
 
     call read_bin_array(twork%del_ttf,          unit, iostat, iomsg)
     call read_bin_array(twork%del_ttf_advhoriz, unit, iostat, iomsg)
@@ -199,9 +201,9 @@ subroutine WRITE_T_TRACER(tracer, unit, iostat, iomsg)
 
     write(unit, iostat=iostat, iomsg=iomsg) tracer%num_tracers
     do i=1, tracer%num_tracers
-        call tracer%data(i)%WRITE_T_TRACER_DATA(unit)
+       write(unit, iostat=iostat, iomsg=iomsg) tracer%data(i)
     end do
-    call tracer%work%WRITE_T_TRACER_WORK(unit)
+    write(unit, iostat=iostat, iomsg=iomsg)    tracer%work
 !   write(unit, iostat=iostat, iomsg=iomsg)    tracer%smooth_bh_tra
 !   write(unit, iostat=iostat, iomsg=iomsg)    tracer%gamma0_tra
 !   write(unit, iostat=iostat, iomsg=iomsg)    tracer%gamma1_tra
@@ -222,10 +224,10 @@ subroutine READ_T_TRACER(tracer, unit, iostat, iomsg)
 !   write(*,*) 'number of tracers to read: ', tracer%num_tracers
     if (.not. allocated(tracer%data)) allocate(tracer%data(tracer%num_tracers))
     do i=1, tracer%num_tracers
-       call tracer%data(i)%READ_T_TRACER_DATA(unit)
+       read(unit, iostat=iostat, iomsg=iomsg) tracer%data(i)
 !      write(*,*) 'tracer info:', tracer%data(i)%ID, TRIM(tracer%data(i)%tra_adv_hor), TRIM(tracer%data(i)%tra_adv_ver), TRIM(tracer%data(i)%tra_adv_lim)
     end do
-    call tracer%work%READ_T_TRACER_WORK(unit)
+    read(unit, iostat=iostat, iomsg=iomsg)    tracer%work
 !   read(unit, iostat=iostat, iomsg=iomsg)    tracer%smooth_bh_tra
 !   read(unit, iostat=iostat, iomsg=iomsg)    tracer%gamma0_tra
 !   read(unit, iostat=iostat, iomsg=iomsg)    tracer%gamma1_tra
@@ -234,3 +236,4 @@ subroutine READ_T_TRACER(tracer, unit, iostat, iomsg)
 end subroutine READ_T_TRACER
 end module MOD_TRACER
 !==========================================================
+
diff --git a/src/MOD_WRITE_BINARY_ARRAYS.F90 b/src/MOD_WRITE_BINARY_ARRAYS.F90
index 366542a4..4f03b5ce 100644
--- a/src/MOD_WRITE_BINARY_ARRAYS.F90
+++ b/src/MOD_WRITE_BINARY_ARRAYS.F90
@@ -7,7 +7,7 @@ use o_PARAM
 private
 public :: write_bin_array, write1d_int_static
 INTERFACE write_bin_array
-          MODULE PROCEDURE write1d_real, write1d_int, write1d_char, write2d_real, write2d_int, write3d_real, write3d_int, write4d_real, write4d_int
+          MODULE PROCEDURE write1d_real, write1d_int, write1d_char, write2d_real, write2d_int, write3d_real, write3d_int
 END INTERFACE
 contains
 
@@ -64,7 +64,7 @@ end subroutine write1d_char
 
 subroutine write1d_int_static(arr, unit, iostat, iomsg)
     IMPLICIT NONE
-    integer,       intent(in)               :: arr(:)
+    integer,       intent(in)               :: arr(:) 
     integer,       intent(in)               :: unit
     integer,       intent(out)              :: iostat
     character(*),  intent(inout)            :: iomsg
@@ -155,51 +155,6 @@ subroutine write3d_int(arr, unit, iostat, iomsg)
        write(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3
     end if
 end subroutine write3d_int
-subroutine write4d_real(arr, unit, iostat, iomsg)
-    real(kind=WP), intent(in), allocatable :: arr(:,:,:,:)
-    integer,       intent(in)              :: unit
-    integer,       intent(out)             :: iostat
-    character(*),  intent(inout)           :: iomsg
-    integer                                :: s1, s2, s3, s4
-
-    if (allocated(arr)) then
-       s1=size(arr, 1)
-       s2=size(arr, 2)
-       s3=size(arr, 3)
-       s4=size(arr, 4)
-       write(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3, s4
-       write(unit, iostat=iostat, iomsg=iomsg) arr(1:s1, 1:s2, 1:s3, 1:s4)
-    else
-       s1=0
-       s2=0
-       s3=0
-       s4=0
-       write(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3, s4
-    end if
-end subroutine write4d_real
-
-subroutine write4d_int(arr, unit, iostat, iomsg)
-    integer,       intent(in), allocatable :: arr(:,:,:,:)
-    integer,       intent(in)              :: unit
-    integer,       intent(out)             :: iostat
-    character(*),  intent(inout)           :: iomsg
-    integer                                :: s1, s2, s3, s4
-
-    if (allocated(arr)) then
-       s1=size(arr, 1)
-       s2=size(arr, 2)
-       s3=size(arr, 3)
-       s4=size(arr, 4)
-       write(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3, s4
-       write(unit, iostat=iostat, iomsg=iomsg) arr(1:s1, 1:s2, 1:s3, 1:s4)
-    else
-       s1=0
-       s2=0
-       s3=0
-       s4=0
-       write(unit, iostat=iostat, iomsg=iomsg) s1, s2, s3, s4
-    end if
-end subroutine write4d_int
-
 end module MOD_WRITE_BINARY_ARRAYS
 !==========================================================
+
diff --git a/src/async_threads_module.F90 b/src/async_threads_module.F90
index 198eb98d..345f14eb 100644
--- a/src/async_threads_module.F90
+++ b/src/async_threads_module.F90
@@ -1,4 +1,3 @@
-#define __FILENAME__ "async_threads_module.F90"
 module async_threads_module
   implicit none  
   public thread_type
@@ -111,7 +110,7 @@ contains
     integer, intent(in) :: line
     ! EO args
     if(.NOT. val) then
-      print *, "error in line ",line, __FILENAME__
+      print *, "error in line ",line, __FILE__
       stop 1
     end if
   end subroutine
diff --git a/src/cpl_driver.F90 b/src/cpl_driver.F90
old mode 100644
new mode 100755
index e4e8c9c9..35d51d6f
--- a/src/cpl_driver.F90
+++ b/src/cpl_driver.F90
@@ -99,217 +99,6 @@ module cpl_driver
 
 contains
 
-    subroutine node_contours(my_x_corners, my_y_corners, partit, mesh)
-        USE MOD_MESH
-        USE MOD_PARTIT
-        USE MOD_PARSUP
-        USE o_PARAM
-        use g_comm_auto
-        use o_ARRAYS
-        use g_rotate_grid
-
-        IMPLICIT NONE
-        type(t_mesh),   intent(in), target :: mesh
-        type(t_partit), intent(inout), target :: partit
-        real(kind=WP), allocatable, intent(inout) :: my_x_corners(:,:)     ! longitude node corners
-        real(kind=WP), allocatable, intent(inout) :: my_y_corners(:,:)     ! latitude node corners    
-        integer                               :: bEdge_left, bEdge_right
-        integer,              dimension(2)    :: belem_left, belem_right
-        integer                               :: edge_left, edge_right
-        integer                               :: n, ee, elem, nn, el(2), flag, nn1, nn2
-        integer                               :: current_pos
-        integer                               :: pos_increment=-1 ! counter clockwise is negative, otherwise +1!
-        integer, allocatable, dimension(:)    :: nedges, nelems, nedges1, nelems1, nedges2, nelems2
-        real(kind=WP)                         :: this_x_coord, this_y_coord
-
-include "associate_part_def.h"
-include "associate_mesh_def.h"
-include "associate_part_ass.h"
-include "associate_mesh_ass.h"
-
-    if (.not. allocated(my_x_corners)) then
-        ALLOCATE(my_x_corners(myDim_nod2D, 25)) !maxval(nod_in_elem2D_num, 1)*2+2))
-    endif
-    if (.not. allocated(my_y_corners)) then
-        ALLOCATE(my_y_corners(myDim_nod2D, 25)) !maxval(nod_in_elem2D_num, 1)*2+2))
-    endif
-    do n=1, myDim_nod2D
-        ! find the type/of node: internal or at boundary
-        bEdge_left =0
-        belem_left =0
-        bEdge_right=0
-        belem_right=0
-
-        do ee=1, nod_in_elem2D_num(n)
-           elem=nod_in_elem2D(ee,n)
-           if (elem2D_nodes(1,elem)==n) then
-              edge_left=elem_edges(3,elem)
-              edge_right=elem_edges(2,elem)
-           elseif (elem2D_nodes(2,elem)==n) then
-              edge_left=elem_edges(1,elem)
-              edge_right=elem_edges(3,elem)
-           else
-              edge_left=elem_edges(2,elem)
-              edge_right=elem_edges(1,elem)
-           end if
-           if (myList_edge2D(edge_left)>edge2D_in) then
-              bEdge_left=bEdge_left+1
-              belem_left(bEdge_left)=elem
-           end if
-           if (myList_edge2D(edge_right)>edge2D_in) then
-              bEdge_right=bEdge_right+1
-              belem_right(bEdge_right)=elem
-           end if
-        end do
-
-    ! now we have three cases
-       if (bEdge_left==0) then      ! inner contour
-          elem=nod_in_elem2D(1, n)  ! we can start from any
-          allocate(nedges(nod_in_elem2D_num(n)))
-          nedges=0
-          allocate(nelems(nod_in_elem2D_num(n)))
-          nelems=0
-          !!!!!!! inner_node_contour
-include "node_contour_inner.h"
-          if (pos_increment<0) then 
-             current_pos=2*nod_in_elem2D_num(n)
-          else
-             current_pos =1
-          end if
-          do nn=1, nod_in_elem2D_num(n)
-             call edge_center(edges(1, nedges(nn)), edges(2, nedges(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-             current_pos=current_pos+pos_increment
-             call elem_center(nelems(nn), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-             current_pos=current_pos+pos_increment
-          end do
-          current_pos=2*nod_in_elem2D_num(n)+1
-          do nn=current_pos, size(my_x_corners, 2)
-             my_x_corners(n, nn)=my_x_corners(n, current_pos-1)
-             my_y_corners(n, nn)=my_y_corners(n, current_pos-1)
-          end do
-          deallocate(nedges, nelems)
-       end if
-
-
-       if (bEdge_left==1) then ! standard boundary node
-          elem=belem_left(1)
-          allocate(nedges(nod_in_elem2D_num(n)+1))
-          nedges=0
-          allocate(nelems(nod_in_elem2D_num(n)))
-          nelems=0
-          !!!!!!!boundary_node_contour
-include "node_contour_boundary.h"
-          if (pos_increment<0) then 
-             current_pos=2*nod_in_elem2D_num(n)+2 !one more for the node n itself also we have 2 boundary edges
-          else
-             current_pos =1
-          end if
-          do nn=1, nod_in_elem2D_num(n)
-             call edge_center(edges(1, nedges(nn)), edges(2, nedges(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-             current_pos=current_pos+pos_increment
-             call elem_center(nelems(nn), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-             current_pos=current_pos+pos_increment
-          end do
-          nn=nod_in_elem2D_num(n)+1
-          call edge_center(edges(1, nedges(nn)), edges(2, nedges(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-          current_pos=current_pos+pos_increment
-          my_x_corners(n, current_pos)=coord_nod2D(1,n)
-          my_y_corners(n, current_pos)=coord_nod2D(2,n)
-          current_pos=2*nod_in_elem2D_num(n)+3
-          do nn=current_pos, size(my_x_corners, 2)
-             my_x_corners(n, nn)=my_x_corners(n, current_pos-1)
-             my_y_corners(n, nn)=my_y_corners(n, current_pos-1)
-          end do
-          !!!!!!!
-          deallocate(nedges, nelems)
-       end if
-
-       if (bEdge_left==2) then  ! strange boundary node
-           elem=belem_left(1)
-           allocate(nedges (nod_in_elem2D_num(n)+1))
-           allocate(nedges1(nod_in_elem2D_num(n)+1))
-           nedges =0
-           nedges1=0
-           allocate(nelems (nod_in_elem2D_num(n)))
-           allocate(nelems1(nod_in_elem2D_num(n)))
-           nelems=0
-           nelems1=0
-           if (pos_increment<0) then 
-            current_pos=2*nod_in_elem2D_num(n)+4 !two more for the node n itself also we have 4 boundary edges
-           else
-            current_pos =1
-           end if
-           !!!!!!!boundary_node_contour
-include "node_contour_boundary.h"
-           where (nedges>0)
-                 nedges1=nedges
-           end where
-           where (nelems>0)
-                 nelems1=nelems
-           end where
-           nn1=nn
-           do nn=1, nn1
-              call edge_center(edges(1, nedges1(nn)), edges(2, nedges1(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-              current_pos=current_pos+pos_increment
-              call elem_center(nelems1(nn), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-              current_pos=current_pos+pos_increment
-           end do
-           nn=nn1+1
-           call edge_center(edges(1, nedges1(nn)), edges(2, nedges1(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-           current_pos=current_pos+pos_increment
-           nn=nn1+2
-           my_x_corners(n, current_pos)=coord_nod2D(1,n)
-           my_y_corners(n, current_pos)=coord_nod2D(2,n)
-           current_pos=current_pos+pos_increment
-           !!!!!!!
-           elem=belem_left(2)
-           allocate(nedges2(nod_in_elem2D_num(n)+1))
-           nedges =0
-           nedges2=0
-           allocate(nelems2(nod_in_elem2D_num(n)))
-           nelems =0
-           nelems2=0
-           !!!!!!!boundary_node_contour
-include "node_contour_boundary.h"
-           where (nedges>0)
-                nedges2=nedges
-           end where
-           where (nelems>0)
-                 nelems2=nelems
-           end where
-           nn2=nn
-           do nn=nn1+3, nn1+nn2+2
-              call edge_center(edges(1, nedges2(nn)), edges(2, nedges2(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-              current_pos=current_pos+pos_increment
-              call elem_center(nelems2(nn), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-              current_pos=current_pos+pos_increment
-           end do
-           nn=nn1+nn2+3
-           call edge_center(edges(1, nedges2(nn)), edges(2, nedges2(nn)), my_x_corners(n, current_pos), my_y_corners(n, current_pos), mesh)
-           current_pos=current_pos+pos_increment
-           nn=nn1+nn2+4
-           my_x_corners(n, nn)=coord_nod2D(1,n)
-           my_y_corners(n, nn)=coord_nod2D(2,n)
-           current_pos=2*nod_in_elem2D_num(n)+5
-           do nn=current_pos, size(my_x_corners, 2)
-              my_x_corners(n, nn)=my_x_corners(n, current_pos-1)
-              my_y_corners(n, nn)=my_y_corners(n, current_pos-1)
-           end do
-           !!!!!!!
-           deallocate(nedges, nelems, nedges1, nelems1, nedges2, nelems2)
-       end if
-    end do
-    do n=1, myDim_nod2D
-       do nn=1, size(my_x_corners, 2)
-          this_x_coord=my_x_corners(n, nn)
-          this_y_coord=my_y_corners(n, nn)
-          call r2g(my_x_corners(n, nn), my_y_corners(n, nn), this_x_coord, this_y_coord)
-       end do
-    end do
-    my_x_corners=my_x_corners/rad
-    my_y_corners=my_y_corners/rad
-    end subroutine node_contours
-
   subroutine cpl_oasis3mct_init(partit, localCommunicator )
     USE MOD_PARTIT
     implicit none   
@@ -448,15 +237,10 @@ include "node_contour_boundary.h"
     real(kind=WP), allocatable :: all_y_coords(:, :)     ! latitude  coordinates
     real(kind=WP), allocatable :: all_area(:,:)    
 
-    real(kind=WP), allocatable :: my_x_corners(:,:)     ! local longitude node corners
-    real(kind=WP), allocatable :: my_y_corners(:,:)     ! local latitude node corners
-    real(kind=WP), allocatable :: all_x_corners(:,:,:)     ! global longitude node corners
-    real(kind=WP), allocatable :: all_y_corners(:,:,:)     ! global latitude node corners
-
-include "associate_part_def.h"
-include "associate_mesh_def.h"
-include "associate_part_ass.h"
-include "associate_mesh_ass.h"
+#include "associate_part_def.h"
+#include "associate_mesh_def.h"
+#include "associate_part_ass.h"
+#include "associate_mesh_ass.h"
 
 
 #ifdef VERBOSE
@@ -532,8 +316,6 @@ include "associate_mesh_ass.h"
       
     ALLOCATE(my_x_coords(my_number_of_points))
     ALLOCATE(my_y_coords(my_number_of_points))
-    ALLOCATE(my_x_corners(myDim_nod2D, 25))
-    ALLOCATE(my_y_corners(myDim_nod2D, 25))
 
     do i = 1, my_number_of_points
       this_x_coord = coord_nod2D(1, i)
@@ -544,30 +326,16 @@ include "associate_mesh_ass.h"
     my_x_coords=my_x_coords/rad
     my_y_coords=my_y_coords/rad
 
-    if (mype .eq. 0) then
-      print *, 'FESOM before corner computation'
-    endif
-    call node_contours(my_x_corners, my_y_corners, partit, mesh)
-    if (mype .eq. 0) then
-      print *, 'FESOM after corner computation'
-    endif
-
     if (mype .eq. localroot) then
       ALLOCATE(all_x_coords(number_of_all_points, 1))
       ALLOCATE(all_y_coords(number_of_all_points, 1))
       ALLOCATE(all_area(number_of_all_points, 1))
-      ALLOCATE(all_x_corners(number_of_all_points, 1, 25))
-      ALLOCATE(all_y_corners(number_of_all_points, 1, 25))
     else 
       ALLOCATE(all_x_coords(1, 1))
       ALLOCATE(all_y_coords(1, 1))
       ALLOCATE(all_area(1, 1))
-      ALLOCATE(all_x_corners(1, 1, 1))
-      ALLOCATE(all_y_corners(1, 1, 1))
     endif
 
-
-
     displs_from_all_pes(1) = 0
     do i = 2, npes
       displs_from_all_pes(i) = SUM(counts_from_all_pes(1:(i-1)))
@@ -591,12 +359,9 @@ include "associate_mesh_ass.h"
     CALL MPI_GATHERV(area(1,:), my_number_of_points, MPI_DOUBLE_PRECISION, all_area,  &
                     counts_from_all_pes, displs_from_all_pes, MPI_DOUBLE_PRECISION, localroot, MPI_COMM_FESOM, ierror)
 
-    do j = 1, 25
-      CALL MPI_GATHERV(my_x_corners(:,j), myDim_nod2D, MPI_DOUBLE_PRECISION, all_x_corners(:,:,j),  &
-                    counts_from_all_pes, displs_from_all_pes, MPI_DOUBLE_PRECISION, localroot, MPI_COMM_FESOM, ierror)
-      CALL MPI_GATHERV(my_y_corners(:,j), myDim_nod2D, MPI_DOUBLE_PRECISION, all_y_corners(:,:,j),  &
-                    counts_from_all_pes, displs_from_all_pes, MPI_DOUBLE_PRECISION, localroot, MPI_COMM_FESOM, ierror)
-    end do
+    if (mype .eq. 0) then 
+      print *, 'FESOM after 3rd GatherV'
+    endif
 
     CALL MPI_Barrier(MPI_COMM_FESOM, ierror)
     if (mype .eq. 0) then 
@@ -604,16 +369,13 @@ include "associate_mesh_ass.h"
     endif
 
     if (mype .eq. localroot) then
-      print *, 'FESOM before grid writing to oasis grid files'
+      print *, 'FESOM before start_grids_writing'
        CALL oasis_start_grids_writing(il_flag)
        IF (il_flag .NE. 0) THEN
 
-          print *, 'FESOM before write grid centers'
+          print *, 'FESOM before write grid'
           CALL oasis_write_grid (grid_name, number_of_all_points, 1, all_x_coords(:,:), all_y_coords(:,:))
 
-          print *, 'FESOM before write corner'
-          CALL oasis_write_corner (grid_name, number_of_all_points, 1, 25, all_x_corners(:,:,:), all_y_corners(:,:,:))
-
           ALLOCATE(unstr_mask(number_of_all_points, 1))
           unstr_mask=0
           print *, 'FESOM before write mask'
diff --git a/src/fesom_module.F90 b/src/fesom_module.F90
index 843152e4..8333f50a 100755
--- a/src/fesom_module.F90
+++ b/src/fesom_module.F90
@@ -30,14 +30,19 @@ module fesom_main_storage_module
   use read_mesh_interface
   use fesom_version_info_module
   use command_line_options_module
-  use, intrinsic :: iso_fortran_env, only : real32
+  !---fwf-code, age-code
   use g_forcing_param, only: use_landice_water, use_age_tracer
   use landice_water_init_interface
   use age_tracer_init_interface
-  use iceberg_params
-  use iceberg_step
+  !---fwf-code-end, age-code-end
+
   ! Define icepack module
 
+  ! --------------
+  ! LA icebergs: 2023-05-17 
+  use iceberg_params
+  ! --------------
+
 #if defined (__icepack)
   use icedrv_main,          only: set_icepack, init_icepack, alloc_icepack
 #endif
@@ -52,8 +57,7 @@ module fesom_main_storage_module
 
     integer           :: n, from_nstep, offset, row, i, provided
     integer           :: which_readr ! read which restart files (0=netcdf, 1=core dump,2=dtype)
-    integer           :: total_nsteps
-    integer, pointer  :: mype, npes, MPIerr, MPI_COMM_FESOM, MPI_COMM_WORLD, MPI_COMM_FESOM_IB
+    integer, pointer  :: mype, npes, MPIerr, MPI_COMM_FESOM, MPI_COMM_FESOM_IB
     real(kind=WP)     :: t0, t1, t2, t3, t4, t5, t6, t7, t8, t0_ice, t1_ice, t0_frc, t1_frc
     real(kind=WP)     :: rtime_fullice,    rtime_write_restart, rtime_write_means, rtime_compute_diag, rtime_read_forcing
     real(kind=real32) :: rtime_setup_mesh, rtime_setup_ocean, rtime_setup_forcing 
@@ -97,13 +101,10 @@ contains
  
   subroutine fesom_init(fesom_total_nsteps)
       use fesom_main_storage_module
-#if defined(__MULTIO)
-      use iom
-#endif
       integer, intent(out) :: fesom_total_nsteps
       ! EO parameters
       logical mpi_is_initialized
-      integer              :: tr_num
+
 #if !defined  __ifsinterface
       if(command_argument_count() > 0) then
         call command_line_options%parse()
@@ -119,7 +120,7 @@ contains
         !OIFS-FESOM2 coupling: does not require MPI_INIT here as this is done by OASIS
         call MPI_Initialized(mpi_is_initialized, f%i)
         if(.not. mpi_is_initialized) then
-            ! TODO: do not initialize MPI here if it has been initialized already, e.g. via IFS when fesom is called as library (__ifsinterface is defined)
+            ! do not initialize MPI here if it has been initialized already, e.g. via IFS when fesom is called as library (__ifsinterface is defined)
             call MPI_INIT_THREAD(MPI_THREAD_MULTIPLE, f%provided, f%i)
             f%fesom_did_mpi_init = .true.
         end if
@@ -137,8 +138,6 @@ contains
         f%MPIerr        =>f%partit%MPIerr
         f%MPI_COMM_FESOM=>f%partit%MPI_COMM_FESOM
         f%MPI_COMM_FESOM_IB=>f%partit%MPI_COMM_FESOM_IB
-        f%MPI_COMM_WORLD=>f%partit%MPI_COMM_WORLD
-
         f%npes          =>f%partit%npes
         if(f%mype==0) then
             write(*,*)
@@ -156,7 +155,6 @@ contains
         call setup_model(f%partit)  ! Read Namelists, always before clock_init
         call clock_init(f%partit)   ! read the clock file 
         call get_run_steps(fesom_total_nsteps, f%partit)
-        f%total_nsteps=fesom_total_nsteps
         if (flag_debug .and. f%mype==0)  print *, achar(27)//'[34m'//' --> call mesh_setup'//achar(27)//'[0m'
         call mesh_setup(f%partit, f%mesh)
 
@@ -218,7 +216,8 @@ contains
         endif
         
         if (f%mype==0) f%t5=MPI_Wtime()
-        call compute_diagnostics(0, f%dynamics, f%tracers, f%ice, f%partit, f%mesh) ! allocate arrays for diagnostic
+        call compute_diagnostics(0, f%dynamics, f%tracers, f%partit, f%mesh) ! allocate arrays for diagnostic
+
         !---fwf-code-begin
         if(f%mype==0)  write(*,*) 'use_landice_water', use_landice_water
         if(use_landice_water) call landice_water_init(f%partit, f%mesh)
@@ -228,6 +227,7 @@ contains
         if(f%mype==0)  write(*,*) 'use_age_tracer', use_age_tracer
         if(use_age_tracer) call age_tracer_init(f%partit, f%mesh)
         !---age-code-end
+
 #if defined (__oasis)
         call cpl_oasis3mct_define_unstr(f%partit, f%mesh)
 
@@ -237,7 +237,7 @@ contains
         ! --------------
         ! LA icebergs: 2023-05-17 
         if (use_icebergs) then
-            call allocate_icb(f%partit)
+            call allocate_icb(f%partit, f%mesh)
         endif
         ! --------------
 
@@ -254,7 +254,7 @@ contains
         call clock_newyear                        ! check if it is a new year
         if (f%mype==0) f%t6=MPI_Wtime()
         !___CREATE NEW RESTART FILE IF APPLICABLE___________________________________
-        call restart(0, 0, 0, r_restart, f%which_readr, f%ice, f%dynamics, f%tracers, f%partit, f%mesh)
+        call restart(0, r_restart, f%which_readr, f%ice, f%dynamics, f%tracers, f%partit, f%mesh)
         if (f%mype==0) f%t7=MPI_Wtime()
         ! store grid information into netcdf file
         if (.not. r_restart) call write_mesh_info(f%partit, f%mesh)
@@ -286,10 +286,6 @@ contains
             write(*,*) '============================================' 
         endif
 
-#if defined(__MULTIO)
-          call iom_send_fesom_domains(f%partit, f%mesh)
-#endif
-
     !    f%dump_dir='DUMP/'
     !    INQUIRE(file=trim(f%dump_dir), EXIST=f%L_EXISTS)
     !    if (.not. f%L_EXISTS) call system('mkdir '//trim(f%dump_dir))
@@ -334,63 +330,14 @@ contains
     f%rtime_read_forcing  = 0._WP
 
     f%from_nstep = 1
-
-    !enter mesh and partit data. 
-    !$ACC ENTER DATA COPYIN (f) 
-    !$ACC ENTER DATA COPYIN (f%mesh, f%mesh%coriolis_node, f%mesh%nn_num, f%mesh%nn_pos) 
-    !$ACC ENTER DATA COPYIN (f%mesh%ssh_stiff, f%mesh%ssh_stiff%rowptr) 
-    !$ACC ENTER DATA COPYIN (f%mesh%gradient_sca, f%mesh%metric_factor, f%mesh%elem_area, f%mesh%area, f%mesh%edge2D_in) 
-    !$ACC ENTER DATA COPYIN (f%mesh%elem2D_nodes, f%mesh%ulevels, f%mesh%ulevels_nod2d, f%mesh%edges, f%mesh%edge_tri) 
-    !$ACC ENTER DATA COPYIN (f%partit, f%partit%eDim_nod2D, f%partit%myDim_edge2D) 
-    !$ACC ENTER DATA COPYIN (f%partit%myDim_elem2D, f%partit%myDim_nod2D, f%partit%myList_edge2D) 
-
-    !$ACC ENTER DATA COPYIN (f%mesh%elem_cos, f%mesh%edge_cross_dxdy, f%mesh%elem2d_nodes, f%mesh%nl) 
-    !$ACC ENTER DATA COPYIN (f%mesh%nlevels_nod2D, f%mesh%nod_in_elem2D, f%mesh%nod_in_elem2D_num) 
-    !$ACC ENTER DATA COPYIN (f%mesh%edge_dxdy, f%mesh%nlevels, f%mesh%ulevels_nod2D_max) 
-    !$ACC ENTER DATA COPYIN (f%mesh%areasvol, f%mesh%nlevels_nod2D_min) 
-    !$ACC ENTER DATA CREATE (f%mesh%helem, f%mesh%hnode, f%mesh%hnode_new, f%mesh%zbar_3d_n, f%mesh%z_3d_n)
-    !do n=f%from_nstep, f%from_nstep-1+current_nsteps
-    !$ACC ENTER DATA COPYIN  (f%ice)
-    !$ACC ENTER DATA CREATE  (f%ice%data, f%ice%work, f%ice%work%fct_massmatrix) 
-    !$ACC ENTER DATA CREATE  (f%ice%delta_min, f%ice%Tevp_inv, f%ice%cd_oce_ice) 
-    !$ACC ENTER DATA CREATE  (f%ice%work%fct_tmax, f%ice%work%fct_tmin) 
-    !$ACC ENTER DATA CREATE  (f%ice%work%fct_fluxes, f%ice%work%fct_plus, f%ice%work%fct_minus) 
-    !$ACC ENTER DATA CREATE  (f%ice%work%eps11, f%ice%work%eps12, f%ice%work%eps22) 
-    !$ACC ENTER DATA CREATE  (f%ice%work%sigma11, f%ice%work%sigma12, f%ice%work%sigma22) 
-    !$ACC ENTER DATA CREATE  (f%ice%work%ice_strength, f%ice%stress_atmice_x, f%ice%stress_atmice_y) 
-    !$ACC ENTER DATA COPYIN  (f%ice%thermo%rhosno, f%ice%thermo%rhoice, f%ice%thermo%inv_rhowat) 
-    !$ACC ENTER DATA CREATE  (f%ice%srfoce_ssh, f%ice%pstar, f%ice%c_pressure) 
-    !$ACC ENTER DATA CREATE  (f%ice%work%inv_areamass, f%ice%work%inv_mass, f%ice%uice_rhs, f%ice%vice_rhs) 
-    !$ACC ENTER DATA CREATE  (f%ice%uice, f%ice%vice, f%ice%srfoce_u, f%ice%srfoce_v, f%ice%uice_old, f%ice%vice_old) 
-    !$ACC ENTER DATA CREATE  (f%ice%data(1)%values, f%ice%data(2)%values, f%ice%data(3)%values) 
-    !$ACC ENTER DATA CREATE  (f%ice%data(1)%valuesl, f%ice%data(2)%valuesl, f%ice%data(3)%valuesl) 
-    !$ACC ENTER DATA CREATE  (f%ice%data(1)%dvalues, f%ice%data(2)%dvalues, f%ice%data(3)%dvalues) 
-    !$ACC ENTER DATA CREATE  (f%ice%data(1)%values_rhs, f%ice%data(2)%values_rhs, f%ice%data(3)%values_rhs) 
-    !$ACC ENTER DATA CREATE  (f%ice%data(1)%values_div_rhs, f%ice%data(2)%values_div_rhs, f%ice%data(3)%values_div_rhs)
-#if defined (__oifs) || defined (__ifsinterface)
-    !$ACC ENTER DATA CREATE (f%ice%data(4)%values, f%ice%data(4)%valuesl, f%ice%data(4)%dvalues, f%ice%data(4)%values_rhs, f%ice%data(4)%values_div_rhs)
-#endif
-    !$ACC ENTER DATA COPYIN (f%dynamics)
-    !$ACC ENTER DATA CREATE (f%dynamics%w, f%dynamics%w_e, f%dynamics%uv)
-    !$ACC ENTER DATA CREATE (f%tracers%work%del_ttf)
-    !$ACC ENTER DATA CREATE (f%tracers%data, f%tracers%work) 
-    do tr_num=1, f%tracers%num_tracers
-    !$ACC ENTER DATA CREATE (f%tracers%data(tr_num)%values, f%tracers%data(tr_num)%valuesAB)
-    !$ACC ENTER DATA CREATE (f%tracers%data(tr_num)%tra_adv_ph, f%tracers%data(tr_num)%tra_adv_pv)
-    end do
-    !$ACC ENTER DATA CREATE (f%tracers%work%fct_ttf_min, f%tracers%work%fct_ttf_max, f%tracers%work%fct_plus, f%tracers%work%fct_minus) &
-    !$ACC CREATE (f%tracers%work%adv_flux_hor, f%tracers%work%adv_flux_ver, f%tracers%work%fct_LO) &
-    !$ACC CREATE (f%tracers%work%del_ttf_advvert, f%tracers%work%del_ttf_advhoriz, f%tracers%work%edge_up_dn_grad) &
-    !$ACC CREATE (f%tracers%work%del_ttf)
   end subroutine
 
 
   subroutine fesom_runloop(current_nsteps)
     use fesom_main_storage_module
-!   use openacc_lib
     integer, intent(in) :: current_nsteps 
     ! EO parameters
-    integer n, nstart, ntotal, tr_num
+    integer n
 
     !=====================
     ! Time stepping
@@ -406,7 +353,8 @@ contains
     ! --------------
 
     if (f%mype==0) write(*,*) 'FESOM start iteration before the barrier...'
-    call MPI_Barrier(f%MPI_COMM_FESOM, f%MPIERR)   
+    call MPI_Barrier(f%MPI_COMM_FESOM, f%MPIERR)
+    
     if (f%mype==0) then
        write(*,*) 'FESOM start iteration after the barrier...'
        f%t0 = MPI_Wtime()
@@ -420,62 +368,67 @@ contains
     if (use_global_tides) then
        call foreph_ini(yearnew, month, f%partit)
     end if
-    
-    nstart=f%from_nstep
-    ntotal=f%from_nstep-1+current_nsteps
+    do n=f%from_nstep, f%from_nstep-1+current_nsteps        
 
-    do n=nstart, ntotal
-        if (use_icebergs) then
-                !n_ib         = n
-                u_wind_ib    = u_wind
-                v_wind_ib    = v_wind
-                f%ice%uice_ib     = f%ice%uice
-                f%ice%vice_ib     = f%ice%vice
         
-        ! LA - this causes the blowup !
-        !        f%ice%data(size(f%ice%data))      = f%ice%data(2)
-        !        f%ice%data(size(f%ice%data)-1)    = f%ice%data(1)
-        !!!!!!!!!!!!!!!!!!
         
         
-        ! kh 08.03.21 support of different ocean ice and iceberg steps:
-        ! if steps_per_ib_step is configured greater 1 then UV is modified via call oce_timestep_ale(n) -> call update_vel while
-        ! the same asynchronous iceberg computation is still active
-                f%dynamics%uv_ib     = f%dynamics%uv
         
-        ! kh 15.03.21 support of different ocean ice and iceberg steps:
-        ! if steps_per_ib_step is configured greater 1 then tr_arr is modified via call oce_timestep_ale(n) -> call solve_tracers_ale() while
-        ! the same asynchronous iceberg computation is still active
-                !tr_arr_ib    = tr_arr
-                Tclim_ib     = f%tracers%data(1)%values
-                Sclim_ib     = f%tracers%data(2)%values
         
-        ! kh 15.03.21 support of different ocean ice and iceberg steps:
-        ! if steps_per_ib_step is configured greater 1 then Tsurf and Ssurf might be changed while
-        ! the same asynchronous iceberg computation is still active
-                Tsurf_ib     = Tsurf
-                Ssurf_ib     = Ssurf
         
-        ! kh 18.03.21 support of different ocean ice and iceberg steps:
-        ! if steps_per_ib_step is configured greater 1 then zbar_3d_n and eta_n might be changed while
-        ! the same asynchronous iceberg computation is still active
-                !zbar_3d_n_ib = zbar_3d_n
-                f%mesh%Z_3d_n_ib     = f%mesh%Z_3d_n
-                f%dynamics%eta_n_ib  = f%dynamics%eta_n
         
-        ! kh 16.03.21 not modified during overlapping ocean/ice and iceberg computations
-        !       coriolis_ib      = coriolis
-        !       coriolis_node_ib = coriolis_node
         
-        ! kh 02.02.21 check iceberg computations mode:
-        ! ib_async_mode == 0: original sequential behavior for both ice sections (for testing purposes, creating reference results etc.)
-        ! ib_async_mode == 1: OpenMP code active to overlapped computations in first (ocean ice) and second (icebergs) parallel section
-        ! ib_async_mode == 2: OpenMP code active, but computations still serialized via spinlock (for testing purposes)
-        
-        ! -----------------------------------------------------------------------------------
-        ! LA asyncronous coupling not included in this FESOM version, yet!!
-        ! 
-        end if        
+if (use_icebergs) then
+        !n_ib         = n
+        u_wind_ib    = u_wind
+        v_wind_ib    = v_wind
+        f%ice%uice_ib     = f%ice%uice
+        f%ice%vice_ib     = f%ice%vice
+
+! LA - this causes the blowup !
+!        f%ice%data(size(f%ice%data))      = f%ice%data(2)
+!        f%ice%data(size(f%ice%data)-1)    = f%ice%data(1)
+!!!!!!!!!!!!!!!!!!
+
+
+! kh 08.03.21 support of different ocean ice and iceberg steps:
+! if steps_per_ib_step is configured greater 1 then UV is modified via call oce_timestep_ale(n) -> call update_vel while
+! the same asynchronous iceberg computation is still active
+        f%dynamics%uv_ib     = f%dynamics%uv
+
+! kh 15.03.21 support of different ocean ice and iceberg steps:
+! if steps_per_ib_step is configured greater 1 then tr_arr is modified via call oce_timestep_ale(n) -> call solve_tracers_ale() while
+! the same asynchronous iceberg computation is still active
+        !tr_arr_ib    = tr_arr
+        Tclim_ib     = f%tracers%data(1)%values
+        Sclim_ib     = f%tracers%data(2)%values
+
+! kh 15.03.21 support of different ocean ice and iceberg steps:
+! if steps_per_ib_step is configured greater 1 then Tsurf and Ssurf might be changed while
+! the same asynchronous iceberg computation is still active
+        Tsurf_ib     = Tsurf
+        Ssurf_ib     = Ssurf
+
+! kh 18.03.21 support of different ocean ice and iceberg steps:
+! if steps_per_ib_step is configured greater 1 then zbar_3d_n and eta_n might be changed while
+! the same asynchronous iceberg computation is still active
+        !zbar_3d_n_ib = zbar_3d_n
+        f%mesh%Z_3d_n_ib     = f%mesh%Z_3d_n
+        f%dynamics%eta_n_ib  = f%dynamics%eta_n
+
+! kh 16.03.21 not modified during overlapping ocean/ice and iceberg computations
+!       coriolis_ib      = coriolis
+!       coriolis_node_ib = coriolis_node
+
+! kh 02.02.21 check iceberg computations mode:
+! ib_async_mode == 0: original sequential behavior for both ice sections (for testing purposes, creating reference results etc.)
+! ib_async_mode == 1: OpenMP code active to overlapped computations in first (ocean ice) and second (icebergs) parallel section
+! ib_async_mode == 2: OpenMP code active, but computations still serialized via spinlock (for testing purposes)
+
+! -----------------------------------------------------------------------------------
+! LA asyncronous coupling not included in this FESOM version, yet!!
+! 
+end if        
         
         if (use_global_tides) then
            call foreph(f%partit, f%mesh)
@@ -494,6 +447,8 @@ contains
         !___compute horizontal velocity on nodes (originaly on elements)________
         if (flag_debug .and. f%mype==0)  print *, achar(27)//'[34m'//' --> call compute_vel_nodes'//achar(27)//'[0m'
         call compute_vel_nodes(f%dynamics, f%partit, f%mesh)
+       
+
         ! --------------
         ! LA icebergs: 2023-05-17 
         if (use_icebergs .and. mod(n - 1, steps_per_ib_step)==0) then
@@ -502,6 +457,8 @@ contains
             call iceberg_calculation(f%ice,f%mesh,f%partit,f%dynamics,n)
         end if
         ! --------------
+
+
         !___model sea-ice step__________________________________________________
         f%t1 = MPI_Wtime()
         if(use_ice) then
@@ -526,13 +483,13 @@ contains
             if (f%ice%ice_update) call ice_timestep(n, f%ice, f%partit, f%mesh)  
 
             
-            ! LA commented for debugging
-            ! --------------
-            ! LA icebergs: 2023-05-17 
-            if (use_icebergs .and. mod(n, steps_per_ib_step)==0.0) then
-                call icb2fesom(f%mesh, f%partit, f%ice)
-            end if
-            ! --------------
+            !! LA commented for debugging
+            !! --------------
+            !! LA icebergs: 2023-05-17 
+            !if (use_icebergs .and. mod(n, steps_per_ib_step)==0.0) then
+            !    call icb2fesom(f%mesh, f%partit, f%ice)
+            !end if
+            !! --------------
 
 
             !___compute fluxes to the ocean: heat, freshwater, momentum_________
@@ -550,7 +507,7 @@ contains
         f%t3 = MPI_Wtime()
         !___compute energy diagnostics..._______________________________________
         if (flag_debug .and. f%mype==0)  print *, achar(27)//'[34m'//' --> call compute_diagnostics(1)'//achar(27)//'[0m'
-        call compute_diagnostics(1, f%dynamics, f%tracers, f%ice, f%partit, f%mesh)
+        call compute_diagnostics(1, f%dynamics, f%tracers, f%partit, f%mesh)
 
         f%t4 = MPI_Wtime()
         !___prepare output______________________________________________________
@@ -564,86 +521,40 @@ contains
         !--------------------------
 
         f%t5 = MPI_Wtime()
-        call restart(n, nstart, f%total_nsteps, .false., f%which_readr, f%ice, f%dynamics, f%tracers, f%partit, f%mesh)
+        call restart(n, .false., f%which_readr, f%ice, f%dynamics, f%tracers, f%partit, f%mesh)
         f%t6 = MPI_Wtime()
         
         f%rtime_fullice       = f%rtime_fullice       + f%t2 - f%t1
         f%rtime_compute_diag  = f%rtime_compute_diag  + f%t4 - f%t3
-        f%rtime_write_means   = f%rtime_write_means   + f%t5 - f%t4
+        f%rtime_write_means   = f%rtime_write_means   + f%t5 - f%t4   
         f%rtime_write_restart = f%rtime_write_restart + f%t6 - f%t5
         f%rtime_read_forcing  = f%rtime_read_forcing  + f%t1_frc - f%t0_frc
     end do
-!call cray_acc_set_debug_global_level(3)    
+
     f%from_nstep = f%from_nstep+current_nsteps
-!call cray_acc_set_debug_global_level(0)    
-!   write(0,*) 'f%from_nstep after the loop:', f%from_nstep    
   end subroutine
 
 
   subroutine fesom_finalize()
     use fesom_main_storage_module
-#if defined(__MULTIO)
-    use iom
-    use mpp_io
-#endif
     ! EO parameters
     real(kind=real32) :: mean_rtime(15), max_rtime(15), min_rtime(15)
-    integer           :: tr_num
+
     ! --------------
     ! LA icebergs: 2023-05-17 
     if (use_icebergs) then
          call iceberg_out(f%partit)
     end if
     ! --------------
+
+
+
     call finalize_output()
     call finalize_restart()
 
     !___FINISH MODEL RUN________________________________________________________
 
     call MPI_Barrier(f%MPI_COMM_FESOM, f%MPIERR)
-    !$ACC EXIT DATA DELETE (f%ice%delta_min, f%ice%Tevp_inv, f%ice%cd_oce_ice)
-    !$ACC EXIT DATA DELETE (f%ice%work%fct_tmax, f%ice%work%fct_tmin)
-    !$ACC EXIT DATA DELETE (f%ice%work%fct_fluxes, f%ice%work%fct_plus, f%ice%work%fct_minus)
-    !$ACC EXIT DATA DELETE (f%ice%work%eps11, f%ice%work%eps12, f%ice%work%eps22)
-    !$ACC EXIT DATA DELETE (f%ice%work%sigma11, f%ice%work%sigma12, f%ice%work%sigma22)
-    !$ACC EXIT DATA DELETE (f%ice%work%ice_strength, f%ice%stress_atmice_x, f%ice%stress_atmice_y)
-    !$ACC EXIT DATA DELETE (f%ice%thermo%rhosno, f%ice%thermo%rhoice, f%ice%thermo%inv_rhowat)
-    !$ACC EXIT DATA DELETE (f%ice%srfoce_ssh, f%ice%pstar, f%ice%c_pressure)
-    !$ACC EXIT DATA DELETE (f%ice%work%inv_areamass, f%ice%work%inv_mass, f%ice%uice_rhs, f%ice%vice_rhs)
-    !$ACC EXIT DATA DELETE (f%ice%uice, f%ice%vice, f%ice%srfoce_u, f%ice%srfoce_v, f%ice%uice_old, f%ice%vice_old)
-    !$ACC EXIT DATA DELETE (f%ice%data(1)%values, f%ice%data(2)%values, f%ice%data(3)%values)
-    !$ACC EXIT DATA DELETE (f%ice%data(1)%valuesl, f%ice%data(2)%valuesl, f%ice%data(3)%valuesl)
-    !$ACC EXIT DATA DELETE (f%ice%data(1)%dvalues, f%ice%data(2)%dvalues, f%ice%data(3)%dvalues)
-    !$ACC EXIT DATA DELETE (f%ice%data(1)%values_rhs, f%ice%data(2)%values_rhs, f%ice%data(3)%values_rhs)
-    !$ACC EXIT DATA DELETE (f%ice%data(1)%values_div_rhs, f%ice%data(2)%values_div_rhs, f%ice%data(3)%values_div_rhs)
-#if defined (__oifs) || defined (__ifsinterface)
-    !$ACC EXIT DATA DELETE (f%ice%data(4)%values, f%ice%data(4)%valuesl, f%ice%data(4)%dvalues, f%ice%data(4)%values_rhs, f%ice%data(4)%values_div_rhs)
-#endif
-    !$ACC EXIT DATA DELETE (f%ice%data, f%ice%work, f%ice%work%fct_massmatrix)
-    !$ACC EXIT DATA DELETE (f%ice)
-    do tr_num=1, f%tracers%num_tracers
-    !$ACC EXIT DATA DELETE (f%tracers%data(tr_num)%values, f%tracers%data(tr_num)%valuesAB)
-    end do
-    !$ACC EXIT DATA DELETE (f%tracers%work%fct_ttf_min, f%tracers%work%fct_ttf_max, f%tracers%work%fct_plus, f%tracers%work%fct_minus)
-    !$ACC EXIT DATA DELETE (f%tracers%work%adv_flux_hor, f%tracers%work%adv_flux_ver, f%tracers%work%fct_LO)
-    !$ACC EXIT DATA DELETE (f%tracers%work%del_ttf_advvert, f%tracers%work%del_ttf_advhoriz, f%tracers%work%edge_up_dn_grad)
-    !$ACC EXIT DATA DELETE (f%tracers%work%del_ttf)
-    !$ACC EXIT DATA DELETE (f%tracers%data, f%tracers%work)
-    !$ACC EXIT DATA DELETE (f%dynamics%w, f%dynamics%w_e, f%dynamics%uv)
-    !$ACC EXIT DATA DELETE (f%dynamics, f%tracers)
-
-    !delete mesh and partit data.
-    !$ACC EXIT DATA DELETE (f%mesh%coriolis_node, f%mesh%nn_num, f%mesh%nn_pos) 
-    !$ACC EXIT DATA DELETE (f%mesh%ssh_stiff, f%mesh%ssh_stiff%rowptr) 
-    !$ACC EXIT DATA DELETE (f%mesh%gradient_sca, f%mesh%metric_factor, f%mesh%elem_area, f%mesh%area, f%mesh%edge2D_in) 
-    !$ACC EXIT DATA DELETE (f%mesh%elem2D_nodes, f%mesh%ulevels, f%mesh%ulevels_nod2d, f%mesh%edges, f%mesh%edge_tri) 
-    !$ACC EXIT DATA DELETE (f%mesh%helem, f%mesh%elem_cos, f%mesh%edge_cross_dxdy, f%mesh%elem2d_nodes, f%mesh%nl) 
-    !$ACC EXIT DATA DELETE (f%mesh%nlevels_nod2D, f%mesh%nod_in_elem2D, f%mesh%nod_in_elem2D_num) 
-    !$ACC EXIT DATA DELETE (f%mesh%edge_dxdy, f%mesh%nlevels, f%mesh%hnode, f%mesh%hnode_new, f%mesh%ulevels_nod2D_max) 
-    !$ACC EXIT DATA DELETE (f%mesh%zbar_3d_n, f%mesh%z_3d_n, f%mesh%areasvol, f%mesh%nlevels_nod2D_min) 
-    !$ACC EXIT DATA DELETE (f%partit%eDim_nod2D, f%partit%myDim_edge2D) 
-    !$ACC EXIT DATA DELETE (f%partit%myDim_elem2D, f%partit%myDim_nod2D, f%partit%myList_edge2D) 
-    !$ACC EXIT DATA DELETE (f%mesh, f%partit, f)
     if (f%mype==0) then
        f%t1 = MPI_Wtime()
        f%runtime_alltimesteps = real(f%t1-f%t0,real32)
@@ -677,10 +588,6 @@ contains
     ! OpenIFS coupled version has to call oasis_terminate through par_ex
     call par_ex(f%partit%MPI_COMM_FESOM, f%partit%mype)
 #endif
-
-#if defined(__MULTIO) && !defined(__ifsinterface) && !defined(__oasis)
-   call mpp_stop
-#endif
     if(f%fesom_did_mpi_init) call par_ex(f%partit%MPI_COMM_FESOM, f%partit%mype) ! finalize MPI before FESOM prints its stats block, otherwise there is sometimes output from other processes from an earlier time in the programm AFTER the starts block (with parastationMPI)
     if (f%mype==0) then
         41 format (a35,a10,2a15) !Format for table heading
diff --git a/src/fortran_utils.F90 b/src/fortran_utils.F90
index 35efdb74..1ebd6232 100644
--- a/src/fortran_utils.F90
+++ b/src/fortran_utils.F90
@@ -27,7 +27,7 @@ contains
     ! EO parameters
     integer w, val_width
     character(:), allocatable :: widthtxt
-
+    
     if(val == 0) then
       val_width = 1
     else
@@ -39,8 +39,8 @@ contains
     allocate(character(w) :: txt)
     write(txt,'(i0.'//widthtxt//')') val
   end function
-
-
+  
+  
   function mpirank_to_txt(mpicomm) result(txt)
     integer, intent(in) :: mpicomm
     character(:), allocatable :: txt
@@ -49,13 +49,13 @@ contains
     integer npes
     integer mpierr
     include 'mpif.h'
-
+  
     call MPI_Comm_Rank(mpicomm, mype, mpierr)
     call MPI_Comm_Size(mpicomm, npes, mpierr)
     txt = int_to_txt_pad(mype,int(log10(real(npes)))+1) ! pad to the width of the number of processes
   end function
-
-
+  
+  
   ! using EXECUTE_COMMAND_LINE to call mkdir sometimes fail (EXECUTE_COMMAND_LINE is forked as an new process, which may be the problem)
   ! try to use the C mkdir as an alternative
   subroutine mkdir(path)
@@ -73,7 +73,7 @@ contains
         integer(c_int), value :: mode
       end function
     end interface
-
+      
     pathcopy = path ! we need to pass an array of c_char to the C funcktion (this is not a correct type conversion, but Fortran characters seem to be of the same kind as c_char)
     ! result is 0 if the dir has been created from this call, otherwise -1
     ! the mode will not exactly be what we pass here, as it is subtracted by the umask bits (and possibly more)
diff --git a/src/fvom_init.F90 b/src/fvom_init.F90
index 30ee8f5f..77d481f1 100755
--- a/src/fvom_init.F90
+++ b/src/fvom_init.F90
@@ -888,7 +888,7 @@ subroutine find_levels(mesh)
                     !___________________________________________________________
                     ! loop over neighbouring triangles
                     do i=1,nneighb
-                        if (elems(i)>0) then
+                        if (elems(i)>1) then
                             if (nlevels(elems(i))>=nz) then
                                 !count neighbours
                                 count_neighb_open=count_neighb_open+1
@@ -974,14 +974,12 @@ subroutine find_levels_cavity(mesh)
     use g_config
     implicit none
     integer        :: nodes(3), elems(3)
-    integer        :: elem, node, nz, j
-    integer        :: min_nlvl, max_ulvl, idx, idx2, val, val2
+    integer        :: elem, node, nz, j, idx
     integer        :: count_neighb_open, nneighb, cavity_maxlev, count_isoelem
-    integer        :: exit_flag1, count_iter, max_iter=1000, exit_flag2, count_iter2, max_iter2=25
+    integer        :: exit_flag1, count_iter, max_iter=1000, exit_flag2, count_iter2, max_iter2=10
     real(kind=WP)  :: dmean
     character(MAX_PATH) :: file_name
-    integer, allocatable, dimension(:)   :: aux_arr, aux_idx 
-    integer, allocatable, dimension(:)   :: numelemtonode
+    integer, allocatable, dimension(:)   :: numelemtonode, idxelemtonode
     logical, allocatable, dimension(:)   :: elemreducelvl, elemfixlvl
     type(t_mesh), intent(inout), target  :: mesh
 #include "associate_mesh_ini.h"
@@ -1036,17 +1034,16 @@ subroutine find_levels_cavity(mesh)
     ! write out elemental cavity-ocean boundary level
     file_name=trim(meshpath)//'cavity_elvls_raw.out'
     open(20, file=file_name)
-    do elem=1,elem2D
-        write(20,*) ulevels(elem)
-    enddo
-    close(20) 
-    
+      do elem=1,elem2D
+         write(20,*) ulevels(elem)
+      enddo
+    close(20)   
     !___________________________________________________________________________
     ! Eliminate cells that have two cavity boundary faces --> should not be 
     ! possible in FESOM2.0
     ! loop over all cavity levels
     allocate(elemreducelvl(elem2d),elemfixlvl(elem2d))
-    allocate(numelemtonode(nl))
+    allocate(numelemtonode(nl),idxelemtonode(nl))
         
     !___________________________________________________________________________
     ! outer iteration loop    
@@ -1077,21 +1074,14 @@ subroutine find_levels_cavity(mesh)
                     !   .___________________________.~~~~~~~~~~~~~~~~~~~~~~~~~~
                     !   |###|###|###|###|###|###|###|
                     !   |#  CAVITY  |###| . |###|###|                    OCEAN
-                    !   |###|###|###|###|/|\|###| 
+                    !   |###|###|###|    /|\|###| 
                     !   |###|###|         |
                     !   |###|             +-- Not good can lead to isolated cells  
                     !
-                    
-                    !___________________________________________________________
-                    ! (1st) Ask the Question: is nz at element elem an here an 
-                    ! valid layer in the ocean
                     if ( nz >= ulevels(elem) .and. nz<nlevels(elem)) then
                         count_neighb_open=0
                         
                         !_______________________________________________________
-                        ! (2nd) loop over the neighbouring elements of this valid ocean 
-                        ! prism and check if their are further valid ocean element
-                        ! neighbours at this level nz
                         ! loop over neighbouring triangles
                         do j = 1, nneighb
                             if (elems(j)>0) then ! if its a valid boundary triangle, 0=missing value
@@ -1105,14 +1095,13 @@ subroutine find_levels_cavity(mesh)
                         end do ! --> do i = 1, nneighb
                         
                         !_______________________________________________________
-                        ! (3rd) check how many open faces to neighboring triangles the cell 
+                        ! check how many open faces to neighboring triangles the cell 
                         ! has, if there are less than 2 its isolated (a cell should 
                         ! have at least 2 valid neighbours)
                         ! --> in this case shift cavity-ocean interface one level down
-                        if (count_neighb_open<=1) then
+                        if (count_neighb_open<2) then
                             count_isoelem = count_isoelem+1
-                            
-                            ! (4th.1 ) if cell elem is isolated convert it to a deeper ocean level
+                            ! if cell is isolated convert it to a deeper ocean levels
                             ! except when this levels would remain less than 3 valid 
                             ! bottom levels --> in case make the levels of all sorounding
                             ! triangles shallower
@@ -1121,45 +1110,31 @@ subroutine find_levels_cavity(mesh)
                                  (elemfixlvl(   elem) .eqv. .False.)       &
                                 ) then 
                                 ulevels(elem)=nz+1
-                                
-                            ! (4th.2) can not increase depth anymore to eleminate 
-                            ! isolated cell, otherwise lessthan 3 valid layers 
-                            ! therefor reduce depth of ONE!!! of the neighbouring 
-                            ! triangles. Choose triangle whos depth is already closest
-                            ! to nz    
-                            else 
-                                !PS replace this with do j=1,3... because of 
-                                !PS indice -999 conflict in elems, ulevels(-999)
-                                !PS not possible 
-                                !PS idx = minloc(ulevels(elems)-nz, 1, MASK=( (elems>0) .and. ((ulevels(elems)-nz)>0) ) )
-                                val=nl
-                                do j = 1, 3
-                                    if (elems(j)>0) then ! if its a valid boundary triangle, 0=missing value
-                                        if (ulevels(elems(j))-nz>0 .and. ulevels(elems(j))-nz<val) then
-                                            val=ulevels(elems(j))-nz
-                                            idx=j
-                                        end if 
-                                    end if 
-                                end do ! --> do i = 1, nneighb
-                                
-                                ulevels(      elems(idx)) = nz
+                            else    
+                                ! --> can not increase depth anymore to eleminate isolated 
+                                !     cell, otherwise lessthan 3 valid layers
+                                ! --> therefor reduce depth of ONE!!! of the neighbouring 
+                                !     triangles. Choose trinagle whos depth is already closest
+                                !     to nz
+                                idx = minloc(ulevels(elems)-nz, 1, MASK=( (elems>0) .and. ((ulevels(elems)-nz)>0) ) )
+                                ulevels(elems(idx)) = nz-1
                                 elemreducelvl(elems(idx)) = .True.
                             end if    
                             
-                            ! force recheck for all current ocean cells
+                            !force recheck for all current ocean cells
                             exit_flag1=0
                         end if ! --> if (count_neighb_open<2) then
                         
                     end if ! --> if ( nz >= ulevels(elem) .and. nz<nlevels(elem)) then
                     
-                end do ! --> do elem=1,elem2D  
+                end do ! --> do elem=1,elem2D
                 
             end do ! --> do while((exit_flag==0).and.(count_iter<1000))
             write(*,"(A, I5, A, i5, A, I3)") '  -[iter ]->: ulevel, iter/maxiter=',count_iter,'/',max_iter,', nz=',nz
         end do ! --> do nz=1,cavity_maxlev 
         
         !_______________________________________________________________________
-        ! compute vertical vertice level index of cavity_ocean boundary
+        ! vertical vertice level index of cavity_ocean boundary
         write(*,"(A)"                  ) '  -[compu]->: ulevels_nod2D '
         ulevels_nod2D = nl
         do elem=1,elem2D
@@ -1173,26 +1148,33 @@ subroutine find_levels_cavity(mesh)
         end do ! --> do elem=1,elem2D
         
         !_______________________________________________________________________
-        ! check if all constrains for ulevel and ulevels_nod2D is fullfilled
+        ! check ulevels if ulevels<nlevels everywhere !
         exit_flag2 = 1
+        
         do elem=1,elem2D
             if (ulevels(elem)>=nlevels(elem)) then 
                 write(*,*) ' -[check]->: elem cavity depth deeper or equal bottom depth, elem=',elem                
                 exit_flag2 = 0
+                
             end if 
             
             if (nlevels(elem)-ulevels(elem)<3) then 
                 write(*,*) ' -[check]->: less than three valid elem ocean layers, elem=',elem    
                 exit_flag2 = 0
+                
             end if 
         end do ! --> do elem=1,elem2D
         
+        !_______________________________________________________________________
+        ! check ulevels_nod2d if ulevels_nod2d<nlevels_nod2d everywhere !
         do node=1,nod2D
+            !___________________________________________________________________
             if (ulevels_nod2D(node)>=nlevels_nod2D(node)) then 
                 write(*,*) ' -[check]->: vertice cavity depth deeper or equal bottom depth, node=', node
                 exit_flag2 = 0
             end if
             
+            !___________________________________________________________________
             if (nlevels_nod2D(node)-ulevels_nod2D(node)<3) then 
                 write(*,*) ' -[check]->: less than three valid vertice ocean layers, node=', node
                 exit_flag2 = 0
@@ -1207,140 +1189,62 @@ subroutine find_levels_cavity(mesh)
         end do ! --> do elem=1,elem2D
         
         !_______________________________________________________________________
-        ! compute how many triangle elements contribute to every vertice in every 
-        ! layer
-        !
-        ! --> What can happen is that a node point in the middle of the vertical 
-        !     domain can become isolated due to the cavity constrains. The model
-        !     would not be able to deal with this kind of situation. So we must 
-        !     prevent it by adapting ulevels!
-        !                                 O  node 
-        !                                _._ 
-        !                              _/ | \_
-        !                            _/   |   \_
-        !                          _/     |     \_
-        !                      elem(1)  elem(2)   elem(3)...  <--elem=nod_in_elem2D(j,node)
-        !                             ._______. ulevel(elem2)=30
-        !                             |_______|
-        !                             |_______|
-        !                             |_______|
-        !                             |_______|
-        !                             |_______| nlevel(elem2)=38
-        !
-        !                 In this possible gap node points
-        !                 would have no neighboring elements
-        !                 
-        !   ulevel(elem1)=42 ._______.         ._______. ulevel(elem3)=42
-        !                    |_______|         |_______|
-        !                    |_______|         |_______|
-        !                    |_______|         |_______|
-        !                    |_______|         |_______|
-        !   nlevel(elem1)=46 |_______|         |_______|
-        !                                      |_______| nlevel(elem3)=48
-        !
-        ! --> Problem here is we want to keep nlevels fixed so what we can do is
-        !     to set ulevels(elem1) and ulevels(elem3) towards nlevel(elem2)
+        ! compute how many triangle elements contribute to every vertice in every layer
         count_iter=0
         do node=1, nod2D
             !___________________________________________________________________
-            ! check if there is a possible gap as described above that would
-            ! allow for node points without neighbors
-            min_nlvl = nl
-            max_ulvl = 1
-            do j=1, nod_in_elem2D_num(node)
-                elem=nod_in_elem2D(j, node)
-                min_nlvl = min(min_nlvl, nlevels(elem))
-                max_ulvl = max(max_ulvl, ulevels(elem))
-            end do    
+            numelemtonode=0
+            idxelemtonode=0
             
-            ! found a potential gap
-            if (min_nlvl < max_ulvl) then 
+            !___________________________________________________________________
+            ! compute how many triangle elements contribute to vertice in every layer
+            do j=1,nod_in_elem2D_num(node)
+                elem=nod_in_elem2D(j,node)
+                do nz=ulevels(elem),nlevels(elem)-1
+                    numelemtonode(nz) = numelemtonode(nz) + 1
+                    idxelemtonode(nz) = elem
+                end do
+            end do
+            
+            !___________________________________________________________________
+            ! check if every vertice in every layer should be connected to at least 
+            ! two triangle elements !
+            do nz = ulevels_nod2D(node), nlevels_nod2D(node)-1
                 
                 !_______________________________________________________________
-                ! compute how many triangle elements contribute to vertice in 
-                ! every layer check if there are layers where a node point has 
-                ! only one or even zero neighboring triangles.
-                numelemtonode=0
-                do j=1, nod_in_elem2D_num(node)
-                    elem=nod_in_elem2D(j, node)
-                    do nz=ulevels(elem), nlevels(elem)-1
-                        numelemtonode(nz) = numelemtonode(nz) + 1
+                ! nodes has zero neighbouring triangles and is completely isolated
+                ! need to adapt ulevels by hand --> inflicts another outher 
+                ! iteration loop (exit_flag2=0)
+                if (numelemtonode(nz)==0) then 
+                    exit_flag2 = 0
+                    count_iter = count_iter+1
+                    write(*,"( A, I1, A, I7, A, I3)") '  -[check]->: node has only ', numelemtonode(nz) ,' triangle: n=', node, ', nz=',nz
+                    !___________________________________________________________
+                    ! if node has no neighboring triangle somewhere in the middle 
+                    ! of the water column at nz (can happen but seldom) than set 
+                    ! all ulevels(elem) of sorounding trinagles whos ulevel is 
+                    ! depper than nz, equal to nz and fix that value to forbit it
+                    ! to be changed (elemfixlvl > 0)
+                    do j=1,nod_in_elem2D_num(node)
+                        elem=nod_in_elem2D(j,node)
+                        if (ulevels(elem)>nz) then
+                            ulevels(elem) = nz
+                            elemfixlvl(elem) = .True.
+                        end if     
                     end do
-                end do
+                end if
                 
                 !_______________________________________________________________
-                ! check in which depth level is an isolated node 
-                nzloop: do nz = ulevels_nod2D(node), nlevels_nod2D(node)-1
-                    !___________________________________________________________
-                    ! nodes has zero neighbouring triangles and is completely 
-                    ! isolated need to adapt ulevels --> inflicts another 
-                    ! outher iteration loop (exit_flag2=0). It needs at least 
-                    ! two neighboring elements so the node is considered as 
-                    ! connected. Search the index of the two elements where ulevels>nz
-                    ! but that are closest to nz
-                    if (numelemtonode(nz)==0) then 
-                        count_iter = count_iter+1
-                        write(*,"( A, I1, A, I7, A, I3)") '  -[check]->: node has only ', numelemtonode(nz) ,' neighb. triangle: n=', node, ', nz=',nz
-                        !_______________________________________________________
-                        allocate(aux_arr(nod_in_elem2D_num(node)), aux_idx(nod_in_elem2D_num(node)))
-                        aux_arr(:) = ulevels(nod_in_elem2D(1:nod_in_elem2D_num(node),node))
-                        aux_arr(:) = aux_arr(:) - nz
-                        ! fill array with index of element
-                        do j=1, nod_in_elem2D_num(node)
-                            aux_idx(j) = j
-                        end do
-                        ! index of closest elem to nz where ulevel>nz
-                        idx  = minloc(aux_arr, 1, MASK=((aux_arr>0)) )
-                        ! index of second closest elem to nz where ulevel>nz
-                        idx2 = minloc(aux_arr, 1, MASK=((aux_arr>0) .and. (aux_idx/=idx)) )
-                        deallocate(aux_arr, aux_idx)
-                        ulevels(   nod_in_elem2D(idx ,node)) = nz
-                        ulevels(   nod_in_elem2D(idx2,node)) = nz
-                        elemfixlvl(nod_in_elem2D(idx ,node)) = .True.
-                        elemfixlvl(nod_in_elem2D(idx2,node)) = .True.
-                        
-                        !_______________________________________________________
-                        ! inflict another outer iteration loop
-                        exit_flag2 = 0
-                        
-                        !_______________________________________________________
-                        ! if the upper most isolated layer is fixed all layers below should be fixed as well
-                        ! --> exit loop do nz = ulevels_nod2D(node), nlevels_nod2D(node)-1
-                        exit nzloop
-                        
-                    !___________________________________________________________
-                    ! nodes has one neighbouring triangles it needs at least 
-                    ! another neighboring elements so the node is considered as 
-                    ! connected     
-                    elseif (numelemtonode(nz)==1) then
-                        count_iter = count_iter+1
-                        write(*,"( A, I1, A, I7, A, I3)") '  -[check]->: node has only ', numelemtonode(nz) ,'neighb. triangle: n=', node, ', nz=',nz
-                        !_______________________________________________________
-                        allocate(aux_arr(nod_in_elem2D_num(node)), aux_idx(nod_in_elem2D_num(node)))
-                        aux_arr(:) = ulevels(nod_in_elem2D(1:nod_in_elem2D_num(node),node))
-                        aux_arr(:) = aux_arr(:) - nz
-                        ! fill array with index of element
-                        do j=1, nod_in_elem2D_num(node)
-                            aux_idx(j) = j
-                        end do
-                        ! index of closest elem to nz where ulevel>nz
-                        idx  = minloc(aux_arr, 1, MASK=((aux_arr>0)) )
-                        deallocate(aux_arr, aux_idx)
-                        ulevels(   nod_in_elem2D(idx,node)) = nz
-                        elemfixlvl(nod_in_elem2D(idx,node)) = .True.
-                        
-                        !_______________________________________________________
-                        ! inflict another outer iteration loop
-                        exit_flag2 = 0
-                        
-                        !_______________________________________________________
-                        ! if the upper most isolated layer is fixed all layers below should be fixed as well
-                        ! --> exit loop do nz = ulevels_nod2D(node), nlevels_nod2D(node)-1
-                        exit nzloop
-                    
-                    end if 
-                end do nzloop ! --> do nz = ulevels_nod2D(node), nlevels_nod2D(node)-1
-            end if ! --> if (min_nlvl < max_ulvl) then 
+                ! nodes has just one neighbouring triangle --> but needs two -->
+                ! inflicts another outher iteration loop (exit_flag2=0)
+                if (numelemtonode(nz)==1) then 
+                    exit_flag2 = 0
+                    count_iter = count_iter+1
+                    write(*,"( A, I1, A, I7, A, I3)") '  -[check]->: node has only ', numelemtonode(nz) ,' triangle: n=', node, ', nz=',nz
+                end if 
+                
+            end do ! --> do nz = ulevels_nod2D(node), nlevels_nod2D(node)-1
+            
         end do ! --> do node=1, nod2D
         
         !_______________________________________________________________________
@@ -1353,9 +1257,9 @@ subroutine find_levels_cavity(mesh)
         end if     
         
         !_______________________________________________________________________
-    end do ! --> do while((exit_flag2==0) .and. (count_iter2<max_iter2))
+    end do
     deallocate(elemreducelvl,elemfixlvl)
-    deallocate(numelemtonode)
+    deallocate(numelemtonode,idxelemtonode)
     
     !___________________________________________________________________________
     ! check if cavity geometry totaly converged or failed to converge in the later
@@ -1372,25 +1276,27 @@ subroutine find_levels_cavity(mesh)
         print *, ' -['//achar(27)//'[7;32m'//' OK  '//achar(27)//'[0m'//']->: Cavity geometry constrains did converge !!! *\(^o^)/*'
     end if     
  
+
     !___________________________________________________________________________
     ! write out cavity mesh files for vertice and elemental position of 
     ! vertical cavity-ocean boundary
     ! write out elemental cavity-ocean boundary level
     file_name=trim(meshpath)//'cavity_elvls.out'
     open(20, file=file_name)
-    do elem=1,elem2D
-        write(20,*) ulevels(elem)
-    enddo
+      do elem=1,elem2D
+         write(20,*) ulevels(elem)
+      enddo
     close(20)
     
     ! write out vertice cavity-ocean boundary level + yes/no cavity flag
     file_name=trim(meshpath)//'cavity_nlvls.out'
     open(20, file=file_name)
-    do node=1,nod2D
-        write(20,*) ulevels_nod2D(node)
-    enddo
+      do node=1,nod2D
+         write(20,*) ulevels_nod2D(node)
+      enddo
     close(20)
 
+
 end subroutine find_levels_cavity
 
 
diff --git a/src/gen_forcing_couple.F90 b/src/gen_forcing_couple.F90
index be12855c..fead4f55 100755
--- a/src/gen_forcing_couple.F90
+++ b/src/gen_forcing_couple.F90
@@ -211,7 +211,7 @@ subroutine update_atm_forcing(istep, ice, tracers, dynamics, partit, mesh)
               end do
             else    
             print *, 'not installed yet or error in cpl_oasis3mct_send', mype
-#else   ! oifs
+#else
             ! AWI-CM2 outgoing state vectors
             do n=1,myDim_nod2D+eDim_nod2D
             exchange(n)=tracers%data(1)%values(1, n)                     ! sea surface temperature [°C]
@@ -267,14 +267,14 @@ subroutine update_atm_forcing(istep, ice, tracers, dynamics, partit, mesh)
 !---wiso-code-end
             else	    
             print *, 'not installed yet or error in cpl_oasis3mct_send', mype
-#endif  ! oifs
+#endif
          endif
          call cpl_oasis3mct_send(i, exchange, action, partit)
-      end do
+      enddo
 #ifdef VERBOSE
       do i=1, nsend 
         if (mype==0) write(*,*) 'SEND: field ', i, ' max val:', maxval(exchange), ' . ACTION? ', action 
-      end do
+      enddo
 #endif
       mask=1.
       do i=1,nrecv
@@ -373,7 +373,7 @@ subroutine update_atm_forcing(istep, ice, tracers, dynamics, partit, mesh)
     	          mask=1.
 	              call force_flux_consv(enthalpyoffuse, mask, i, 0, action, partit, mesh)
              end if
-#else ! oifs
+#else
          elseif (i.eq.13) then
             if (action) then
                  if (lwiso) then         
@@ -463,8 +463,8 @@ subroutine update_atm_forcing(istep, ice, tracers, dynamics, partit, mesh)
              if (use_icebergs.and.lwiso) then    
                  call force_flux_consv(v_wind, mask, i, 0, action, partit, mesh)
              end if
-#endif !   oifs
          end if
+# endif
 
 #ifdef VERBOSE
 	  if (mype==0) then
@@ -574,6 +574,15 @@ subroutine update_atm_forcing(istep, ice, tracers, dynamics, partit, mesh)
 #endif /* skip all in case of __ifsinterface */
 #endif /* (__oasis) */
 
+  !! PSong: Antarctica runoff masked (lat<-60)
+  if (use_cavity) then
+     do i=1, myDim_nod2D+eDim_nod2D
+        if (geo_coord_nod2D(2,i) < -60.0*rad) then
+           runoff(i) = 0.0_WP
+        end if
+     end do
+  end if
+
   t2=MPI_Wtime()
 
 #ifdef VERBOSE
diff --git a/src/gen_forcing_init.F90 b/src/gen_forcing_init.F90
index 70334985..789841ec 100755
--- a/src/gen_forcing_init.F90
+++ b/src/gen_forcing_init.F90
@@ -145,6 +145,8 @@ subroutine forcing_array_setup(partit, mesh)
   !---wiso-code-end
 #endif 
 
+
+
 ! Temp storage for averaging
 !!PS   allocate(aver_temp(n2))
 
diff --git a/src/gen_halo_exchange.F90 b/src/gen_halo_exchange.F90
index df75e909..c1dbd3ea 100755
--- a/src/gen_halo_exchange.F90
+++ b/src/gen_halo_exchange.F90
@@ -1,13 +1,13 @@
 ! ========================================================================
 ! Halo exchange routines + broadcast routines that collect information
 ! on the entire field (needed for output)
-! The routines here are very similar, difference is the data type and
+! The routines here are very similar, difference is the data type and  
 ! exchange pattern.
 ! exchange_nod2D_i(arr(myDim_nod2D+eDim_nod2D))    INTEGER
 ! exchange_nod2D(arr(myDim_nod2D+eDim_nod2D))      WP
 ! exchange_nod3D(arr(nl-1,myDim_nod2D+eDim_nod2D)) WP
 ! exchange_nod3D_full(arr(nl,myDim_nod2D+eDim_nod2D)) WP
-! exchange_edge2D(edge_array2D)     WP  not used currently  !!! no buffer!!!
+! exchange_edge2D(edge_array2D)     WP  not used currently  !!! no buffer!!!  
 ! exchange_edge3D(edge_array3D)     WP  not used currently  !!! no buffer!!!
 ! exchange_elem3D(elem_array3D)     WP
 ! exchange_elem2d_full
@@ -16,7 +16,7 @@
 
 module g_comm
 
-  use, intrinsic :: ISO_FORTRAN_ENV, only: int16, int32, real32, real64
+  use, intrinsic :: ISO_FORTRAN_ENV
 
   implicit none
 
@@ -35,7 +35,7 @@ integer,        intent(in)         :: sn, rn, r_mpitype(:), s_mpitype(:), rPE(:)
 integer                            :: n, sdebug, rdebug, status(MPI_STATUS_SIZE), request
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
-DO n=1,rn
+DO n=1,rn    
    CALL MPI_TYPE_SIZE(r_mpitype(n), rdebug, MPIerr)
    CALL MPI_ISEND(rdebug, 1, MPI_INTEGER, rPE(n), 10, MPI_COMM_FESOM, request, MPIerr)
 END DO
@@ -55,25 +55,24 @@ END SUBROUTINE check_mpi_comm
 #endif
 
 
-subroutine exchange_nod2D_i(nod_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_i(nod_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
 integer,        intent(inout)         :: nod_array2D(:)
-logical,        intent(in),optional   :: luse_g2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 if (npes > 1) then
-    call exchange_nod2D_i_begin(nod_array2D, partit, luse_g2g)
+    call exchange_nod2D_i_begin(nod_array2D, partit)
     call exchange_nod_end(partit)
 endif
 END SUBROUTINE exchange_nod2D_i
 
 !=============================================================================
 ! General version of the communication routine for 2D nodal fields
-subroutine exchange_nod2D_i_begin(nod_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_i_begin(nod_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -81,17 +80,9 @@ IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
 integer,        intent(inout)         :: nod_array2D(:)
 integer                               :: n, sn, rn
-logical,        intent(in),optional   :: luse_g2g
-logical                               :: lg2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
   if (npes > 1) then
 
      sn=com_nod2D%sPEnum
@@ -102,37 +93,19 @@ end if
      call check_mpi_comm(rn, sn, r_mpitype_nod2D_i, s_mpitype_nod2D_i,         &
           com_nod2D%rPE, com_nod2D%sPE)
 #endif
-     if (lg2g) then
 
+     DO n=1,rn    
 
-     !$ACC HOST_DATA USE_DEVICE(nod_array2D)
-         DO n=1,rn
+        call MPI_IRECV(nod_array2D, 1, r_mpitype_nod2D_i(n), com_nod2D%rPE(n), &
+             com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr) 
+     END DO
 
-            call MPI_IRECV(nod_array2D, 1, r_mpitype_nod2D_i(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-         END DO
-
-         DO n=1, sn
+     DO n=1, sn
 
-            call MPI_ISEND(nod_array2D, 1, s_mpitype_nod2D_i(n), com_nod2D%sPE(n), &
-                 mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-         END DO
-         !$ACC END HOST_DATA
-     else
+        call MPI_ISEND(nod_array2D, 1, s_mpitype_nod2D_i(n), com_nod2D%sPE(n), &
+             mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
+     END DO
 
-         DO n=1,rn
-    
-            call MPI_IRECV(nod_array2D, 1, r_mpitype_nod2D_i(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-         END DO
-    
-         DO n=1, sn
-    
-            call MPI_ISEND(nod_array2D, 1, s_mpitype_nod2D_i(n), com_nod2D%sPE(n), &
-                 mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-         END DO
-
-     endif
      com_nod2D%nreq = rn+sn
 
   endif
@@ -140,27 +113,26 @@ END SUBROUTINE exchange_nod2D_i_begin
 
 ! ========================================================================
 ! General version of the communication routine for 2D nodal fields
-subroutine exchange_nod2D(nod_array2D, partit, luse_g2g)
+subroutine exchange_nod2D(nod_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod_array2D(:)
-logical,        intent(in),optional   :: luse_g2g
+real(real64),   intent(inout)      :: nod_array2D(:)
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
  if (npes > 1) then
-    call exchange_nod2D_begin(nod_array2D, partit, luse_g2g)
+    call exchange_nod2D_begin(nod_array2D, partit)  
     call exchange_nod_end(partit)
  end if
-
+ 
 END SUBROUTINE exchange_nod2D
 
 ! ========================================================================
 ! General version of the communication routine for 2D nodal fields
-subroutine exchange_nod2D_begin(nod_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_begin(nod_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -168,17 +140,9 @@ IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
 real(real64),   intent(inout)         :: nod_array2D(:)
 integer                               :: n, sn, rn
-logical,        intent(in),optional   :: luse_g2g
-logical                               :: lg2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
   if (npes > 1) then
 
      sn=com_nod2D%sPEnum
@@ -190,30 +154,15 @@ end if
           com_nod2D%rPE, com_nod2D%sPE)
 #endif
 
-     if(lg2g) then
-         !$ACC HOST_DATA USE_DEVICE(nod_array2D)
-
-         DO n=1,rn
-            call MPI_IRECV(nod_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-         END DO
-         DO n=1, sn
-            call MPI_ISEND(nod_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                 mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-         END DO
-
-        !$ACC END HOST_DATA
+     DO n=1,rn         
+        call MPI_IRECV(nod_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
+             com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr) 
+     END DO
+     DO n=1, sn
+        call MPI_ISEND(nod_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
+             mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
+     END DO
 
-     else 
-         DO n=1,rn
-            call MPI_IRECV(nod_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-         END DO
-         DO n=1, sn
-            call MPI_ISEND(nod_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                 mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-         END DO
-     endif
      com_nod2D%nreq = rn+sn
 
   end if
@@ -221,28 +170,28 @@ end if
 END SUBROUTINE exchange_nod2D_begin
 !===============================================
 ! General version of the communication routine for 2D nodal fields
-subroutine exchange_nod2D_2fields(nod1_array2D, nod2_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_2fields(nod1_array2D, nod2_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod1_array2D(:)
-real(real64),   intent(inout)         :: nod2_array2D(:)
-logical,        intent(in),optional   :: luse_g2g
+real(real64),   intent(inout)      :: nod1_array2D(:)
+real(real64),   intent(inout)      :: nod2_array2D(:)
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
+
  if (npes > 1) then
-    call exchange_nod2D_2fields_begin(nod1_array2D, nod2_array2D, partit, luse_g2g)
+    call exchange_nod2D_2fields_begin(nod1_array2D, nod2_array2D, partit)  
     call exchange_nod_end(partit)
  end if
-
+ 
 END SUBROUTINE exchange_nod2D_2fields
 
 ! ========================================================================
 ! General version of the communication routine for 2D nodal fields
-subroutine exchange_nod2D_2fields_begin(nod1_array2D, nod2_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_2fields_begin(nod1_array2D, nod2_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -251,17 +200,9 @@ type(t_partit), intent(inout), target :: partit
 real(real64),   intent(inout)         :: nod1_array2D(:)
 real(real64),   intent(inout)         :: nod2_array2D(:)
 integer                               :: n, sn, rn
-logical,        intent(in),optional   :: luse_g2g
-logical                               :: lg2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
 if (npes > 1) then
 
   sn=com_nod2D%sPEnum
@@ -272,94 +213,65 @@ if (npes > 1) then
      call check_mpi_comm(rn, sn, r_mpitype_nod2D, s_mpitype_nod2D,           &
           com_nod2D%rPE, com_nod2D%sPE)
 #endif
-  if (lg2g) then 
-    !$ACC HOST_DATA USE_DEVICE(nod1_array2D, nod2_array2D) 
-  
-    DO n=1,rn
-       call MPI_IRECV(nod1_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n),      MPI_COMM_FESOM, com_nod2D%req(2*n-1), MPIerr)
-  
-       call MPI_IRECV(nod2_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n)+npes, MPI_COMM_FESOM, com_nod2D%req(2*n),   MPIerr)
-    END DO
-    DO n=1, sn
-       call MPI_ISEND(nod1_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype,      MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n-1), MPIerr)
-  
-       call MPI_ISEND(nod2_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype+npes, MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n),   MPIerr)
-    END DO
-  
-    !$ACC END HOST_DATA
-  else  
-    DO n=1,rn
-       call MPI_IRECV(nod1_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n),      MPI_COMM_FESOM, com_nod2D%req(2*n-1), MPIerr)
-  
-       call MPI_IRECV(nod2_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n)+npes, MPI_COMM_FESOM, com_nod2D%req(2*n),   MPIerr)
-    END DO
-    DO n=1, sn
-       call MPI_ISEND(nod1_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype,      MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n-1), MPIerr)
-  
-       call MPI_ISEND(nod2_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype+npes, MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n),   MPIerr)
-    END DO
-  endif
-  com_nod2D%nreq = 2*(rn+sn)
 
+  DO n=1,rn         
+     call MPI_IRECV(nod1_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
+               com_nod2D%rPE(n),      MPI_COMM_FESOM, com_nod2D%req(2*n-1), MPIerr) 
+ 
+     call MPI_IRECV(nod2_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
+               com_nod2D%rPE(n)+npes, MPI_COMM_FESOM, com_nod2D%req(2*n),   MPIerr) 
+  END DO  
+  DO n=1, sn
+     call MPI_ISEND(nod1_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
+                    mype,      MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n-1), MPIerr)
 
-end if
+     call MPI_ISEND(nod2_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
+                    mype+npes, MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n),   MPIerr)
+  END DO
+
+   com_nod2D%nreq = 2*(rn+sn)
 
+end if
+ 
 END SUBROUTINE exchange_nod2D_2fields_begin
 
 !===============================================
-subroutine exchange_nod2D_3fields(nod1_array2D, nod2_array2D, nod3_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_3fields(nod1_array2D, nod2_array2D, nod3_array2D, partit)
 ! General version of the communication routine for 2D nodal fields
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod1_array2D(:)
-real(real64),   intent(inout)         :: nod2_array2D(:)
-real(real64),   intent(inout)         :: nod3_array2D(:)
-logical,        intent(in),optional   :: luse_g2g
+real(real64),   intent(inout)      :: nod1_array2D(:)
+real(real64),   intent(inout)      :: nod2_array2D(:)
+real(real64),   intent(inout)      :: nod3_array2D(:)
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
 
  if (npes > 1) then
-    call exchange_nod2D_3fields_begin(nod1_array2D, nod2_array2D, nod3_array2D, partit, luse_g2g)
+    call exchange_nod2D_3fields_begin(nod1_array2D, nod2_array2D, nod3_array2D, partit)  
     call exchange_nod_end(partit)
  end if
-
+ 
 END SUBROUTINE exchange_nod2D_3fields
 
 ! ========================================================================
-subroutine exchange_nod2D_3fields_begin(nod1_array2D, nod2_array2D, nod3_array2D, partit, luse_g2g)
+subroutine exchange_nod2D_3fields_begin(nod1_array2D, nod2_array2D, nod3_array2D, partit)
 ! General version of the communication routine for 2D nodal fields
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod1_array2D(:)
-real(real64),   intent(inout)         :: nod2_array2D(:)
-real(real64),   intent(inout)         :: nod3_array2D(:)
-integer                               :: n, sn, rn
-logical,        intent(in),optional   :: luse_g2g
-logical                               :: lg2g
+real(real64),   intent(inout)      :: nod1_array2D(:)
+real(real64),   intent(inout)      :: nod2_array2D(:)
+real(real64),   intent(inout)      :: nod3_array2D(:)
+integer                            :: n, sn, rn
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
  if (npes > 1) then
 
   sn=com_nod2D%sPEnum
@@ -371,75 +283,46 @@ end if
           com_nod2D%rPE, com_nod2D%sPE)
 #endif
 
-  if (lg2g) then
+  DO n=1,rn         
+     call MPI_IRECV(nod1_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
+               com_nod2D%rPE(n),        MPI_COMM_FESOM, com_nod2D%req(3*n-2), MPIerr) 
+ 
+     call MPI_IRECV(nod2_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
+               com_nod2D%rPE(n)+npes,   MPI_COMM_FESOM, com_nod2D%req(3*n-1), MPIerr) 
 
-   !$ACC HOST_DATA USE_DEVICE(nod1_array2D, nod2_array2D, nod3_array2D)
+     call MPI_IRECV(nod3_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
+               com_nod2D%rPE(n)+2*npes, MPI_COMM_FESOM, com_nod2D%req(3*n),   MPIerr) 
+  END DO  
+  DO n=1, sn
+     call MPI_ISEND(nod1_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
+                    mype,        MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n-2), MPIerr)
 
-    DO n=1,rn
-       call MPI_IRECV(nod1_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n),        MPI_COMM_FESOM, com_nod2D%req(3*n-2), MPIerr)
-  
-       call MPI_IRECV(nod2_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n)+npes,   MPI_COMM_FESOM, com_nod2D%req(3*n-1), MPIerr)
-  
-       call MPI_IRECV(nod3_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n)+2*npes, MPI_COMM_FESOM, com_nod2D%req(3*n),   MPIerr)
-    END DO
-    DO n=1, sn
-       call MPI_ISEND(nod1_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype,        MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n-2), MPIerr)
-  
-       call MPI_ISEND(nod2_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype+npes,   MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n-1), MPIerr)
-  
-       call MPI_ISEND(nod3_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype+2*npes, MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n),   MPIerr)
-    END DO
-  
-   !$ACC END HOST_DATA
-  else
-    DO n=1,rn
-       call MPI_IRECV(nod1_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n),        MPI_COMM_FESOM, com_nod2D%req(3*n-2), MPIerr)
-  
-       call MPI_IRECV(nod2_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n)+npes,   MPI_COMM_FESOM, com_nod2D%req(3*n-1), MPIerr)
-  
-       call MPI_IRECV(nod3_array2D, 1, r_mpitype_nod2D(n), com_nod2D%rPE(n), &
-                 com_nod2D%rPE(n)+2*npes, MPI_COMM_FESOM, com_nod2D%req(3*n),   MPIerr)
-    END DO
-    DO n=1, sn
-       call MPI_ISEND(nod1_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype,        MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n-2), MPIerr)
-  
-       call MPI_ISEND(nod2_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype+npes,   MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n-1), MPIerr)
-  
-       call MPI_ISEND(nod3_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
-                      mype+2*npes, MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n),   MPIerr)
-    END DO
-  
-  endif
-  com_nod2D%nreq = 3*(rn+sn)
+     call MPI_ISEND(nod2_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
+                    mype+npes,   MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n-1), MPIerr)
 
-end if
+     call MPI_ISEND(nod3_array2D, 1, s_mpitype_nod2D(n), com_nod2D%sPE(n), &
+                    mype+2*npes, MPI_COMM_FESOM, com_nod2D%req(3*rn+3*n),   MPIerr)
+  END DO
+
+   com_nod2D%nreq = 3*(rn+sn)
 
+end if
+ 
 END SUBROUTINE exchange_nod2D_3fields_begin
 
 ! ========================================================================
 ! General version of the communication routine for 3D nodal fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_nod3D(nod_array3D, partit, luse_g2g)
+subroutine exchange_nod3D(nod_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod_array3D(:,:)
-logical,        intent(in),optional   :: luse_g2g
+real(real64),   intent(inout)         :: nod_array3D(:,:) 
 
 if (partit%npes > 1) then
-   call exchange_nod3D_begin(nod_array3D, partit, luse_g2g)
+   call exchange_nod3D_begin(nod_array3D, partit)
    call exchange_nod_end(partit)
 endif
 
@@ -448,26 +331,18 @@ END SUBROUTINE exchange_nod3D
 ! ========================================================================
 ! General version of the communication routine for 3D nodal fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_nod3D_begin(nod_array3D, partit, luse_g2g)
+subroutine exchange_nod3D_begin(nod_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod_array3D(:,:)
-integer                               :: n, sn, rn
-integer                               :: nz, nl1
-logical,        intent(in),optional   :: luse_g2g
-logical                               :: lg2g
+real(real64),   intent(inout)      :: nod_array3D(:,:) 
+integer                            :: n, sn, rn
+integer                            :: nz, nl1
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
  if (npes > 1) then
     sn=com_nod2D%sPEnum
     rn=com_nod2D%rPEnum
@@ -487,30 +362,14 @@ end if
     call check_mpi_comm(rn, sn, r_mpitype_nod3D(:,nl1,1), s_mpitype_nod3D(:,nl1,1), &
          com_nod2D%rPE, com_nod2D%sPE)
 #endif
-    if(lg2g) then
-      !$ACC HOST_DATA USE_DEVICE(nod_array3D) 
-
-      DO n=1,rn
-         call MPI_IRECV(nod_array3D, 1, r_mpitype_nod3D(n,nl1,1), com_nod2D%rPE(n), &
-              com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-      END DO
-      DO n=1, sn
-         call MPI_ISEND(nod_array3D, 1, s_mpitype_nod3D(n,nl1,1), com_nod2D%sPE(n), &
-              mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-      END DO
-
-  
-      !$ACC END HOST_DATA
-    else
-      DO n=1,rn
-         call MPI_IRECV(nod_array3D, 1, r_mpitype_nod3D(n,nl1,1), com_nod2D%rPE(n), &
-              com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-      END DO
-      DO n=1, sn
-         call MPI_ISEND(nod_array3D, 1, s_mpitype_nod3D(n,nl1,1), com_nod2D%sPE(n), &
-              mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-      END DO
-    endif
+    DO n=1,rn    
+       call MPI_IRECV(nod_array3D, 1, r_mpitype_nod3D(n,nl1,1), com_nod2D%rPE(n), &
+            com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr) 
+    END DO
+    DO n=1, sn
+       call MPI_ISEND(nod_array3D, 1, s_mpitype_nod3D(n,nl1,1), com_nod2D%sPE(n), &
+            mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
+    END DO
     com_nod2D%nreq = rn+sn
 
  endif
@@ -519,26 +378,25 @@ END SUBROUTINE exchange_nod3D_begin
 ! ========================================================================
 ! General version of the communication routine for 3D nodal fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_nod3D_2fields(nod1_array3D,nod2_array3D, partit, luse_g2g)
+subroutine exchange_nod3D_2fields(nod1_array3D,nod2_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod1_array3D(:,:)
-real(real64),   intent(inout)         :: nod2_array3D(:,:)
-logical,        intent(in),optional   :: luse_g2g
+real(real64),   intent(inout)         :: nod1_array3D(:,:) 
+real(real64),   intent(inout)         :: nod2_array3D(:,:) 
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
 if (npes > 1) then
-   call exchange_nod3D_2fields_begin(nod1_array3D,nod2_array3D, partit, luse_g2g)
+   call exchange_nod3D_2fields_begin(nod1_array3D,nod2_array3D, partit)
    call exchange_nod_end(partit)
 endif
 END SUBROUTINE exchange_nod3D_2fields
 
 ! ========================================================================
-subroutine exchange_nod3D_2fields_begin(nod1_array3D,nod2_array3D, partit, luse_g2g)
+subroutine exchange_nod3D_2fields_begin(nod1_array3D,nod2_array3D, partit)
 ! General version of the communication routine for 3D nodal fields
 ! stored in (vertical, horizontal) format
 use MOD_MESH
@@ -546,21 +404,13 @@ USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target :: partit
-real(real64),   intent(inout)         :: nod1_array3D(:,:)
-real(real64),   intent(inout)         :: nod2_array3D(:,:)
+real(real64),   intent(inout)         :: nod1_array3D(:,:) 
+real(real64),   intent(inout)         :: nod2_array3D(:,:) 
 integer                               :: n, sn, rn
 integer                               :: nz, nl1, nl2
-logical,        intent(in),optional   :: luse_g2g
-logical                               :: lg2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
  if (npes > 1) then
     sn=com_nod2D%sPEnum
     rn=com_nod2D%rPEnum
@@ -589,15 +439,12 @@ end if
          com_nod2D%rPE, com_nod2D%sPE)
 #endif
 
-  if(lg2g) then 
-     !$ACC HOST_DATA USE_DEVICE(nod1_array3D, nod2_array3D) 
-
-    DO n=1,rn
+    DO n=1,rn    
        call MPI_IRECV(nod1_array3D, 1, r_mpitype_nod3D(n,nl1,1), com_nod2D%rPE(n), &
-            com_nod2D%rPE(n),      MPI_COMM_FESOM, com_nod2D%req(2*n-1), MPIerr)
+            com_nod2D%rPE(n),      MPI_COMM_FESOM, com_nod2D%req(2*n-1), MPIerr)  
 
        call MPI_IRECV(nod2_array3D, 1, r_mpitype_nod3D(n,nl2,1), com_nod2D%rPE(n), &
-            com_nod2D%rPE(n)+npes, MPI_COMM_FESOM, com_nod2D%req(2*n  ), MPIerr)
+            com_nod2D%rPE(n)+npes, MPI_COMM_FESOM, com_nod2D%req(2*n  ), MPIerr) 
     END DO
 
     DO n=1, sn
@@ -608,40 +455,20 @@ end if
             mype+npes, MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n), MPIerr)
     END DO
 
-     !$ACC END HOST_DATA
-  else
-    DO n=1,rn
-       call MPI_IRECV(nod1_array3D, 1, r_mpitype_nod3D(n,nl1,1), com_nod2D%rPE(n), &
-            com_nod2D%rPE(n),      MPI_COMM_FESOM, com_nod2D%req(2*n-1), MPIerr)
-
-       call MPI_IRECV(nod2_array3D, 1, r_mpitype_nod3D(n,nl2,1), com_nod2D%rPE(n), &
-            com_nod2D%rPE(n)+npes, MPI_COMM_FESOM, com_nod2D%req(2*n  ), MPIerr)
-    END DO
-
-    DO n=1, sn
-       call MPI_ISEND(nod1_array3D, 1, s_mpitype_nod3D(n,nl1,1), com_nod2D%sPE(n), &
-            mype,      MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n-1), MPIerr)
-
-       call MPI_ISEND(nod2_array3D, 1, s_mpitype_nod3D(n,nl2,1), com_nod2D%sPE(n), &
-            mype+npes, MPI_COMM_FESOM, com_nod2D%req(2*rn+2*n), MPIerr)
-    END DO
-
-  endif
-  com_nod2D%nreq = 2*(rn+sn)
+    com_nod2D%nreq = 2*(rn+sn)
 
  endif
 END SUBROUTINE exchange_nod3D_2fields_begin
 ! ========================================================================
-subroutine exchange_nod3D_n(nod_array3D, partit, luse_g2g)
+subroutine exchange_nod3D_n(nod_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
-type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)          :: nod_array3D(:,:,:)
-logical,        intent(in),optional    :: luse_g2g
+type(t_partit), intent(inout), target :: partit
+real(real64),   intent(inout)         :: nod_array3D(:,:,:) 
 if (partit%npes>1) then
-   call exchange_nod3D_n_begin(nod_array3D, partit, luse_g2g)
+   call exchange_nod3D_n_begin(nod_array3D, partit)
    call exchange_nod_end(partit)
 endif
 
@@ -650,32 +477,23 @@ END SUBROUTINE exchange_nod3D_n
 !=================================================
 ! General version of the communication routine for 3D nodal fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_nod3D_n_begin(nod_array3D, partit, luse_g2g)
+subroutine exchange_nod3D_n_begin(nod_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)          :: nod_array3D(:,:,:)
-integer                                :: n, sn, rn
-integer                                :: nz, nl1, n_val
-logical,        intent(in),optional    :: luse_g2g
-logical                                :: lg2g
+real(real64),   intent(inout)       :: nod_array3D(:,:,:)
+integer                             :: n, sn, rn
+integer                             :: nz, nl1, n_val
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
-
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
 if (npes>1) then
  ! nod_array3D(n_val,nl1,nod2D_size)
   nl1   = ubound(nod_array3D,2)
   n_val = ubound(nod_array3D,1)
   if ((nl1<ubound(r_mpitype_nod3D, 2)-1) .or. (nl1>ubound(r_mpitype_nod3D, 2)) .or. (n_val > 3)) then
-     ! This routine also works for swapped dimensions nod_array3D(nl1,n_val, nod2D_size)
+     ! This routine also works for swapped dimensions nod_array3D(nl1,n_val, nod2D_size) 
      nl1   = ubound(nod_array3D,1)
      n_val = ubound(nod_array3D,2)
 
@@ -697,32 +515,16 @@ if (npes>1) then
        s_mpitype_nod3D(:,nl1,n_val), com_nod2D%rPE, com_nod2D%sPE)
 #endif
 
-  if(lg2g) then
-    !$ACC HOST_DATA USE_DEVICE(nod_array3D)
-  
-    DO n=1,rn
-       call MPI_IRECV(nod_array3D, 1, r_mpitype_nod3D(n,nl1,n_val), com_nod2D%rPE(n), &
-            com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-    END DO
-  
-    DO n=1, sn
-       call MPI_ISEND(nod_array3D, 1, s_mpitype_nod3D(n,nl1,n_val), com_nod2D%sPE(n), &
-            mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-    END DO
-  
-  !$ACC END HOST_DATA
+  DO n=1,rn    
+     call MPI_IRECV(nod_array3D, 1, r_mpitype_nod3D(n,nl1,n_val), com_nod2D%rPE(n), &
+          com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr) 
+  END DO
+ 
+  DO n=1, sn
+     call MPI_ISEND(nod_array3D, 1, s_mpitype_nod3D(n,nl1,n_val), com_nod2D%sPE(n), &
+          mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
+  END DO
 
-  else
-    DO n=1,rn
-       call MPI_IRECV(nod_array3D, 1, r_mpitype_nod3D(n,nl1,n_val), com_nod2D%rPE(n), &
-            com_nod2D%rPE(n), MPI_COMM_FESOM, com_nod2D%req(n), MPIerr)
-    END DO
-  
-    DO n=1, sn
-       call MPI_ISEND(nod_array3D, 1, s_mpitype_nod3D(n,nl1,n_val), com_nod2D%sPE(n), &
-            mype, MPI_COMM_FESOM, com_nod2D%req(rn+n), MPIerr)
-    END DO
-  endif
   com_nod2D%nreq = rn+sn
 
  endif
@@ -733,7 +535,7 @@ END SUBROUTINE exchange_nod3D_n_begin
 !=======================================
 ! AND WAITING
 !=======================================
-
+ 
 SUBROUTINE exchange_nod_end(partit)
 use MOD_MESH
 USE MOD_PARTIT
@@ -758,51 +560,42 @@ type(t_partit), intent(inout), target :: partit
   if (npes > 1) then
      if (elem_full_flag) then
         call MPI_WAITALL(com_elem2D_full%nreq, &
-             com_elem2D_full%req, MPI_STATUSES_IGNORE, MPIerr)
+             com_elem2D_full%req, MPI_STATUSES_IGNORE, MPIerr)     
      else
         call MPI_WAITALL(com_elem2D%nreq, &
-             com_elem2D%req, MPI_STATUSES_IGNORE, MPIerr)
+             com_elem2D%req, MPI_STATUSES_IGNORE, MPIerr)     
      endif
   end if
 END SUBROUTINE exchange_elem_end
 !=============================================================================
-subroutine exchange_elem3D(elem_array3D, partit, luse_g2g)
+subroutine exchange_elem3D(elem_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)          :: elem_array3D(:,:)
-logical,        intent(in),optional    :: luse_g2g
+real(real64),   intent(inout)          :: elem_array3D(:,:) 
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-call exchange_elem3D_begin(elem_array3D, partit, luse_g2g)
+call exchange_elem3D_begin(elem_array3D, partit)
 call exchange_elem_end(partit)
 
 END SUBROUTINE exchange_elem3D
 !===========================================
 ! General version of the communication routine for 3D elemental fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_elem3D_begin(elem_array3D, partit, luse_g2g)
+subroutine exchange_elem3D_begin(elem_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)       :: elem_array3D(:,:)
+real(real64),   intent(inout)       :: elem_array3D(:,:) 
 integer                             :: n, sn, rn, nl1
-logical                                :: lg2g
-logical,        intent(in),optional    :: luse_g2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
 if (npes> 1) then
 
    nl1=ubound(elem_array3D,1)
@@ -822,31 +615,17 @@ if (npes> 1) then
               com_elem2D%rPE, com_elem2D%sPE)
 #endif
 
-        if(lg2g) then
-         !$ACC HOST_DATA USE_DEVICE(elem_array3D)
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D(n,nl1,1), com_elem2D%rPE(n), &
-                   com_elem2D%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D(n,nl1,1), com_elem2D%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D%req(rn+n), MPIerr)
-           END DO
-         !$ACC END HOST_DATA
-        else 
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D(n,nl1,1), com_elem2D%rPE(n), &
-                   com_elem2D%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D(n,nl1,1), com_elem2D%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D%req(rn+n), MPIerr)
-           END DO
-        endif 
+         DO n=1,rn         
+            call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D(n,nl1,1), com_elem2D%rPE(n), &
+                 com_elem2D%rPE(n), MPI_COMM_FESOM, &
+                 com_elem2D%req(n), MPIerr)
+         END DO
+         DO n=1, sn
+            call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D(n,nl1,1), com_elem2D%sPE(n), &
+                 mype,    MPI_COMM_FESOM, &
+                 com_elem2D%req(rn+n), MPIerr)
+         END DO
+
       elseif (nl1 <= 4) then
          ! In fact, this is a 2D-array with up to 4 values, e.g. derivatives
 
@@ -855,33 +634,17 @@ if (npes> 1) then
          call check_mpi_comm(rn, sn, r_mpitype_elem2D(:,nl1), s_mpitype_elem2D(:,nl1), &
               com_elem2D%rPE, com_elem2D%sPE)
 #endif
-        if(lg2g) then
-          !$ACC HOST_DATA USE_DEVICE(elem_array3D)
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem2D(n,nl1), com_elem2D%rPE(n), &
-                   com_elem2D%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem2D(n,nl1), com_elem2D%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D%req(rn+n), MPIerr)
-           END DO
-         !$ACC END HOST_DATA
-  
-        else 
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem2D(n,nl1), com_elem2D%rPE(n), &
-                   com_elem2D%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem2D(n,nl1), com_elem2D%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D%req(rn+n), MPIerr)
-           END DO
 
-        endif
+         DO n=1,rn         
+            call MPI_IRECV(elem_array3D, 1, r_mpitype_elem2D(n,nl1), com_elem2D%rPE(n), &
+                 com_elem2D%rPE(n), MPI_COMM_FESOM, &
+                 com_elem2D%req(n), MPIerr) 
+         END DO
+         DO n=1, sn
+            call MPI_ISEND(elem_array3D, 1, s_mpitype_elem2D(n,nl1), com_elem2D%sPE(n), &
+                 mype,    MPI_COMM_FESOM, &
+                 com_elem2D%req(rn+n), MPIerr)
+         END DO
       else
          if (mype==0) print *,'Sorry, no MPI datatype prepared for',nl1,'values per element (exchange_elem3D)'
          call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
@@ -903,35 +666,18 @@ if (npes> 1) then
               s_mpitype_elem3D_full(:,nl1,1), com_elem2D_full%rPE, com_elem2D_full%sPE)
 #endif
 
-         if(lg2g) then
-           !$ACC HOST_DATA USE_DEVICE(elem_array3D)
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D_full(n,nl1,1), &
-                   com_elem2D_full%rPE(n), &
-                   com_elem2D_full%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D_full%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D_full(n,nl1,1), &
-                   com_elem2D_full%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D_full%req(rn+n), MPIerr)
-           END DO
-           !$ACC END HOST_DATA
-         else
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D_full(n,nl1,1), &
-                   com_elem2D_full%rPE(n), &
-                   com_elem2D_full%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D_full%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D_full(n,nl1,1), &
-                   com_elem2D_full%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D_full%req(rn+n), MPIerr)
-           END DO
-         endif
+         DO n=1,rn         
+            call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D_full(n,nl1,1), &
+                 com_elem2D_full%rPE(n), &
+                 com_elem2D_full%rPE(n), MPI_COMM_FESOM, &
+                 com_elem2D_full%req(n), MPIerr) 
+         END DO
+         DO n=1, sn
+            call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D_full(n,nl1,1), &
+                 com_elem2D_full%sPE(n), & 
+                 mype,    MPI_COMM_FESOM, &
+                 com_elem2D_full%req(rn+n), MPIerr)
+         END DO
       elseif (nl1 <= 4) then
          ! Check MPI point-to-point communication for consistency
 #ifdef DEBUG
@@ -940,35 +686,18 @@ if (npes> 1) then
 #endif
 
          ! In fact, this is a 2D-array with up to 4 values, e.g. derivatives
-         if(lg2g) then
-           !$ACC HOST_DATA USE_DEVICE(elem_array3D)
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem2D_full(n,nl1), &
-                   com_elem2D_full%rPE(n), &
-                   com_elem2D_full%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D_full%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem2D_full(n,nl1), &
-                   com_elem2D_full%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D_full%req(rn+n), MPIerr)
-           END DO
-           !$ACC END HOST_DATA
-         else
-           DO n=1,rn
-              call MPI_IRECV(elem_array3D, 1, r_mpitype_elem2D_full(n,nl1), &
-                   com_elem2D_full%rPE(n), &
-                   com_elem2D_full%rPE(n), MPI_COMM_FESOM, &
-                   com_elem2D_full%req(n), MPIerr)
-           END DO
-           DO n=1, sn
-              call MPI_ISEND(elem_array3D, 1, s_mpitype_elem2D_full(n,nl1), &
-                   com_elem2D_full%sPE(n), &
-                   mype,    MPI_COMM_FESOM, &
-                   com_elem2D_full%req(rn+n), MPIerr)
-           END DO
-         endif
+         DO n=1,rn         
+            call MPI_IRECV(elem_array3D, 1, r_mpitype_elem2D_full(n,nl1), &
+                 com_elem2D_full%rPE(n), &
+                 com_elem2D_full%rPE(n), MPI_COMM_FESOM, &
+                 com_elem2D_full%req(n), MPIerr) 
+         END DO
+         DO n=1, sn
+            call MPI_ISEND(elem_array3D, 1, s_mpitype_elem2D_full(n,nl1), &
+                 com_elem2D_full%sPE(n), & 
+                 mype,    MPI_COMM_FESOM, &
+                 com_elem2D_full%req(rn+n), MPIerr)
+         END DO
       else
          if (mype==0) print *,'Sorry, no MPI datatype prepared for',nl1,'values per element (exchange_elem3D)'
          call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
@@ -985,24 +714,23 @@ END SUBROUTINE exchange_elem3D_begin
 !=============================================================================
 ! General version of the communication routine for 3D elemental fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_elem3D_n(elem_array3D, partit, luse_g2g)
+subroutine exchange_elem3D_n(elem_array3D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)       :: elem_array3D(:,:,:)
-logical,        intent(in),optional    :: luse_g2g
+real(real64),   intent(inout)       :: elem_array3D(:,:,:) 
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
  if (npes> 1) then
-    call exchange_elem3D_n_begin(elem_array3D, partit, luse_g2g)
+    call exchange_elem3D_n_begin(elem_array3D, partit)
     call exchange_elem_end(partit)
  endif
 END SUBROUTINE exchange_elem3D_n
 !=============================================================================
-subroutine exchange_elem3D_n_begin(elem_array3D, partit, luse_g2g)
+subroutine exchange_elem3D_n_begin(elem_array3D, partit)
 ! General version of the communication routine for 3D elemental fields
 ! stored in (vertical, horizontal) format
 use MOD_MESH
@@ -1010,26 +738,18 @@ USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)       :: elem_array3D(:,:,:)
+real(real64),   intent(inout)       :: elem_array3D(:,:,:) 
 integer                             :: n, sn, rn, n_val, nl1
-logical                                :: lg2g
-logical,        intent(in),optional    :: luse_g2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
  if (npes> 1) then
  nl1   = ubound(elem_array3D,2)
  n_val = ubound(elem_array3D,1)
-
+ 
   if ((nl1<ubound(r_mpitype_elem3D, 2)-1) .or. (nl1>ubound(r_mpitype_elem3D, 2)) .or. (n_val > 4)) then
 
-     ! This routine also works for swapped dimensions elem_array3D(nl1,n_val, elem2D_size)
+     ! This routine also works for swapped dimensions elem_array3D(nl1,n_val, elem2D_size) 
      nl1= ubound(elem_array3D,1)
      n_val = ubound(elem_array3D,2)
 
@@ -1055,31 +775,20 @@ end if
      call check_mpi_comm(rn, sn, r_mpitype_elem3D(:,nl1,n_val), &
           s_mpitype_elem3D(:,nl1,n_val), com_elem2D%rPE, com_elem2D%sPE)
 #endif
-     if(lg2g) then
-       !$ACC HOST_DATA USE_DEVICE(elem_array3D)
-       DO n=1,rn
-          call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D(n,nl1,n_val), com_elem2D%rPE(n), &
-                         com_elem2D%rPE(n), MPI_COMM_FESOM, com_elem2D%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D(n,nl1,n_val), com_elem2D%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D%req(rn+n), MPIerr)
-       END DO
-       !$ACC END HOST_DATA
-     else
-       DO n=1,rn
-          call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D(n,nl1,n_val), com_elem2D%rPE(n), &
-                         com_elem2D%rPE(n), MPI_COMM_FESOM, com_elem2D%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D(n,nl1,n_val), com_elem2D%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D%req(rn+n), MPIerr)
-       END DO
-     endif
+
+     DO n=1,rn         
+        call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D(n,nl1,n_val), com_elem2D%rPE(n), &
+                       com_elem2D%rPE(n), MPI_COMM_FESOM, com_elem2D%req(n), MPIerr) 
+     END DO
+     DO n=1, sn
+        call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D(n,nl1,n_val), com_elem2D%sPE(n), &
+                       mype, MPI_COMM_FESOM, com_elem2D%req(rn+n), MPIerr)
+     END DO
+
      com_elem2D%nreq = rn+sn
 
   else
-
+     
      elem_full_flag = .true.
 
      sn=com_elem2D_full%sPEnum
@@ -1090,78 +799,56 @@ end if
      call check_mpi_comm(rn, sn, r_mpitype_elem3D_full(:,nl1,n_val), &
           s_mpitype_elem3D_full(:,nl1,n_val), com_elem2D_full%rPE, com_elem2D_full%sPE)
 #endif
-     if(lg2g) then
-       !$ACC HOST_DATA USE_DEVICE(elem_array3D)
-
-       DO n=1,rn
-          call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D_full(n,nl1,n_val), com_elem2D_full%rPE(n), &
-                         com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D_full(n,nl1,n_val), com_elem2D_full%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
-       END DO
-       !$ACC END HOST_DATA
-     else
-       DO n=1,rn
-          call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D_full(n,nl1,n_val), com_elem2D_full%rPE(n), &
-                         com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D_full(n,nl1,n_val), com_elem2D_full%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
-       END DO
-     endif
 
-     com_elem2D_full%nreq = rn+sn
+     DO n=1,rn         
+        call MPI_IRECV(elem_array3D, 1, r_mpitype_elem3D_full(n,nl1,n_val), com_elem2D_full%rPE(n), &
+                       com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr) 
+     END DO
+     DO n=1, sn
+        call MPI_ISEND(elem_array3D, 1, s_mpitype_elem3D_full(n,nl1,n_val), com_elem2D_full%sPE(n), &
+                       mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
+     END DO
 
-  end if
+     com_elem2D_full%nreq = rn+sn
 
+  end if     
+  
 
-endif
+endif  
 END SUBROUTINE exchange_elem3D_n_begin
 !========================================================================
 ! General version of the communication routine for 3D elemental fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_elem2D(elem_array2D, partit, luse_g2g)
+subroutine exchange_elem2D(elem_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)       :: elem_array2D(:)
-logical,        intent(in),optional    :: luse_g2g
+real(real64),   intent(inout)       :: elem_array2D(:) 
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
  if (npes> 1) then
-    call exchange_elem2D_begin(elem_array2D, partit, luse_g2g)
+    call exchange_elem2D_begin(elem_array2D, partit)
     call exchange_elem_end(partit)
  end if
-
+  
 END SUBROUTINE exchange_elem2D
 !========================================================================
 ! General version of the communication routine for 3D elemental fields
 ! stored in (vertical, horizontal) format
-subroutine exchange_elem2D_begin(elem_array2D, partit, luse_g2g)
+subroutine exchange_elem2D_begin(elem_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
 IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
-real(real64),   intent(inout)       :: elem_array2D(:)
+real(real64),   intent(inout)       :: elem_array2D(:) 
 integer                             :: n, sn, rn
-logical                                :: lg2g
-logical,        intent(in),optional    :: luse_g2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
 if (npes> 1) then
 
   if (ubound(elem_array2D,1)<=myDim_elem2D+eDim_elem2D) then
@@ -1177,27 +864,15 @@ if (npes> 1) then
           com_elem2D%rPE, com_elem2D%sPE)
 #endif
 
-     if(lg2g) then
-       !$ACC HOST_DATA USE_DEVICE(elem_array2D)
-       DO n=1,rn
-          call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D(n,1), com_elem2D%rPE(n), &
-                         com_elem2D%rPE(n), MPI_COMM_FESOM, com_elem2D%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D(n,1), com_elem2D%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D%req(rn+n), MPIerr)
-       END DO
-       !$ACC END HOST_DATA
-     else
-       DO n=1,rn
-          call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D(n,1), com_elem2D%rPE(n), &
-                         com_elem2D%rPE(n), MPI_COMM_FESOM, com_elem2D%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D(n,1), com_elem2D%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D%req(rn+n), MPIerr)
-       END DO
-     endif
+     DO n=1,rn         
+        call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D(n,1), com_elem2D%rPE(n), &
+                       com_elem2D%rPE(n), MPI_COMM_FESOM, com_elem2D%req(n), MPIerr) 
+     END DO
+     DO n=1, sn
+        call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D(n,1), com_elem2D%sPE(n), &
+                       mype, MPI_COMM_FESOM, com_elem2D%req(rn+n), MPIerr)
+     END DO
+
      com_elem2D%nreq = rn+sn
 
   else
@@ -1212,38 +887,25 @@ if (npes> 1) then
           com_elem2D_full%rPE, com_elem2D_full%sPE)
 #endif
 
-     if(lg2g) then
-       !$ACC HOST_DATA USE_DEVICE(elem_array2D)
-       DO n=1,rn
-          call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D_full(n,1), com_elem2D_full%rPE(n), &
-                         com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D_full(n,1), com_elem2D_full%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
-       END DO
-       !$ACC END HOST_DATA
-     else
-       DO n=1,rn
-          call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D_full(n,1), com_elem2D_full%rPE(n), &
-                         com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr)
-       END DO
-       DO n=1, sn
-          call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D_full(n,1), com_elem2D_full%sPE(n), &
-                         mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
-       END DO
-     endif
+     DO n=1,rn         
+        call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D_full(n,1), com_elem2D_full%rPE(n), &
+                       com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr) 
+     END DO
+     DO n=1, sn
+        call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D_full(n,1), com_elem2D_full%sPE(n), &
+                       mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
+     END DO
 
      com_elem2D_full%nreq = rn+sn
 
-  end if
+  end if     
 
 end if
-
+  
 END SUBROUTINE exchange_elem2D_begin
 ! ========================================================================
 !Exchange with ALL(!) the neighbours
-subroutine exchange_elem2D_i(elem_array2D, partit, luse_g2g)
+subroutine exchange_elem2D_i(elem_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -1251,19 +913,18 @@ IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
 integer,        intent(inout)       :: elem_array2D(:)
 integer                             :: n, sn, rn
-logical,        intent(in),optional    :: luse_g2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
  if (npes> 1) then
-    call exchange_elem2D_i_begin(elem_array2D, partit, luse_g2g)
+    call exchange_elem2D_i_begin(elem_array2D, partit)
     call exchange_elem_end(partit)
 end if
 
 END SUBROUTINE exchange_elem2D_i
 !=============================================================================
 !Exchange with ALL(!) the neighbours
-subroutine exchange_elem2D_i_begin(elem_array2D, partit, luse_g2g)
+subroutine exchange_elem2D_i_begin(elem_array2D, partit)
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -1271,17 +932,9 @@ IMPLICIT NONE
 type(t_partit), intent(inout), target  :: partit
 integer,        intent(inout)       :: elem_array2D(:)
 integer                             :: n, sn, rn
-logical                                :: lg2g
-logical,        intent(in),optional    :: luse_g2g
 #include "associate_part_def.h"
 #include "associate_part_ass.h"
 
-if(present(luse_g2g)) then
-   lg2g = luse_g2g
-else
-   lg2g = .false.
-end if
-
  if (npes> 1) then
 
     elem_full_flag = .true.
@@ -1291,37 +944,23 @@ end if
 
      ! Check MPI point-to-point communication for consistency
 #ifdef DEBUG
-    call check_mpi_comm(rn, sn, r_mpitype_elem2D_full_i, s_mpitype_elem2D_full_i,        &
+     call check_mpi_comm(rn, sn, r_mpitype_elem2D_full_i, s_mpitype_elem2D_full_i,        &
           com_elem2D_full%rPE, com_elem2D_full%sPE)
 #endif
-    if(lg2g) then
-      !$ACC HOST_DATA USE_DEVICE(elem_array2D)
-      DO n=1,rn
-         call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D_full_i(n), com_elem2D_full%rPE(n), &
-              com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr)
-      END DO
-  
-      DO n=1, sn
-  
-         call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D_full_i(n), com_elem2D_full%sPE(n), &
-              mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
-      END DO
-      !$ACC END HOST_DATA
-    else
-      DO n=1,rn
-         call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D_full_i(n), com_elem2D_full%rPE(n), &
-              com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr)
-      END DO
-  
-      DO n=1, sn
-  
-         call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D_full_i(n), com_elem2D_full%sPE(n), &
-              mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
-      END DO
-    endif
 
-    com_elem2D_full%nreq = rn+sn
+    DO n=1,rn       
+       call MPI_IRECV(elem_array2D, 1, r_mpitype_elem2D_full_i(n), com_elem2D_full%rPE(n), &
+            com_elem2D_full%rPE(n), MPI_COMM_FESOM, com_elem2D_full%req(n), MPIerr) 
+    END DO
+ 
+    DO n=1, sn
+     
+       call MPI_ISEND(elem_array2D, 1, s_mpitype_elem2D_full_i(n), com_elem2D_full%sPE(n), &
+            mype, MPI_COMM_FESOM, com_elem2D_full%req(rn+n), MPIerr)
+    END DO
 
+    com_elem2D_full%nreq = rn+sn
+  
 end if
 
 END SUBROUTINE exchange_elem2D_i_begin
@@ -1566,7 +1205,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 end subroutine broadcast_elem2D
 !
 !============================================================================
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 subroutine gather_nod3D(arr3D, arr3D_global, partit)
 use MOD_MESH
@@ -1592,7 +1231,7 @@ nl1=ubound(arr3D,1)
 ! Consider MPI-datatypes to recv directly into arr3D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(nl1,ubound(arr3D_global,2)))
 
@@ -1601,11 +1240,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(1,start), n3D, MPI_DOUBLE_PRECISION, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_nod2D(1:myDim_nod2D)) = arr3D(1:nl1,1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr3D_global(1:nl1, remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1:nl1, 1 : remPtr_nod2D(npes)-1)
 
@@ -1616,9 +1255,9 @@ IF ( mype == 0 ) THEN
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr3D, myDim_nod2D*nl1, MPI_DOUBLE_PRECISION, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 end if
@@ -1628,7 +1267,7 @@ end subroutine gather_nod3D
 !
 subroutine gather_real4_nod3D(arr3D, arr3D_global, partit)
 
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 !
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 use MOD_MESH
@@ -1654,7 +1293,7 @@ nl1=ubound(arr3D,1)
 ! Consider MPI-datatypes to recv directly into arr3D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(nl1,ubound(arr3D_global,2)))
 
@@ -1663,11 +1302,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(1,start), n3D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_nod2D(1:myDim_nod2D)) = arr3D(1:nl1,1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr3D_global(1:nl1, remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1:nl1, 1 : remPtr_nod2D(npes)-1)
 
@@ -1678,9 +1317,9 @@ IF ( mype == 0 ) THEN
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr3D, myDim_nod2D*nl1, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 end if
@@ -1689,7 +1328,7 @@ end subroutine gather_real4_nod3D
 
 subroutine gather_int2_nod3D(arr3D, arr3D_global, partit)
 
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 !
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 use MOD_MESH
@@ -1716,7 +1355,7 @@ nl1=ubound(arr3D,1)
 ! Consider MPI-datatypes to recv directly into arr3D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(nl1,ubound(arr3D_global,2)))
 
@@ -1725,11 +1364,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(1,start), n3D, MPI_SHORT, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_nod2D(1:myDim_nod2D)) = arr3D(1:nl1,1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr3D_global(1:nl1, remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1:nl1, 1 : remPtr_nod2D(npes)-1)
 
@@ -1740,16 +1379,16 @@ IF ( mype == 0 ) THEN
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr3D, myDim_nod2D*nl1, MPI_SHORT, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 end if
 end subroutine gather_int2_nod3D
 !==============================================
 subroutine gather_nod2D(arr2D, arr2D_global, partit)
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -1771,7 +1410,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(ubound(arr2D_global,1)))
       do  n = 1, npes-1
@@ -1779,31 +1418,31 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(start), n2D, MPI_DOUBLE_PRECISION, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_nod2D(1:myDim_nod2D)) = arr2D(1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1 : remPtr_nod2D(npes)-1)
       deallocate(recvbuf)
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_nod2D, MPI_DOUBLE_PRECISION, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
 end subroutine gather_nod2D
 !==============================================
 subroutine gather_real4_nod2D(arr2D, arr2D_global, partit)
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -1825,7 +1464,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(ubound(arr2D_global,1)))
       do  n = 1, npes-1
@@ -1833,24 +1472,24 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(start), n2D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_nod2D(1:myDim_nod2D)) = arr2D(1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1 : remPtr_nod2D(npes)-1)
       deallocate(recvbuf)
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_nod2D, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
@@ -1880,7 +1519,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(ubound(arr2D_global,1)))
       do  n = 1, npes-1
@@ -1888,24 +1527,24 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(start), n2D, MPI_SHORT, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_nod2D(1:myDim_nod2D)) = arr2D(1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1 : remPtr_nod2D(npes)-1)
       deallocate(recvbuf)
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_nod2D, MPI_SHORT, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
@@ -1913,7 +1552,7 @@ end subroutine gather_int2_nod2D
 
 !============================================================================
 subroutine gather_elem3D(arr3D, arr3D_global, partit)
-! Make element information available to master PE
+! Make element information available to master PE 
 !
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 use MOD_MESH
@@ -1942,9 +1581,9 @@ nl1=ubound(arr3D,1)
 !  PEs at once!)
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
-!
+! 
       allocate(recvbuf(nl1,remPtr_elem2D(npes)))
 
       do  n = 1, npes-1
@@ -1952,9 +1591,9 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(1,start), e3D, MPI_DOUBLE_PRECISION, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_elem2D(1:myDim_elem2D)) = arr3D(1:nl1,1:myDim_elem2D)
-
+   
 
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
 
@@ -1968,16 +1607,16 @@ IF ( mype == 0 ) THEN
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr3D, myDim_elem2D*nl1, MPI_DOUBLE_PRECISION, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
 end subroutine gather_elem3D
 
 !===================================================================
-! Make element information available to master PE
+! Make element information available to master PE 
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 subroutine gather_real4_elem3D(arr3D, arr3D_global, partit)
 use MOD_MESH
@@ -2006,9 +1645,9 @@ nl1=ubound(arr3D,1)
 !  PEs at once!)
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
-!
+! 
       allocate(recvbuf(nl1,remPtr_elem2D(npes)))
 
       do  n = 1, npes-1
@@ -2016,9 +1655,9 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(1,start), e3D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_elem2D(1:myDim_elem2D)) = arr3D(1:nl1,1:myDim_elem2D)
-
+   
 
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
 
@@ -2032,9 +1671,9 @@ IF ( mype == 0 ) THEN
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr3D, myDim_elem2D*nl1, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
@@ -2042,7 +1681,7 @@ end subroutine gather_real4_elem3D
 
 
 !===================================================================
-! Make element information available to master PE
+! Make element information available to master PE 
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 subroutine gather_int2_elem3D(arr3D, arr3D_global, partit)
 use MOD_MESH
@@ -2071,9 +1710,9 @@ nl1=ubound(arr3D,1)
 !  PEs at once!)
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
-!
+! 
       allocate(recvbuf(nl1,remPtr_elem2D(npes)))
 
       do  n = 1, npes-1
@@ -2081,9 +1720,9 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(1,start), e3D, MPI_SHORT, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_elem2D(1:myDim_elem2D)) = arr3D(1:nl1,1:myDim_elem2D)
-
+   
 
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
 
@@ -2097,9 +1736,9 @@ IF ( mype == 0 ) THEN
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr3D, myDim_elem2D*nl1, MPI_SHORT, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
@@ -2107,7 +1746,7 @@ end subroutine gather_int2_elem3D
 
 
 !==============================================
-! Make element information available to master PE
+! Make element information available to master PE 
 subroutine gather_elem2D(arr2D, arr2D_global, partit)
 use MOD_MESH
 USE MOD_PARTIT
@@ -2129,7 +1768,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
 
       allocate(recvbuf(remPtr_elem2D(npes)))
@@ -2139,11 +1778,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(start), e2D, MPI_DOUBLE_PRECISION, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_elem2D(1:myDim_elem2D)) = arr2D(1:myDim_elem2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_elem2D(1 : remPtr_elem2D(npes)-1)) &
                        = recvbuf(1 : remPtr_elem2D(npes)-1)
 
@@ -2152,20 +1791,20 @@ IF ( mype == 0 ) THEN
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_elem2D, MPI_DOUBLE_PRECISION, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 end if
 
 end subroutine gather_elem2D
 
 !================================================
-! Make element information available to master PE
+! Make element information available to master PE 
 subroutine gather_real4_elem2D(arr2D, arr2D_global, partit)
 use MOD_MESH
 USE MOD_PARTIT
@@ -2188,7 +1827,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
 
       allocate(recvbuf(remPtr_elem2D(npes)))
@@ -2198,11 +1837,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(start), e2D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_elem2D(1:myDim_elem2D)) = arr2D(1:myDim_elem2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_elem2D(1 : remPtr_elem2D(npes)-1)) &
                        = recvbuf(1 : remPtr_elem2D(npes)-1)
 
@@ -2211,20 +1850,20 @@ IF ( mype == 0 ) THEN
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_elem2D, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 end if
 
 end subroutine gather_real4_elem2D
 
 !================================================
-! Make element information available to master PE
+! Make element information available to master PE 
 subroutine gather_int2_elem2D(arr2D, arr2D_global, partit)
 use MOD_MESH
 USE MOD_PARTIT
@@ -2246,7 +1885,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
 
       allocate(recvbuf(remPtr_elem2D(npes)))
@@ -2256,11 +1895,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(start), e2D, MPI_SHORT, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_elem2D(1:myDim_elem2D)) = arr2D(1:myDim_elem2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_elem2D(1 : remPtr_elem2D(npes)-1)) &
                        = recvbuf(1 : remPtr_elem2D(npes)-1)
 
@@ -2269,13 +1908,13 @@ IF ( mype == 0 ) THEN
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_elem2D, MPI_SHORT, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 end if
 
@@ -2283,7 +1922,7 @@ end subroutine gather_int2_elem2D
 
 
 !============================================================================
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 subroutine gather_real8to4_nod3D(arr3D, arr3D_global, partit)
 use MOD_MESH
@@ -2310,7 +1949,7 @@ nl1=ubound(arr3D,1)
 ! Consider MPI-datatypes to recv directly into arr3D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(nl1, ubound(arr3D_global,2)))
 
@@ -2319,11 +1958,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(1,start), n3D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_nod2D(1:myDim_nod2D)) = arr3D(1:nl1,1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr3D_global(1:nl1, remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                      = recvbuf(1:nl1, 1 : remPtr_nod2D(npes)-1)
 
@@ -2337,17 +1976,17 @@ ELSE
 
    allocate(sendbuf(nl1,myDim_nod2D))
    sendbuf(1:nl1,1:myDim_nod2D) = arr3D(1:nl1,1:myDim_nod2D)
-
+   
    call MPI_SEND(sendbuf, myDim_nod2D*nl1, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
    deallocate(sendbuf)
-
+   
 ENDIF
 
 end if
 
 end subroutine gather_real8to4_nod3D
 !==============================================
-! Make nodal information available to master PE
+! Make nodal information available to master PE 
 subroutine gather_real8to4_nod2D(arr2D, arr2D_global, partit)
 use MOD_MESH
 USE MOD_PARTIT
@@ -2369,7 +2008,7 @@ integer                             :: start, n2D
  if (npes> 1) then
 CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(ubound(arr2D_global,1)))
       do  n = 1, npes-1
@@ -2377,32 +2016,32 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(start), n2D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_nod2D(1:myDim_nod2D)) = arr2D(1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1 : remPtr_nod2D(npes)-1)
       deallocate(recvbuf)
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
    sendbuf(1:myDim_nod2D) = real(arr2D(1:myDim_nod2D),real32)
 
    call MPI_SEND(sendbuf, myDim_nod2D, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 end if
 end subroutine gather_real8to4_nod2D
 !============================================================================
 subroutine gather_real8to4_elem3D(arr3D, arr3D_global, partit)
-! Make element information available to master PE
+! Make element information available to master PE 
 ! Use only with 3D arrays stored in (vertical, horizontal) way
 use MOD_MESH
 USE MOD_PARTIT
@@ -2428,7 +2067,7 @@ nl1=ubound(arr3D,1)
 ! Consider MPI-datatypes to recv directly into arr3D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(nl1,remPtr_elem2D(npes)))
 
@@ -2437,11 +2076,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(1,start), e3D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr3D_global(1:nl1,myList_elem2D(1:myDim_elem2D)) = arr3D(1:nl1,1:myDim_elem2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr3D_global(1:nl1, remList_elem2D(1 : remPtr_elem2D(npes)-1)) &
                      = recvbuf(1:nl1, 1 : remPtr_elem2D(npes)-1)
 
@@ -2454,7 +2093,7 @@ IF ( mype == 0 ) THEN
 ELSE
    allocate(sendbuf(nl1,myDim_elem2D))
    sendbuf(1:nl1,1:myDim_elem2D) = arr3D(1:nl1,1:myDim_elem2D)
-
+   
    call MPI_SEND(sendbuf, myDim_elem2D*nl1, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
    deallocate(sendbuf)
 ENDIF
@@ -2462,7 +2101,7 @@ ENDIF
 end if
 end subroutine gather_real8to4_elem3D
 !================================================
-! Make element information available to master PE
+! Make element information available to master PE 
 subroutine gather_real8to4_elem2D(arr2D, arr2D_global, partit)
 use MOD_MESH
 USE MOD_PARTIT
@@ -2485,7 +2124,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(remPtr_elem2D(npes)))
 
@@ -2494,11 +2133,11 @@ IF ( mype == 0 ) THEN
          start = remPtr_elem2D(n)
          call MPI_IRECV(recvbuf(start), e2D, MPI_REAL, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_elem2D(1:myDim_elem2D)) = arr2D(1:myDim_elem2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_elem2D(1 : remPtr_elem2D(npes)-1)) &
                        = recvbuf(1 : remPtr_elem2D(npes)-1)
 
@@ -2507,21 +2146,21 @@ IF ( mype == 0 ) THEN
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    sendbuf(1:myDim_elem2D) = real(arr2D(1:myDim_elem2D),real32)
    call MPI_SEND(sendbuf, myDim_elem2D, MPI_REAL, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 end if
 end subroutine gather_real8to4_elem2D
 !==============================================
 subroutine gather_elem2D_i(arr2D, arr2D_global, partit)
-! Make element information available to master PE
+! Make element information available to master PE 
 use MOD_MESH
 USE MOD_PARTIT
 USE MOD_PARSUP
@@ -2546,7 +2185,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
             e2D   = remPtr_elem2D(n+1) - remPtr_elem2D(n)
             start = remPtr_elem2D(n)
             call MPI_IRECV(recvbuf(start), e2D, MPI_INTEGER, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
-        enddo
+        enddo      
         arr2D_global(myList_elem2D(1:myDim_elem2D)) = arr2D(1:myDim_elem2D)
         call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
         arr2D_global(remList_elem2D(1 : remPtr_elem2D(npes)-1)) &
@@ -2583,7 +2222,7 @@ CALL MPI_BARRIER(MPI_COMM_FESOM,MPIerr)
 ! Consider MPI-datatypes to recv directly into arr2D_global!
 
 IF ( mype == 0 ) THEN
-
+   
    if (npes>1) then
       allocate(recvbuf(ubound(arr2D_global, 1)))
       do  n = 1, npes-1
@@ -2591,24 +2230,24 @@ IF ( mype == 0 ) THEN
          start = remPtr_nod2D(n)
          call MPI_IRECV(recvbuf(start), n2D, MPI_INTEGER, n, 2, MPI_COMM_FESOM, req(n), MPIerr)
       enddo
-
+      
       arr2D_global(myList_nod2D(1:myDim_nod2D)) = arr2D(1:myDim_nod2D)
-
+   
       call MPI_WAITALL(npes-1, req, MPI_STATUSES_IGNORE, MPIerr)
-
+   
       arr2D_global(remList_nod2D(1 : remPtr_nod2D(npes)-1)) &
                        = recvbuf(1 : remPtr_nod2D(npes)-1)
       deallocate(recvbuf)
    else
 
       arr2D_global(:) = arr2D(:)
-
+     
    endif
 
 ELSE
-
+   
    call MPI_SEND( arr2D, myDim_nod2D, MPI_INTEGER, 0, 2, MPI_COMM_FESOM, MPIerr )
-
+   
 ENDIF
 
 endif
@@ -2784,7 +2423,7 @@ interface gather_edge
 end interface gather_edge
 
 
-private  ! hides items not listed on public statement
+private  ! hides items not listed on public statement 
 public :: exchange_nod,exchange_elem,broadcast_nod,broadcast_elem, &
           gather_nod, gather_elem, exchange_nod_begin, exchange_nod_end, exchange_elem_begin, &
           exchange_elem_end, gather_edge
diff --git a/src/gen_model_setup.F90 b/src/gen_model_setup.F90
index faac60fe..282e75ec 100755
--- a/src/gen_model_setup.F90
+++ b/src/gen_model_setup.F90
@@ -6,11 +6,10 @@ subroutine setup_model(partit)
 !   use i_therm_param
   use g_forcing_param
   use g_config
-  use diagnostics, only: ldiag_solver,lcurt_stress_surf,lcurt_stress_surf, ldiag_Ri, ldiag_TurbFlux, ldiag_trflx, &
+  use diagnostics, only: ldiag_solver,lcurt_stress_surf,lcurt_stress_surf, ldiag_Ri, ldiag_TurbFlux, &
                          ldiag_dMOC, ldiag_DVD, diag_list
   use g_clock,     only: timenew, daynew, yearnew
-  use g_ic3d
-  use mod_transit
+  use g_ic3d 
   implicit none
   type(t_partit), intent(inout), target :: partit
   character(len=MAX_PATH)               :: nmlfile
@@ -29,8 +28,9 @@ subroutine setup_model(partit)
   read (fileunit, NML=geometry)
   read (fileunit, NML=calendar)
   read (fileunit, NML=run_config)
-  read (fileunit,NML=icebergs)
-
+#if defined (__async_icebergs)
+    read (fileunit,NML=icebergs)
+#endif
 !!$  read (fileunit, NML=machine)
   close (fileunit)
   
@@ -81,21 +81,6 @@ subroutine setup_model(partit)
   read (fileunit, NML=diag_list)
   close (fileunit)
 
-  if (use_transit) then
-! Transient tracer input, input file names have to be specified in
-! namelist.config, nml=run_config
-    if(partit%mype==0) print *, "Transient tracers are ON. Tracer input file: ", ifile_transit
-    open (20,file=ifile_transit)
-    if (anthro_transit .or. paleo_transit) then
-      call read_transit_input
-    else
-!     Spinup / equilibrium runs with constant tracer input,
-!     read parameter values from namelist.oce
-      read (20,nml=transit_param)
-    end if
-    close (20)
-  end if
-
   if(partit%mype==0) write(*,*) 'Namelist files are read in'
   
   !_____________________________________________________________________________
diff --git a/src/gen_modules_clock.F90 b/src/gen_modules_clock.F90
index e3abda4d..40f9abc3 100755
--- a/src/gen_modules_clock.F90
+++ b/src/gen_modules_clock.F90
@@ -12,7 +12,6 @@ module g_clock
   integer                  :: ndpyr                !number of days in yearnew 
   integer                  :: num_day_in_month(0:1,12)
   character(4)             :: cyearold, cyearnew   !year as character string      
-  character(2)             :: cmonth               !month as character string      
   data num_day_in_month(0,:) /31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31/
   data num_day_in_month(1,:) /31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31/
 
@@ -56,7 +55,6 @@ contains
        aux2=aux1+num_day_in_month(fleapyear,i)
        if(daynew>aux1 .and. daynew<=aux2) then
           month=i
-          write(cmonth, '(I2.2)') month
           day_in_month=daynew-aux1
           exit
        end if
@@ -71,7 +69,6 @@ contains
     USE MOD_PARTIT
     USE MOD_PARSUP
     use g_config
-    use mod_transit, only: ti_transit, ti_start_transit
     implicit none
     type(t_partit), intent(in), target    :: partit
     integer                               :: i, daystart, yearstart
@@ -96,9 +93,6 @@ contains
     else
        r_restart=.true.
     end if
-!   For simulations with transient tracer input data
-    if (use_transit) ti_transit = yearnew - yearstart + ti_start_transit
-
 
     ! year as character string 
     write(cyearold,'(i4)') yearold
diff --git a/src/gen_modules_config.F90 b/src/gen_modules_config.F90
index a12ff8c9..cce25132 100755
--- a/src/gen_modules_config.F90
+++ b/src/gen_modules_config.F90
@@ -28,9 +28,8 @@ module g_config
   character(MAX_PATH)        :: ClimateDataPath='./hydrography/'
   character(MAX_PATH)        :: TideForcingPath='./tide_forcing/'
   character(MAX_PATH)        :: ResultPath='./result/'
-  character(20)              :: MeshId='NONE'
   namelist /paths/  MeshPath, ClimateDataPath, &
-       TideForcingPath, ResultPath, MeshId
+       TideForcingPath, ResultPath
        
   !_____________________________________________________________________________
   ! *** restart_log ***
@@ -122,6 +121,9 @@ module g_config
   logical                       :: turn_off_hf=.false.
   logical                       :: turn_off_fw=.false.
   logical                       :: use_icesheet_coupling=.false.  
+  logical                       :: lbalance_fw=.true.  
+  logical                       :: lcell_saturation=.true.  
+  logical                       :: lmin_latent_hf=.true.  
   integer                       :: ib_num=0
   integer                       :: steps_per_ib_step=8
 
@@ -132,7 +134,7 @@ module g_config
   integer                       :: ib_async_mode=0
   integer                       :: thread_support_level_required=3 ! 2 = MPI_THREAD_SERIALIZED, 3 = MPI_THREAD_MULTIPLE
 
-  namelist /icebergs/ use_icebergs, turn_off_hf, turn_off_fw, use_icesheet_coupling, ib_num, steps_per_ib_step, ib_async_mode, thread_support_level_required
+  namelist /icebergs/ use_icebergs, turn_off_hf, turn_off_fw, use_icesheet_coupling, lbalance_fw, lcell_saturation, lmin_latent_hf, ib_num, steps_per_ib_step, ib_async_mode, thread_support_level_required
 
 !wiso-code!!!
   logical                       :: lwiso  =.false.  ! enable isotope?
@@ -146,11 +148,9 @@ module g_config
   character(100)                :: which_toy="soufflet" 
   logical                       :: flag_debug=.false.    ! prints name of actual subroutine he is in 
   logical                       :: flag_warn_cflz=.true. ! switches off cflz warning
-  logical                       :: use_transit=.false.    ! switches off transient tracers
   namelist /run_config/ use_ice,use_floatice, use_sw_pene, use_cavity, & 
                         use_cavity_partial_cell, cavity_partial_cell_thresh, &
-                        use_cavity_fw2press, toy_ocean, which_toy, flag_debug, flag_warn_cflz, lwiso, &
-                        use_transit
+                        use_cavity_fw2press, toy_ocean, which_toy, flag_debug, flag_warn_cflz, lwiso !---wiso-code: add lwiso
   
   !_____________________________________________________________________________
   ! *** others ***
diff --git a/src/gen_modules_diag.F90 b/src/gen_modules_diag.F90
index b9c6f874..3ad921fe 100644
--- a/src/gen_modules_diag.F90
+++ b/src/gen_modules_diag.F90
@@ -6,7 +6,6 @@ module diagnostics
   USE MOD_PARSUP
   use MOD_TRACER
   use MOD_DYN
-  use MOD_ICE
   use g_clock
   use g_comm_auto
   use o_ARRAYS
@@ -18,12 +17,12 @@ module diagnostics
 
   private
   public :: ldiag_solver, lcurt_stress_surf, ldiag_Ri, ldiag_TurbFlux, ldiag_dMOC, ldiag_DVD,        &
-            ldiag_forc, ldiag_salt3D, ldiag_curl_vel3, diag_list, ldiag_vorticity, ldiag_extflds, ldiag_ice,   &
+            ldiag_forc, ldiag_salt3D, ldiag_curl_vel3, diag_list, ldiag_vorticity, ldiag_extflds,    &
             compute_diagnostics, rhs_diag, curl_stress_surf, curl_vel3, shear, Ri, KvdTdZ, KvdSdZ,   & 
-            std_dens_min, std_dens_max, std_dens_N, std_dens, ldiag_trflx,                           &
-            std_dens_UVDZ, std_dens_DIV, std_dens_DIV_fer, std_dens_Z, std_dens_H, std_dens_dVdT, std_dens_flux,       &
+            std_dens_min, std_dens_max, std_dens_N, std_dens,                                        &
+            std_dens_UVDZ, std_dens_DIV, std_dens_Z, std_dens_H, std_dens_dVdT, std_dens_flux,       &
             dens_flux_e, vorticity, zisotherm, tempzavg, saltzavg, compute_diag_dvd_2ndmoment_klingbeil_etal_2014,       &
-            compute_diag_dvd_2ndmoment_burchard_etal_2008, compute_diag_dvd, vol_ice, vol_snow, compute_ice_diag, thetao, tuv, suv
+            compute_diag_dvd_2ndmoment_burchard_etal_2008, compute_diag_dvd, thetao
   ! Arrays used for diagnostics, some shall be accessible to the I/O
   ! 1. solver diagnostics: A*x=rhs? 
   ! A=ssh_stiff, x=d_eta, rhs=ssh_rhs; rhs_diag=A*x;
@@ -34,10 +33,9 @@ module diagnostics
   real(kind=WP),  save, allocatable, target      :: shear(:,:), Ri(:,:), KvdTdZ(:,:), KvdSdZ(:,:)
   real(kind=WP),  save, allocatable, target      :: stress_bott(:,:), u_bott(:), v_bott(:), u_surf(:), v_surf(:)
   real(kind=WP),  save, allocatable, target      :: vorticity(:,:)
-  real(kind=WP),  save, allocatable, target      :: zisotherm(:)              !target temperature is specified as whichtemp in compute_extflds
+  real(kind=WP),  save, allocatable, target      :: zisotherm(:)             !target temperature is specified as whichtemp in compute_extflds
   real(kind=WP),  save, allocatable, target      :: tempzavg(:), saltzavg(:)  !target depth for averaging is specified as whichdepth in compute_extflds
-  real(kind=WP),  save, allocatable, target      :: vol_ice(:),  vol_snow(:)
-  ! defining a set of standard density bins which will be used for computing densMOC
+! defining a set of standard density bins which will be used for computing densMOC
 ! integer,        parameter                      :: std_dens_N  = 100
 ! real(kind=WP),  save, target                   :: std_dens(std_dens_N)
   integer,        parameter                      :: std_dens_N  =89
@@ -54,10 +52,9 @@ module diagnostics
                                                             37.11979, 37.13630, 37.15257, 37.16861, 37.18441, 37.50000, 37.75000, 40.00000/)
   real(kind=WP),  save, target                   :: std_dd(std_dens_N-1)
   real(kind=WP),  save, target                   :: std_dens_min=1030., std_dens_max=1040.
-  real(kind=WP),  save, allocatable, target      :: std_dens_UVDZ(:,:,:), std_dens_flux(:,:,:), std_dens_dVdT(:,:), std_dens_DIV(:,:), std_dens_DIV_fer(:,:), std_dens_Z(:,:), std_dens_H(:,:)
+  real(kind=WP),  save, allocatable, target      :: std_dens_UVDZ(:,:,:), std_dens_flux(:,:,:), std_dens_dVdT(:,:), std_dens_DIV(:,:), std_dens_Z(:,:), std_dens_H(:,:)
   real(kind=WP),  save, allocatable, target      :: dens_flux_e(:)
   real(kind=WP),  save, allocatable, target      :: thetao(:) ! sst in K
-  real(kind=WP),  save, allocatable, target      :: tuv(:,:,:), suv(:,:,:)
 
   logical                                       :: ldiag_solver     =.false.
   logical                                       :: lcurt_stress_surf=.false.
@@ -78,11 +75,8 @@ module diagnostics
   
   logical                                       :: ldiag_vorticity  =.false.
   logical                                       :: ldiag_extflds    =.false.
-  logical                                       :: ldiag_ice        =.false.
-  logical                                       :: ldiag_trflx      =.false.
-
-  namelist /diag_list/ ldiag_solver, lcurt_stress_surf, ldiag_curl_vel3, ldiag_Ri, ldiag_TurbFlux, ldiag_dMOC, ldiag_DVD, &
-                       ldiag_salt3D, ldiag_forc, ldiag_vorticity, ldiag_extflds, ldiag_trflx
+  
+  namelist /diag_list/ ldiag_solver, lcurt_stress_surf, ldiag_curl_vel3, ldiag_Ri, ldiag_TurbFlux, ldiag_dMOC, ldiag_DVD, ldiag_salt3D, ldiag_forc, ldiag_vorticity, ldiag_extflds
   
   contains
 
@@ -287,61 +281,6 @@ salt   => tracers%data(2)%values(:,:)
   end do
 end subroutine diag_turbflux
 ! ==============================================================
-!
-subroutine diag_trflx(mode, dynamics, tracers, partit, mesh)
-  implicit none
-  type(t_dyn)   , intent(inout), target :: dynamics
-  type(t_tracer), intent(in)   , target :: tracers
-  type(t_partit), intent(inout), target :: partit
-  type(t_mesh)  , intent(in)   , target :: mesh
-  integer,        intent(in)            :: mode
-  logical,        save                     :: firstcall=.true.
-  integer                                  :: elem, nz, nzu, nzl, elnodes(3)
-  real(kind=WP), dimension(:,:,:), pointer :: UV, fer_UV
-  real(kind=WP), dimension(:,:),   pointer :: temp, salt
-
-#include "associate_part_def.h"
-#include "associate_mesh_def.h"
-#include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
-UV     => dynamics%uv(:,:,:)
-temp   => tracers%data(1)%values(:,:)
-salt   => tracers%data(2)%values(:,:)
-fer_UV => dynamics%fer_uv(:,:,:)
-!=====================
-  if (firstcall) then !allocate the stuff at the first call
-      allocate(tuv(2,nl-1,myDim_elem2D+eDim_elem2D))
-      allocate(suv(2,nl-1,myDim_elem2D+eDim_elem2D))
-      tuv = 0.0_WP
-      suv = 0.0_WP
-      firstcall=.false.
-      if (mode==0) return
-  end if
-
-  !___________________________________________________________________________
-  ! compute tracer fluxes
-  do elem=1,myDim_elem2D
-      elnodes = elem2D_nodes(:,elem)
-      nzu     = ulevels(elem)
-      nzl     = nlevels(elem)-1
-      if (Fer_GM) then
-          do nz=nzu, nzl
-              tuv(1,nz,elem) = (UV(1,nz,elem) + fer_UV(1,nz, elem)) * sum(temp(nz,elnodes))/3._WP
-              tuv(2,nz,elem) = (UV(2,nz,elem) + fer_UV(2,nz, elem)) * sum(temp(nz,elnodes))/3._WP
-              suv(1,nz,elem) = (UV(1,nz,elem) + fer_UV(1,nz, elem)) * sum(salt(nz,elnodes))/3._WP
-              suv(2,nz,elem) = (UV(2,nz,elem) + fer_UV(2,nz, elem)) * sum(salt(nz,elnodes))/3._WP
-          end do
-      else
-          do nz=nzu, nzl
-              tuv(1,nz,elem) = UV(1,nz,elem) * sum(temp(nz,elnodes))/3._WP
-              tuv(2,nz,elem) = UV(2,nz,elem) * sum(temp(nz,elnodes))/3._WP
-              suv(1,nz,elem) = UV(1,nz,elem) * sum(salt(nz,elnodes))/3._WP
-              suv(2,nz,elem) = UV(2,nz,elem) * sum(salt(nz,elnodes))/3._WP
-          end do
-      end if
-  end do
-end subroutine diag_trflx
-! ==============================================================
 ! 
 subroutine diag_Ri(mode, dynamics, partit, mesh)
   implicit none
@@ -413,7 +352,6 @@ subroutine diag_densMOC(mode, dynamics, tracers, partit, mesh)
      allocate(std_dens_w   (  std_dens_N, myDim_elem2D))
      allocate(std_dens_dVdT(  std_dens_N, myDim_elem2D))
      allocate(std_dens_DIV (  std_dens_N, myDim_nod2D+eDim_nod2D))
-     if (Fer_GM) allocate(std_dens_DIV_fer(  std_dens_N, myDim_nod2D+eDim_nod2D))
      allocate(std_dens_VOL1(  std_dens_N, myDim_elem2D))
      allocate(std_dens_VOL2(  std_dens_N, myDim_elem2D))
      allocate(std_dens_flux(3,std_dens_N, myDim_elem2D))
@@ -436,7 +374,6 @@ subroutine diag_densMOC(mode, dynamics, tracers, partit, mesh)
      std_dens_UVDZ=0. !will be U & V transports within the density class
      std_dens_dVdT=0. !rate of change of a bin volume (for estimating the 'model drift')
      std_dens_DIV =0. !meridional divergence within a density bin (for reconstruction of the diapycnal velocity) !TOP PRIORITY
-     if (Fer_GM) std_dens_DIV_fer =0. !meridional divergence of bolus velocity within a density bin (for reconstruction of the diapycnal velocity) !TOP PRIORITY
      std_dens_VOL1=0. !temporal arrays for computing std_dens_dVdT
      std_dens_VOL2=0.
      std_dens_flux=0. !bouyancy flux for computation of surface bouyancy transformations
@@ -454,10 +391,8 @@ subroutine diag_densMOC(mode, dynamics, tracers, partit, mesh)
   dens_flux_e    =0.
   std_dens_VOL2=0.
   std_dens_DIV =0.
-  if (Fer_GM) std_dens_DIV_fer =0. !meridional divergence of bolus velocity within a density bin (for reconstruction of the diapycnal velocity) !TOP PRIORITY
   std_dens_Z   =0.
   std_dens_H   =0.
-  
   ! proceed with fields at elements...
   do elem=1, myDim_elem2D
      elnodes=elem2D_nodes(:,elem)     
@@ -560,133 +495,83 @@ subroutine diag_densMOC(mode, dynamics, tracers, partit, mesh)
      end do
   end do
 
-    !___________________________________________________________________________
-    ! proceed with fields at nodes (cycle over edges to compute the divergence)...
-    do edge=1, myDim_edge2D
-        if (myList_edge2D(edge) > edge2D_in) cycle
-        enodes=edges(:,edge)
-        eelems=edge_tri(:,edge)
-        nzmax =nlevels(eelems(1))
-        nzmin =ulevels(eelems(1))
-        if (eelems(2)>0) nzmax=max(nzmax, nlevels(eelems(2)))
-        do nz=nzmin, nzmax-1
-            aux(nz)=sum(density_dmoc(nz, enodes))/2.-1000.
+  ! proceed with fields at nodes (cycle over edges to compute the divergence)...
+  do edge=1, myDim_edge2D
+     if (myList_edge2D(edge) > edge2D_in) cycle
+     enodes=edges(:,edge)
+     eelems=edge_tri(:,edge)
+     nzmax =nlevels(eelems(1))
+     nzmin =ulevels(eelems(1))
+     if (eelems(2)>0) nzmax=max(nzmax, nlevels(eelems(2)))
+     !!PS do nz=1, nzmax-1
+     do nz=nzmin, nzmax-1
+        aux(nz)=sum(density_dmoc(nz, enodes))/2.-1000.
+     end do
+
+     do e=1,2
+        elem=eelems(e)
+        if (elem<=0) CYCLE
+        deltaX=edge_cross_dxdy(1+(e-1)*2,edge) 
+        deltaY=edge_cross_dxdy(2+(e-1)*2,edge)
+        nzmax =nlevels(elem)
+        nzmin =ulevels(elem)
+
+        do nz=nzmax-1,nzmin+1,-1
+           dens(nz)   = (aux(nz)     * helem(nz-1,elem)+&
+                         aux(nz-1)   * helem(nz,  elem))/sum(helem(nz-1:nz,elem))
         end do
-        
-        !_______________________________________________________________________
-        do e=1,2
-            elem=eelems(e)
-            if (elem<=0) CYCLE
-            deltaX=edge_cross_dxdy(1+(e-1)*2,edge) 
-            deltaY=edge_cross_dxdy(2+(e-1)*2,edge)
-            nzmax =nlevels(elem)
-            nzmin =ulevels(elem)
-            
-            !___________________________________________________________________
-            do nz=nzmax-1,nzmin+1,-1
-                dens(nz)   = (aux(nz)     * helem(nz-1,elem)+&
-                            aux(nz-1)   * helem(nz,  elem))/sum(helem(nz-1:nz,elem))
-            end do
-            dens(nzmax)=dens(nzmax-1)+(dens(nzmax-1)-dens(nzmax-2))*helem(nzmax-1,elem)/helem(nzmax-2,elem)
-            dens(nzmin)    =dens(nzmin+1)      +(dens(nzmin+1)-dens(nzmin+2))            *helem(nzmin, elem)     /helem(nzmin+1,elem)       
-            is=minloc(abs(std_dens-dens(nzmin)),1)
-            
-            !___________________________________________________________________
-            do nz=nzmax-1,nzmin,-1
-                div=(UV(2,nz,elem)*deltaX-UV(1,nz,elem)*deltaY)*helem(nz,elem)
-                if (e==2) div=-div
-                dmin =minval(dens(nz:nz+1))
-                dmax =maxval(dens(nz:nz+1))
-                ddiff=abs(dens(nz)-dens(nz+1))
-                
-                ! do vertical  binning onto prescribed density classes
-                is=std_dens_N
-                do jj = 1, std_dens_N
-                    if (std_dens(jj) > dmin) then
-                        is = jj
-                        exit
-                    endif
-                end do
-                ie=1
-                do jj = std_dens_N,1,-1
-                    if (std_dens(jj) < dmax) then
-                        ie = jj
-                        exit
-                    endif
-                end do
-                
-                if (std_dens(is)>=dmax) is=ie
-                if (std_dens(ie)<=dmin) ie=is
-                if (ie-is > 0) then
-                    weight=(std_dens(is)-dmin)+std_dd(is)/2.
-                    weight=max(weight, 0.)/ddiff
-                    std_dens_DIV(is, enodes(1))=std_dens_DIV(is, enodes(1))+weight*div
-                    std_dens_DIV(is, enodes(2))=std_dens_DIV(is, enodes(2))-weight*div
-                    do snz=is+1, ie-1
-                        weight=(sum(std_dd(snz-1:snz))/2.)/ddiff
-                        std_dens_DIV(snz, enodes(1))=std_dens_DIV(snz, enodes(1))+weight*div
-                        std_dens_DIV(snz, enodes(2))=std_dens_DIV(snz, enodes(2))-weight*div
-                    end do
-                    weight=(dmax-std_dens(ie))+std_dd(ie-1)/2.
-                    weight=max(weight, 0.)/ddiff
-                    std_dens_DIV(ie, enodes(1))=std_dens_DIV(ie, enodes(1))+weight*div
-                    std_dens_DIV(ie, enodes(2))=std_dens_DIV(ie, enodes(2))-weight*div
-                else
-                    std_dens_DIV(is, enodes(1))=std_dens_DIV(is, enodes(1))+div
-                    std_dens_DIV(is, enodes(2))=std_dens_DIV(is, enodes(2))-div
-                end if ! --> if (ie-is > 0) then
-            end do ! --> do nz=nzmax-1,nzmin,-1
-            
-            !___________________________________________________________________
-            ! compute density class divergence from GM Bolus velocity
-            if (Fer_GM) then
-                do nz=nzmax-1,nzmin,-1
-                    div=(fer_uv(2,nz,elem)*deltaX-fer_uv(1,nz,elem)*deltaY)*helem(nz,elem)
-                    if (e==2) div=-div
-                    dmin =minval(dens(nz:nz+1))
-                    dmax =maxval(dens(nz:nz+1))
-                    ddiff=abs(dens(nz)-dens(nz+1))
-                    
-                    ! do vertical  binning onto prescribed density classes
-                    is=std_dens_N
-                    do jj = 1, std_dens_N
-                        if (std_dens(jj) > dmin) then
-                            is = jj
-                            exit
-                        endif
-                    end do
-                    ie=1
-                    do jj = std_dens_N,1,-1
-                        if (std_dens(jj) < dmax) then
-                            ie = jj
-                            exit
-                        endif
-                    end do
-                    
-                    if (std_dens(is)>=dmax) is=ie
-                    if (std_dens(ie)<=dmin) ie=is
-                    if (ie-is > 0) then
-                        weight=(std_dens(is)-dmin)+std_dd(is)/2.
-                        weight=max(weight, 0.)/ddiff
-                        std_dens_DIV_fer(is, enodes(1))=std_dens_DIV_fer(is, enodes(1))+weight*div
-                        std_dens_DIV_fer(is, enodes(2))=std_dens_DIV_fer(is, enodes(2))-weight*div
-                        do snz=is+1, ie-1
-                            weight=(sum(std_dd(snz-1:snz))/2.)/ddiff
-                            std_dens_DIV_fer(snz, enodes(1))=std_dens_DIV_fer(snz, enodes(1))+weight*div
-                            std_dens_DIV_fer(snz, enodes(2))=std_dens_DIV_fer(snz, enodes(2))-weight*div
-                        end do
-                        weight=(dmax-std_dens(ie))+std_dd(ie-1)/2.
-                        weight=max(weight, 0.)/ddiff
-                        std_dens_DIV_fer(ie, enodes(1))=std_dens_DIV_fer(ie, enodes(1))+weight*div
-                        std_dens_DIV_fer(ie, enodes(2))=std_dens_DIV_fer(ie, enodes(2))-weight*div
-                    else
-                        std_dens_DIV_fer(is, enodes(1))=std_dens_DIV_fer(is, enodes(1))+div
-                        std_dens_DIV_fer(is, enodes(2))=std_dens_DIV_fer(is, enodes(2))-div
-                    end if ! --> if (ie-is > 0) then
-                end do ! --> do nz=nzmax-1,nzmin,-1
-            end if ! --> if (Fer_GM) then
-        end do ! --> do e=1,2
-    end do ! --> do edge=1, myDim_edge2D
+        dens(nzmax)=dens(nzmax-1)+(dens(nzmax-1)-dens(nzmax-2))*helem(nzmax-1,elem)/helem(nzmax-2,elem)
+        dens(nzmin)    =dens(nzmin+1)      +(dens(nzmin+1)-dens(nzmin+2))            *helem(nzmin, elem)     /helem(nzmin+1,elem)       
+        is=minloc(abs(std_dens-dens(nzmin)),1)
+
+        do nz=nzmax-1,nzmin,-1
+           div=(UV(2,nz,elem)*deltaX-UV(1,nz,elem)*deltaY)*helem(nz,elem)
+           if (e==2) div=-div
+           dmin =minval(dens(nz:nz+1))
+           dmax =maxval(dens(nz:nz+1))
+           ddiff=abs(dens(nz)-dens(nz+1))
+   
+          ! do vertical  binning onto prescribed density classes
+           is=std_dens_N
+           do jj = 1, std_dens_N
+              if (std_dens(jj) > dmin) then
+                 is = jj
+                 exit
+              endif
+           end do
+
+           ie=1
+           do jj = std_dens_N,1,-1
+              if (std_dens(jj) < dmax) then
+                 ie = jj
+                 exit
+              endif
+           end do
+
+           if (std_dens(is)>=dmax) is=ie
+           if (std_dens(ie)<=dmin) ie=is
+
+           if (ie-is > 0) then
+           weight=(std_dens(is)-dmin)+std_dd(is)/2.
+           weight=max(weight, 0.)/ddiff
+           std_dens_DIV(is, enodes(1))=std_dens_DIV(is, enodes(1))+weight*div
+           std_dens_DIV(is, enodes(2))=std_dens_DIV(is, enodes(2))-weight*div
+           do snz=is+1, ie-1
+              weight=(sum(std_dd(snz-1:snz))/2.)/ddiff
+              std_dens_DIV(snz, enodes(1))=std_dens_DIV(snz, enodes(1))+weight*div
+              std_dens_DIV(snz, enodes(2))=std_dens_DIV(snz, enodes(2))-weight*div
+           end do
+           weight=(dmax-std_dens(ie))+std_dd(ie-1)/2.
+           weight=max(weight, 0.)/ddiff
+           std_dens_DIV(ie, enodes(1))=std_dens_DIV(ie, enodes(1))+weight*div
+           std_dens_DIV(ie, enodes(2))=std_dens_DIV(ie, enodes(2))-weight*div
+        else
+           std_dens_DIV(is, enodes(1))=std_dens_DIV(is, enodes(1))+div
+           std_dens_DIV(is, enodes(2))=std_dens_DIV(is, enodes(2))-div
+        end if
+      end do
+    end do
+  end do
   
   !_____________________________________________________________________________
   where (std_dens_w > 0.)
@@ -887,35 +772,7 @@ subroutine compute_extflds(mode, dynamics, tracers, partit, mesh)
   call exchange_nod(tempzavg, partit)
   call exchange_nod(saltzavg, partit)
 end subroutine compute_extflds
-!_______________________________________________________________________________
-subroutine compute_ice_diag(mode, ice, partit, mesh)
-    IMPLICIT NONE
-    integer,        intent(in)              :: mode
-    logical,        save                    :: firstcall=.true.
-    type(t_ice)   , intent(in),     target  :: ice
-    type(t_partit), intent(inout),  target  :: partit
-    type(t_mesh)  , intent(in)   ,  target  :: mesh
-    integer                                 :: n
-
-#include "associate_part_def.h"
-#include "associate_mesh_def.h"
-#include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
-    if (firstcall) then  !allocate the stuff at the first call
-        allocate(vol_ice(myDim_nod2D+eDim_nod2D), vol_snow(myDim_nod2D+eDim_nod2D))
-        vol_ice =0.0_WP
-        vol_snow=0.0_WP
-        firstcall=.false.
-        if (mode==0) return
-    end if
-!$OMP PARALLEL DO
-DO n=1, myDim_nod2D
-   vol_ice(n) =ice%data(1)%values(n)*ice%data(2)%values(n)
-   vol_snow(n)=ice%data(1)%values(n)*ice%data(3)%values(n)
-END DO
-!$OMP END PARALLEL DO
 
-end subroutine compute_ice_diag
 
 ! SST in K
 subroutine compute_thetao(mode, tracers, partit, mesh)
@@ -941,12 +798,11 @@ subroutine compute_thetao(mode, tracers, partit, mesh)
 end subroutine compute_thetao
 
 ! ==============================================================
-subroutine compute_diagnostics(mode, dynamics, tracers, ice, partit, mesh)
+subroutine compute_diagnostics(mode, dynamics, tracers, partit, mesh)
   implicit none
   type(t_mesh)  , intent(in)   , target :: mesh
   type(t_partit), intent(inout), target :: partit
   type(t_tracer), intent(inout), target :: tracers
-  type(t_ice),    intent(inout), target :: ice
   type(t_dyn)   , intent(inout), target :: dynamics
   integer, intent(in)                   :: mode !constructor mode (0=only allocation; any other=do diagnostic)
   real(kind=WP)                         :: val  !1. solver diagnostic
@@ -970,14 +826,11 @@ subroutine compute_diagnostics(mode, dynamics, tracers, ice, partit, mesh)
   if (ldiag_dMOC)        call diag_densMOC(mode, dynamics, tracers, partit, mesh)
   !7. compute turbulent fluxes
   if (ldiag_turbflux)    call diag_turbflux(mode, dynamics, tracers, partit, mesh)
-  !8. compute tracers fluxes
-  if (ldiag_trflx)       call diag_trflx(mode, dynamics, tracers, partit, mesh)
   ! compute relative vorticity
   if (ldiag_vorticity)   call relative_vorticity(mode, dynamics, partit, mesh)
   ! soe exchanged fields requested by IFS/FESOM in NextGEMS.
   if (ldiag_extflds)     call compute_extflds(mode, dynamics, tracers, partit, mesh)
-  !fields required for for destinE
-  if (ldiag_ice)         call compute_ice_diag(mode, ice, partit, mesh)
+
   call compute_thetao(mode, tracers, partit, mesh) 
 end subroutine compute_diagnostics
 
diff --git a/src/gen_modules_forcing.F90 b/src/gen_modules_forcing.F90
index 2c27b711..1a571696 100755
--- a/src/gen_modules_forcing.F90
+++ b/src/gen_modules_forcing.F90
@@ -94,7 +94,7 @@ use o_param
 #if defined (__oasis) || defined (__ifsinterface)
   real(kind=WP), allocatable, dimension(:)         :: residualifwflx
 #endif
-
+  
   real(kind=WP), allocatable, dimension(:)         :: runoff_landice
   real(kind=WP)                                    :: landice_season(12)
 
diff --git a/src/gen_modules_partitioning.F90 b/src/gen_modules_partitioning.F90
index 04ecd94d..3b97c7a2 100644
--- a/src/gen_modules_partitioning.F90
+++ b/src/gen_modules_partitioning.F90
@@ -42,26 +42,22 @@ subroutine par_init(partit)    ! initializes MPI
   USE o_PARAM
   USE MOD_PARTIT
   USE MOD_PARSUP
-#ifdef __MULTIO
-  USE iom
-  USE mpp_io
-#endif
-
   implicit none
   type(t_partit), intent(inout), target :: partit
   integer                               :: i
   integer                               :: provided_mpi_thread_support_level
   character(:), allocatable             :: provided_mpi_thread_support_level_name
+
 #if defined __oasis || defined  __ifsinterface
   ! use comm from coupler or ifs
 #else
   partit%MPI_COMM_FESOM=MPI_COMM_WORLD ! use global comm if not coupled (e.g. no __oasis or __ifsinterface)
-#endif
+#endif  
   call MPI_Comm_Size(partit%MPI_COMM_FESOM,partit%npes,i)
-  call MPI_Comm_Rank(partit%MPI_COMM_FESOM,partit%mype,i) 
+  call MPI_Comm_Rank(partit%MPI_COMM_FESOM,partit%mype,i)
+ 
 
   if(partit%mype==0) then
-#if !defined(__PGI)
     call MPI_Query_thread(provided_mpi_thread_support_level, i)
     if(provided_mpi_thread_support_level == MPI_THREAD_SINGLE) then
       provided_mpi_thread_support_level_name = "MPI_THREAD_SINGLE"
@@ -76,12 +72,26 @@ subroutine par_init(partit)    ! initializes MPI
     end if
     write(*,*) 'MPI has been initialized, provided MPI thread support level: ', &
          provided_mpi_thread_support_level_name,provided_mpi_thread_support_level
-#endif
     write(*, *) 'Running on                   ', partit%npes, ' PEs'
 #if defined(_OPENMP)
     write(*, *) 'This is MPI/OpenMP run, with ', OMP_GET_MAX_THREADS(), ' threads per PE'
 #endif
   end if
+  
+!---------------------------------------------------
+!LA 2023-01-31 add asynchronous icebergs
+#ifndef __async_icebergs
+! kh 04.02.21 OpenMP is used for parallel partitioning
+ !$OMP threadprivate(com_nod2D,com_elem2D,com_elem2D_full)
+ !$OMP threadprivate(mype)
+ !$OMP threadprivate(myDim_nod2D, eDim_nod2D, myList_nod2D)
+ !$OMP threadprivate(myDim_elem2D, eDim_elem2D, eXDim_elem2D, myList_elem2D)
+ !$OMP threadprivate(myDim_edge2D, eDim_edge2D, myList_edge2D)
+!#else
+! kh 04.02.21 OpenMP is used for asynchronous iceberg computations, i.e. a shared access is required
+#endif  
+!---------------------------------------------------
+
 end subroutine par_init
 !=================================================================
 subroutine par_ex(COMM, mype, abort)       ! finalizes MPI
@@ -96,10 +106,8 @@ subroutine par_ex(COMM, mype, abort)       ! finalizes MPI
 #else
   !For ECHAM coupled runs we use the old OASIS nameing scheme (prism / prism_proto)
   use mod_prism 
-#endif
-         ! oifs/echam
-#endif
-         ! oasis
+#endif ! oifs/echam
+#endif ! oasis
 
   implicit none
   integer,           intent(in)   :: COMM
@@ -109,23 +117,16 @@ subroutine par_ex(COMM, mype, abort)       ! finalizes MPI
 
 ! For standalone runs we directly call the MPI_barrier and MPI_finalize
 !---------------------------------------------------------------
-!TODO: logic is convoluted here, not defined oasis and model needs to abort doesn't happen using par_ex 
 #ifndef __oasis
   if (present(abort)) then
      if (mype==0) write(*,*) 'Run finished unexpectedly!'
-     call MPI_ABORT(MPI_COMM_WORLD, 1 )
+     call MPI_ABORT(COMM, 1 )
   else
-          ! TODO: this is where fesom standalone, ifsinterface etc get to 
-          !1. there no abort actually even when model calls abort, and barrier may hang
-          !2. when using fesom as lib using finalize is bad here as there may 
-          !   be other MPI tasks running in calling library like IFS, better 
-          !   better practice in that case would be to free the communicator.
      call  MPI_Barrier(COMM, error)
      call  MPI_Finalize(error)
   endif
+#else ! standalone
 
-#else !  standalone
-! TODO logic below is also convoluted really not really for standalone
 ! From here on the two coupled options
 !-------------------------------------
 #if defined (__oifs)
@@ -148,10 +149,8 @@ subroutine par_ex(COMM, mype, abort)       ! finalizes MPI
   
   if (mype==0) print *, 'FESOM calls MPI_Finalize'
   call MPI_Finalize(error)
-#endif
-         ! oifs/echam
-#endif
-         ! oasis
+#endif ! oifs/echam
+#endif ! oasis
 
 ! Regardless of standalone, OpenIFS oder ECHAM coupling, if we reach to this point
 ! we should be fine shutting the whole model down
diff --git a/src/gen_modules_read_NetCDF.F90 b/src/gen_modules_read_NetCDF.F90
index 3f7fcbeb..8e459d1d 100755
--- a/src/gen_modules_read_NetCDF.F90
+++ b/src/gen_modules_read_NetCDF.F90
@@ -12,7 +12,7 @@ subroutine read_other_NetCDF(file, vari, itime, model_2Darray, check_dummy, do_o
   ! if check_dummy=.true.,  missing value is replaced with a meaningful value nearby
   ! if check_dummy=.false., missing value is replaced with 0.0
 
-  use, intrinsic :: ISO_FORTRAN_ENV, only: real64
+  use, intrinsic :: ISO_FORTRAN_ENV
   use g_config
   use o_param
   USE MOD_MESH
@@ -198,7 +198,6 @@ subroutine read_surf_hydrography_NetCDF(file, vari, itime, model_2Darray, partit
     USE MOD_PARTIT
     USE MOD_PARSUP
     use g_rotate_grid
-    use, intrinsic :: ISO_FORTRAN_ENV, only: real64
     implicit none
 #include "netcdf.inc" 
   type(t_mesh),   intent(in),    target :: mesh
@@ -206,7 +205,7 @@ subroutine read_surf_hydrography_NetCDF(file, vari, itime, model_2Darray, partit
   integer                       :: i, j,  n, num
   integer                       :: itime, latlen, lonlen
   integer                       :: status, ncid, varid
-  integer                       :: lonid, latid, drain_num
+  integer                       :: lonid, latid
   integer                       :: istart(4), icount(4)
   real(real64)                  :: x, y, miss
   real(real64), allocatable     :: lon(:), lat(:)
@@ -313,7 +312,7 @@ end subroutine read_surf_hydrography_NetCDF
 !
 subroutine read_2ddata_on_grid_NetCDF(file, vari, itime, model_2Darray, partit, mesh)  
 
-  use, intrinsic :: ISO_FORTRAN_ENV, only: real64
+  use, intrinsic :: ISO_FORTRAN_ENV
 
   use g_config
   use o_param
@@ -364,4 +363,7 @@ subroutine read_2ddata_on_grid_NetCDF(file, vari, itime, model_2Darray, partit,
   call MPI_BCast(ncdata, nod2D, MPI_DOUBLE_PRECISION, 0, MPI_COMM_FESOM, ierror)
   model_2Darray=ncdata(myList_nod2D) 
 end subroutine read_2ddata_on_grid_NetCDF
+  
 end module g_read_other_NetCDF
+
+
diff --git a/src/gen_surface_forcing.F90 b/src/gen_surface_forcing.F90
index 4b38b553..967f217f 100644
--- a/src/gen_surface_forcing.F90
+++ b/src/gen_surface_forcing.F90
@@ -30,7 +30,7 @@ MODULE g_sbf
    !!   we assume that all NetCDF files have identical grid and time variable
    !!
    !! public:
-   !!   sbc_ini  -- initialization atmpospheric forcing
+   !!   sbc_ini  -- inizialization atmpospheric forcing
    !!   sbc_do   -- provide a sbc (surface boundary conditions) each time step
    !!
    USE MOD_MESH
@@ -52,7 +52,6 @@ MODULE g_sbf
    public  sbc_ini  ! routine called before 1st time step (open files, read namelist,...)
    public  sbc_do   ! routine called each time step to provide a sbc fileds (wind,...)
    public  sbc_end  ! routine called after last time step
-   public  RUNOFF_MAPPER
    public  julday   ! get julian day from date
    public  atmdata
    public  i_totfl, i_xwind, i_ywind, i_xstre, i_ystre, i_humi, i_qsr, i_qlw, i_tair, i_prec, i_mslp, i_cloud, i_snow
@@ -96,10 +95,7 @@ MODULE g_sbf
    character(10),           save   :: chl_data_source   ='None' ! 'Sweeney' Chlorophyll climatology Sweeney et al. 2005
    character(len=MAX_PATH), save   :: nm_chl_data_file  ='/work/ollie/dsidoren/input/forcing/Sweeney_2005.nc'
    real(wp),                save   :: chl_const         = 0.1
-   logical                         :: use_runoff_mapper = .false.               !runof mapper to be used in the coupled mode with IFS
-   character(len=MAX_PATH), save   :: runoff_basins_file='runoff_basins.nc'     ! definition file for runoff basins
-   real(wp)                        :: runoff_radius     =500000._WP             !smoothing radius for runoff mapper (in meters)
-   type(sparse_matrix)             :: RUNOFF_MAPPER
+
 
    logical :: runoff_climatology =.false.
 
@@ -908,7 +904,7 @@ CONTAINS
       !!---------------------------------------------------------------------
       !!                    ***  ROUTINE sbc_ini ***
       !!
-      !! ** Purpose : initialization of ocean forcing
+      !! ** Purpose : inizialization of ocean forcing
       !! ** Method  :
       !! ** Action  :
       !!----------------------------------------------------------------------
@@ -930,7 +926,7 @@ CONTAINS
                         nm_mslp_var, nm_cloud_var, nm_cloud_file, nm_nc_iyear, nm_nc_imm, nm_nc_idd, nm_nc_freq, nm_nc_tmid, y_perpetual, &
                         l_xwind, l_ywind, l_xstre, l_ystre, l_humi, l_qsr, l_qlw, l_tair, l_prec, l_mslp, l_cloud, l_snow, &
                         nm_runoff_file, runoff_data_source, runoff_climatology, nm_sss_data_file, sss_data_source, &
-                        chl_data_source, nm_chl_data_file, chl_const, use_runoff_mapper, runoff_basins_file, runoff_radius
+                        chl_data_source, nm_chl_data_file, chl_const
 
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
@@ -949,13 +945,13 @@ CONTAINS
       READ( nm_sbc_unit, nml=nam_sbc, iostat=iost )
       close( nm_sbc_unit )
       
-      if (mype==0) write(*,*) "Start: Ocean forcing initialization."
+      if (mype==0) write(*,*) "Start: Ocean forcing inizialization."
       rdate = real(julday(yearnew,1,1))
       rdate = rdate+real(daynew-1,WP)+timenew/86400._WP
       idate = int(rdate)
 
       if (mype==0) then
-         write(*,*) "Start: Ocean forcing initialization."
+         write(*,*) "Start: Ocean forcing inizialization."
          write(*,*) "Surface boundary conditions parameters:"
       end if
 
@@ -1084,7 +1080,7 @@ CONTAINS
       call nc_sbc_ini(rdate, partit, mesh)
       !==========================================================================
 #endif
-      ! runoff
+      ! runoff    
       if (runoff_data_source=='CORE1' .or. runoff_data_source=='CORE2' ) then
          ! runoff in CORE is constant in time
          ! Warning: For a global mesh, conservative scheme is to be updated!!
@@ -1099,12 +1095,11 @@ CONTAINS
          else
             if (mype==0) write(*,*) 'using constant chlorophyll concentration: ', chl_const
             chl=chl_const
-         end if
+         end if          
       end if
 
-      if (mype==0) write(*,*) "DONE:  Ocean forcing initialization."
+      if (mype==0) write(*,*) "DONE:  Ocean forcing inizialization."
       if (mype==0) write(*,*) 'Parts of forcing data (only constant in time fields) are read'
-      if (use_runoff_mapper) call read_runoff_mapper(runoff_basins_file, "arrival_point_id", runoff_radius, partit, mesh)
    END SUBROUTINE sbc_ini
 
    SUBROUTINE sbc_do(partit, mesh)
@@ -2227,265 +2222,8 @@ CONTAINS
         return
     end function lowercase 
 
-subroutine read_runoff_mapper(file, vari, R, partit, mesh)
-   ! 1. Read arrival points from the runoff mapper
-   ! 2. Create conservative remapping A*X=runoff:
-   ! A=remapping operator; X=runoff into drainage basins (in Sv); runoff= runoff im [m/s] to be put into the ocean
-   use g_config
-   use o_param
-   USE MOD_MESH
-   USE MOD_PARTIT
-   USE MOD_PARSUP
-   USE g_forcing_arrays,    only: runoff
-   use g_support
-   implicit none
- 
-#include "netcdf.inc"
-   character(*),   intent(in) :: file
-   character(*),   intent(in) :: vari
-   real(kind=WP),   intent(in):: R
-   type(t_mesh),   intent(in),    target :: mesh
-   type(t_partit), intent(inout), target :: partit
-   integer                    :: i, j, n, num, cnt, number_arrival_points, offset
-   real(kind=WP)              :: dist, W
-   integer                    :: itime, latlen, lonlen
-   integer                    :: status, ncid, varid
-   integer                    :: lonid, latid, drain_num
-   integer                    :: istart(2), icount(2)
-   real(kind=WP), allocatable :: lon(:), lat(:)
-   integer, allocatable       :: ncdata(:,:)
-   real(kind=WP), allocatable :: lon_sparse(:), lat_sparse(:), dist_min(:), dist_min_glo(:)
-   real(kind=WP), allocatable :: arrival_area(:)
-   integer, allocatable       :: data_sparse(:), dist_ind(:)
-   integer                    :: ierror           ! return error code
-!  type(sparse_matrix)        :: RUNOFF_MAPPER
- 
-#include "associate_part_def.h"
-#include "associate_mesh_def.h"
-#include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
- 
-   if (mype==0) write(*,*) 'building RUNOFF MAPPER with radius of smoothing= ', R*1.e-3, ' km'
-   if (mype==0) then
-      ! open file
-      status=nf_open(trim(file), nf_nowrite, ncid)
-   end if
- 
-   call MPI_BCast(status, 1, MPI_INTEGER, 0, MPI_COMM_FESOM, ierror)
-   if (status.ne.nf_noerr)then
-      print*,'ERROR: CANNOT READ 2D netCDF FILE CORRECTLY !!!!!'
-      print*,'Error in opening netcdf file '//file
-      call par_ex(partit%MPI_COMM_FESOM, partit%mype)
-      stop
-   endif
- 
-   if (mype==0) then
-      ! lat
-      status=nf_inq_dimid(ncid, 'lat', latid)
-      status=nf_inq_dimlen(ncid, latid, latlen)
-      ! lon
-      status=nf_inq_dimid(ncid, 'lon', lonid)
-      status=nf_inq_dimlen(ncid, lonid, lonlen)
-   end if
-   call MPI_BCast(latlen, 1, MPI_INTEGER, 0, MPI_COMM_FESOM, ierror)
-   call MPI_BCast(lonlen, 1, MPI_INTEGER, 0, MPI_COMM_FESOM, ierror)
- 
-   ! lat
-   if (mype==0) then
-      allocate(lat(latlen))
-      status=nf_inq_varid(ncid, 'lat', varid)
-      status=nf_get_vara_double(ncid,varid,1,latlen,lat)
-   end if
-
-   ! lon
-   if (mype==0) then
-      allocate(lon(lonlen))
-      status=nf_inq_varid(ncid, 'lon', varid)
-      status=nf_get_vara_double(ncid,varid,1,lonlen,lon)
-   ! make sure range 0. - 360.
-   do n=1,lonlen
-      if (lon(n)<0.0_WP) then
-         lon(n)=lon(n)+360._WP
-      end if
-   end do
-   end if
-
-   if (mype==0) then
-      allocate(ncdata(lonlen,latlen))
-      ncdata = 0.0_WP
-     ! data
-      status=nf_inq_varid(ncid, trim(vari), varid)
-      istart = (/1,1/)
-      icount= (/lonlen,latlen/)
-      status=nf_get_vara_int(ncid,varid,istart,icount,ncdata)
-     ! close file
-     status=nf_close(ncid)
-     number_arrival_points=0
-     do i=1, lonlen
-        do j=1, latlen
-           if (ncdata(i,j)>0) then
-            number_arrival_points=number_arrival_points+1
-           end if
-        end do
-     end do
-   end if
-
-   call MPI_BCast(number_arrival_points, 1, MPI_INTEGER, 0, MPI_COMM_FESOM, ierror)
-   allocate(lon_sparse(number_arrival_points), lat_sparse(number_arrival_points))
-   allocate(dist_min(number_arrival_points), dist_min_glo(number_arrival_points), dist_ind(number_arrival_points))
-   allocate(data_sparse(number_arrival_points))
-
-   if (mype==0) then
-      cnt=1
-      do i=1, lonlen
-         do j=1, latlen
-            if (ncdata(i,j)>0) then
-               lon_sparse(cnt)=lon(i)
-               lat_sparse(cnt)=lat(j)
-               data_sparse(cnt)=ncdata(i,j)
-               cnt=cnt+1
-            end if
-         end do
-      end do
-      deallocate(ncdata, lon, lat)
-   end if
-   call MPI_BCast(lon_sparse,  number_arrival_points, MPI_DOUBLE_PRECISION, 0, MPI_COMM_FESOM, ierror)
-   call MPI_BCast(lat_sparse,  number_arrival_points, MPI_DOUBLE_PRECISION, 0, MPI_COMM_FESOM, ierror)
-   call MPI_BCast(data_sparse, number_arrival_points, MPI_INTEGER, 0, MPI_COMM_FESOM, ierror)
-   drain_num=maxval(data_sparse)
-   ALLOCATE(arrival_area(drain_num))
-   arrival_area=0.0_WP ! will be used further to normalize the total flux
-   lon_sparse=lon_sparse-360.0_WP
-   lon_sparse=lon_sparse*rad
-   lat_sparse=lat_sparse*rad
-
-   do n=1, number_arrival_points
-      do i=1, myDim_nod2d
-         dist=distance_on_sphere(lon_sparse(n), lat_sparse(n), geo_coord_nod2D(1,i), geo_coord_nod2D(2,i))
-         if (i==1) then
-            dist_min(n)=dist
-            dist_ind(n)=1
-         end if
-         if (dist<dist_min(n)) then
-             dist_min(n)=dist
-             dist_ind(n)=i
-         end if
-      end do
-   end do
-   do i=1, number_arrival_points
-      dist_min_glo(i)=dist_min(i)
-   end do
-   call MPI_AllREDUCE(MPI_IN_PLACE , dist_min_glo , number_arrival_points, MPI_DOUBLE, MPI_MIN, MPI_COMM_FESOM, MPIerr)
-
-   lon_sparse=0.0_WP
-   lat_sparse=0.0_WP
-   status=0
-   do i=1, number_arrival_points
-      n=dist_ind(i)
-      if ((dist_min(i)==dist_min_glo(i)) .AND. (n<=myDim_nod2d)) then
-           lon_sparse(i)=geo_coord_nod2D(1, n)
-           lat_sparse(i)=geo_coord_nod2D(2, n)
-           status=status+1
-      end if
-   end do
-   call MPI_AllREDUCE(MPI_IN_PLACE , lon_sparse , number_arrival_points, MPI_DOUBLE, MPI_SUM, MPI_COMM_FESOM, MPIerr)
-   call MPI_AllREDUCE(MPI_IN_PLACE , lat_sparse , number_arrival_points, MPI_DOUBLE, MPI_SUM, MPI_COMM_FESOM, MPIerr)
-   call MPI_AllREDUCE(MPI_IN_PLACE , status ,     1, MPI_INTEGER, MPI_SUM, MPI_COMM_FESOM, MPIerr)
+!-----------------------------------------------------------------------
+! Copyright by the GOTM-team under the GNU Public License - www.gnu.org
+!-----------------------------------------------------------------------
 
-   if (status/=number_arrival_points) then
-      if (mype==0) then
-         write(*,*) 'RUNOFF MAPPER ERROR: total number of arrival points does not sum up among partitions: ', status, number_arrival_points
-         write(*,*) 'two different grid points have same distance to a target point!'
-      end if
-      call par_ex(partit%MPI_COMM_FESOM, partit%mype)
-      STOP
-   end if
-   RUNOFF_MAPPER%dim=myDim_nod2d+eDim_nod2D
-   ALLOCATE(RUNOFF_MAPPER%rowptr(RUNOFF_MAPPER%dim+1))
-   
-   RUNOFF_MAPPER%rowptr(1) = 1
-   
-   DO n=1, myDim_nod2d+eDim_nod2D
-      cnt=0
-      DO i=1, number_arrival_points
-         dist=distance_on_sphere(lon_sparse(i), lat_sparse(i), geo_coord_nod2D(1,n), geo_coord_nod2D(2,n))
-         if (dist < R) cnt=cnt+1
-      END DO
-      RUNOFF_MAPPER%rowptr(n+1)=RUNOFF_MAPPER%rowptr(n)+max(cnt,1)
-   END DO
-   
-   ALLOCATE(RUNOFF_MAPPER%colind(RUNOFF_MAPPER%rowptr(RUNOFF_MAPPER%dim+1)-1))
-   ALLOCATE(RUNOFF_MAPPER%values(RUNOFF_MAPPER%rowptr(RUNOFF_MAPPER%dim+1)-1))
-   DO n=1, myDim_nod2d+eDim_nod2D
-      offset=RUNOFF_MAPPER%rowptr(n)
-      cnt=0
-      W=areasvol(ulevels_nod2D(n),n)
-      DO i=1, number_arrival_points
-         j=data_sparse(i)
-         if ((j<0) .OR. (j>drain_num)) then
-            if (mype==0) then
-               write(*,*) 'RUNOFF MAPPER ERROR: arrival point has an index outside of permitted range', j, drain_num
-               write(*,*) 'two different grid points have same distance to a target point!'
-            end if
-            call par_ex(partit%MPI_COMM_FESOM, partit%mype)
-            STOP
-         end if
-         dist=distance_on_sphere(lon_sparse(i), lat_sparse(i), geo_coord_nod2D(1,n), geo_coord_nod2D(2,n))
-         if (dist < R) then
-            RUNOFF_MAPPER%values(offset+cnt)=(1.0-dist/R)
-            RUNOFF_MAPPER%colind(offset+cnt)=j
-            if (n<=myDim_nod2d) then
-               arrival_area(j)=arrival_area(j)+(1.0-dist/R)*W
-            end if
-            cnt=cnt+1
-         end if
-      END DO
-      if (cnt==0) then
-         RUNOFF_MAPPER%values(offset)=0.0_WP
-         RUNOFF_MAPPER%colind(offset)=1
-      end if
-   END DO
-
-   call MPI_AllREDUCE(MPI_IN_PLACE , arrival_area, drain_num, MPI_DOUBLE, MPI_SUM, MPI_COMM_FESOM, MPIerr)
-
-   DO i=1, drain_num
-      where (RUNOFF_MAPPER%colind==i)
-            RUNOFF_MAPPER%values=RUNOFF_MAPPER%values/arrival_area(i)
-      end where
-   END DO
-
-   deallocate(lon_sparse, lat_sparse, dist_min, dist_min_glo)
-   deallocate(arrival_area)
-   deallocate(data_sparse, dist_ind)
-
-do n=1, myDim_nod2D+eDim_nod2D
-   i=RUNOFF_MAPPER%rowptr(n)
-   j=RUNOFF_MAPPER%rowptr(n+1)-1
-   runoff(n)=sum(RUNOFF_MAPPER%values(i:j))
-end do
-
-call integrate_nod(runoff, W, partit, mesh)
-
-if (mype==0) write(*,*) 'RUNOFF MAPPER check (total amount of basins):', drain_num
-if (mype==0) write(*,*) 'RUNOFF MAPPER check (input of 1Sv from each basin results in runoff of):', W, ' Sv'
-
-!allocate(BASIN_RUNOFF(drain_num))
-!BASIN_RUNOFF=0.0_WP
-end subroutine read_runoff_mapper
-
-real(kind=WP) function distance_on_sphere(lon1, lat1, lon2, lat2)
-!   use, intrinsic :: ISO_FORTRAN_ENV
-    use o_param
-    use g_config
-    implicit none    
-!lons & lats are in radians
-    real(kind=WP), intent(in) :: lon1, lat1, lon2, lat2
-    real(kind=WP)             :: r, delta_lon, delta_lat
-    
-   delta_lon=abs(lon1-lon2)
-   if (delta_lon > cyclic_length/2.0_WP) delta_lon=delta_lon-cyclic_length
-   delta_lat=(lat1-lat2)
-   r = sin(delta_lat/2.0)**2 + cos(lat1) * cos(lat2) * sin(delta_lon/2.0)**2
-   distance_on_sphere=2.0 * atan2(sqrt(r), sqrt(1.0 - r))*r_earth
-end function distance_on_sphere
 END MODULE g_sbf
diff --git a/src/icb_allocate.F90 b/src/icb_allocate.F90
index 169f7312..73bff016 100644
--- a/src/icb_allocate.F90
+++ b/src/icb_allocate.F90
@@ -1,21 +1,33 @@
-subroutine allocate_icb(partit)
+subroutine allocate_icb(partit, mesh)
   use iceberg_params
   use g_config
+  use g_comm
+  use g_comm_auto
+  use o_param
   use MOD_PARTIT
+  use MOD_MESH
 
   integer       :: n2
 type(t_partit), intent(inout), target :: partit
+type(t_mesh), intent(in), target :: mesh
 #include "associate_part_def.h"
+#include "associate_mesh_def.h"
 #include "associate_part_ass.h"
+#include "associate_mesh_ass.h"
   n2=myDim_nod2D+eDim_nod2D
+  icb_outfreq = step_per_day / steps_per_ib_step
 
   allocate(ibhf(n2), ibfwb(n2), ibfwl(n2), ibfwe(n2), ibfwbv(n2))
-  ibhf=0
-  ibfwb=0
-  ibfwl=0
-  ibfwe=0
-  ibfwbv=0
+  ibhf      = 0.0
+  ibfwb     = 0.0
+  ibfwl     = 0.0
+  ibfwe     = 0.0
+  ibfwbv    = 0.0
+  allocate(ibhf_n(mesh%nl, n2))
+  ibhf_n    = 0.0_WP
 
+  allocate(T_ave(ib_num))
+  T_ave = 0.0
   allocate(calving_day(ib_num))
   calving_day = 1   !28.0: September 29 for restart in 1 SEP 97 ! 271.0: September 29 for year 1997
   allocate(height_ib(ib_num))
@@ -83,16 +95,29 @@ type(t_partit), intent(inout), target :: partit
   allocate(fwl_flux_ib(ib_num))
   allocate(fwb_flux_ib(ib_num))
   allocate(fwbv_flux_ib(ib_num))
-  allocate(heat_flux_ib(ib_num))
-  allocate(lheat_flux_ib(ib_num))
+  allocate(hfe_flux_ib(ib_num))
+  allocate(hfl_flux_ib(ib_num))
+  allocate(hfb_flux_ib(ib_num))
+  allocate(hfbv_flux_ib(ib_num))
+  allocate(lhfb_flux_ib(ib_num))
   fwe_flux_ib = 0.0
   fwl_flux_ib = 0.0
   fwb_flux_ib = 0.0
   fwbv_flux_ib = 0.0
-  heat_flux_ib = 0.0
-  lheat_flux_ib = 0.0
+  hfe_flux_ib = 0.0
+  hfl_flux_ib = 0.0
+  hfb_flux_ib = 0.0
+  hfbv_flux_ib = 0.0
+  lhfb_flux_ib = 0.0
   allocate(arr_block(15*ib_num))
   allocate(elem_block(ib_num))
+  allocate(pe_block(ib_num))
+  
+  allocate(elem_area_glob(elem2D))
+  elem_area_glob=0.0
+  call gather_elem(elem_area(1:myDim_elem2D), elem_area_glob, partit)
+  call MPI_Bcast(elem_area_glob, elem2D, MPI_DOUBLE, 0, MPI_COMM_FESOM, MPIERR)
+
   allocate(vl_block(4*ib_num))
   allocate(buoy_props(ib_num,13))
   buoy_props = 0.0
diff --git a/src/icb_coupling.F90 b/src/icb_coupling.F90
index a715f392..a9ed4573 100644
--- a/src/icb_coupling.F90
+++ b/src/icb_coupling.F90
@@ -5,15 +5,16 @@ subroutine reset_ib_fluxes()
     use g_config
     use iceberg_params
 
-    ibfwbv = 0
-    ibfwb = 0
-    ibfwl = 0
-    ibfwe = 0
-    ibhf = 0
+    ibfwbv  = 0.0
+    ibfwb   = 0.0
+    ibfwl   = 0.0
+    ibfwe   = 0.0
+    ibhf    = 0.0
+    ibhf_n  = 0.0_WP
 end subroutine
 
 
-subroutine prepare_icb2fesom(mesh, partit, ib,i_have_element,localelement,depth_ib)
+subroutine prepare_icb2fesom(mesh, partit, ib,i_have_element,localelement,depth_ib,height_ib_single)
     !transmits the relevant fields from the iceberg to the ocean model
     !Lars Ackermann, 17.03.2020
 
@@ -23,14 +24,19 @@ subroutine prepare_icb2fesom(mesh, partit, ib,i_have_element,localelement,depth_
     use MOD_PARTIT
     use iceberg_params
 
+    implicit none
+
     logical                 :: i_have_element
-    real                    :: depth_ib
+    real, intent(in)        :: depth_ib, height_ib_single
+    real                    :: lev_low, lev_up
     integer                 :: localelement
     integer                 :: iceberg_node  
     integer, dimension(3)   :: ib_nods_in_ib_elem
     integer                 :: num_ib_nods_in_ib_elem
-    real                    :: tot_area_nods_in_ib_elem
-    integer                 :: i, ib
+    real                    :: dz
+    real, dimension(:), allocatable      :: tot_area_nods_in_ib_elem
+    integer                 :: i, j, k, ib
+    integer, dimension(3)   :: idx_d
 type(t_mesh), intent(in) , target :: mesh
 type(t_partit), intent(inout), target :: partit
 #include "associate_part_def.h"
@@ -39,30 +45,87 @@ type(t_partit), intent(inout), target :: partit
 #include "associate_mesh_ass.h"
 
     if(i_have_element) then 
+        dz = 0.0
+        allocate(tot_area_nods_in_ib_elem(mesh%nl))
+
         num_ib_nods_in_ib_elem=0
-        tot_area_nods_in_ib_elem=0
+        tot_area_nods_in_ib_elem=0.0
+        idx_d = 0
 
         do i=1,3
             iceberg_node=elem2d_nodes(i,localelement)
 
+            ! LOOP: consider all neighboring pairs (n_up,n_low) of 3D nodes
+            ! below n2..
+            !innerloop: do k=1, nl+1
+            do k=1, nlevels_nod2D(iceberg_node)
+                idx_d(i) = k
+                lev_up  = mesh%zbar_3d_n(k, iceberg_node)
+
+                if( k==nlevels_nod2D(iceberg_node) ) then
+                    lev_low = mesh%zbar_n_bot(iceberg_node)
+                else
+                    lev_low = mesh%zbar_3d_n(k+1, iceberg_node)
+                end if
+                
+                if( abs(lev_low)==abs(lev_up) ) then
+                    idx_d(i) = idx_d(i) - 1
+                    exit
+                else if( abs(lev_low)>=abs(depth_ib) ) then
+                    exit
+                else
+                    cycle
+                end if
+            end do
+                
             if (iceberg_node<=mydim_nod2d) then
-                ib_nods_in_ib_elem(i)   = iceberg_node
-                num_ib_nods_in_ib_elem  = num_ib_nods_in_ib_elem + 1
-                tot_area_nods_in_ib_elem= tot_area_nods_in_ib_elem + mesh%area(1,iceberg_node)
+                ib_nods_in_ib_elem(i)           = iceberg_node
+                num_ib_nods_in_ib_elem          = num_ib_nods_in_ib_elem + 1
+                tot_area_nods_in_ib_elem        = tot_area_nods_in_ib_elem + mesh%area(:,iceberg_node)
             else
-                ib_nods_in_ib_elem(i)   = 0
+                ib_nods_in_ib_elem(i)           = 0
             end if
         end do
-        
+       
         do i=1, 3
             iceberg_node=ib_nods_in_ib_elem(i)
 
             if (iceberg_node>0) then
-                ibfwbv(iceberg_node) = ibfwbv(iceberg_node) - fwbv_flux_ib(ib) / tot_area_nods_in_ib_elem
-                ibfwb(iceberg_node) = ibfwb(iceberg_node) - fwb_flux_ib(ib) / tot_area_nods_in_ib_elem
-                ibfwl(iceberg_node) = ibfwl(iceberg_node) - fwl_flux_ib(ib) / tot_area_nods_in_ib_elem
-                ibfwe(iceberg_node) = ibfwe(iceberg_node) - fwe_flux_ib(ib) / tot_area_nods_in_ib_elem
-                ibhf(iceberg_node) = ibhf(iceberg_node) - heat_flux_ib(ib) / tot_area_nods_in_ib_elem
+                ibfwbv(iceberg_node) = ibfwbv(iceberg_node) - fwbv_flux_ib(ib) / tot_area_nods_in_ib_elem(1)
+                ibfwb(iceberg_node) = ibfwb(iceberg_node) - fwb_flux_ib(ib) / tot_area_nods_in_ib_elem(1)
+                ibfwl(iceberg_node) = ibfwl(iceberg_node) - fwl_flux_ib(ib) / tot_area_nods_in_ib_elem(1)
+                ibfwe(iceberg_node) = ibfwe(iceberg_node) - fwe_flux_ib(ib) / tot_area_nods_in_ib_elem(1)
+                !ibhf(iceberg_node) = ibhf(iceberg_node) - hfb_flux_ib(ib) / tot_area_nods_in_ib_elem(1)
+                
+                do j=1,idx_d(i)
+                    lev_up  = mesh%zbar_3d_n(j, iceberg_node)
+                    if( j==nlevels_nod2D(iceberg_node) ) then
+                        lev_low = mesh%zbar_n_bot(iceberg_node)
+                    else
+                        lev_low = mesh%zbar_3d_n(j+1, iceberg_node)
+                    end if
+                    dz = abs( lev_low - lev_up )
+                    if( (abs(lev_low)>=abs(depth_ib)) .and. (abs(lev_up)<abs(depth_ib)) ) then 
+                        dz = abs( lev_up - abs(depth_ib) )
+                    end if              
+                   
+!                    write(*,*) "LA DEBUG: hfbv_flux_ib(ib)=",hfbv_flux_ib(ib),", hfb_flux_ib(ib)=",hfb_flux_ib(ib),", hfl_flux_ib(ib)=",hfl_flux_ib(ib),", hfe_flux_ib(ib)=",hfe_flux_ib(ib)
+                    if( depth_ib==0.0 ) then
+                        ibhf_n(j,iceberg_node) = ibhf_n(j,iceberg_node) & 
+                                                    - ((hfbv_flux_ib(ib)+hfl_flux_ib(ib)) &
+                                                    + hfe_flux_ib(ib)) / tot_area_nods_in_ib_elem(j)
+                    else
+                        ibhf_n(j,iceberg_node) = ibhf_n(j,iceberg_node) & 
+                                                    - ((hfbv_flux_ib(ib)+hfl_flux_ib(ib)) * (dz / abs(depth_ib)) & 
+                                                    + hfe_flux_ib(ib) * (dz / abs(height_ib_single))) &
+                                                    / tot_area_nods_in_ib_elem(j)
+                    end if
+!                    write(*,*) "LA DEBUG: ibhf_n(j,iceberg_node)=",ibhf_n(j,iceberg_node),", height_ib_single=",height_ib_single
+                end do
+                ibhf_n(idx_d(i),iceberg_node) = ibhf_n(idx_d(i),iceberg_node) - hfb_flux_ib(ib) / tot_area_nods_in_ib_elem(idx_d(i))
+                if( height_ib_single .ne. 0.0 ) then
+                    ibhf_n(1,iceberg_node) = ibhf_n(1,iceberg_node) - hfe_flux_ib(ib) * ((abs(height_ib_single)-abs(depth_ib))/abs(height_ib_single)) / tot_area_nods_in_ib_elem(1)
+                end if
             end if
         end do
     end if
@@ -90,16 +153,13 @@ type(t_partit), intent(inout), target :: partit
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
-  
-    fresh_wa_flux => ice%flx_fw(:)
-    net_heat_flux => ice%flx_h(:)
 
     do n=1, myDim_nod2d+eDim_nod2D
-        if (.not.turn_off_hf) then
-                net_heat_flux(n)   = net_heat_flux(n) + ibhf(n) 
-        end if
+        !if (.not.turn_off_hf) then
+        !        heat_flux(n)   = heat_flux(n) - ibhf(n) !* steps_per_ib_step
+        !end if
         if (.not.turn_off_fw) then
-                fresh_wa_flux(n)   = fresh_wa_flux(n) + (ibfwb(n)+ibfwl(n)+ibfwe(n)+ibfwbv(n))
+                water_flux(n)  = water_flux(n) - (ibfwb(n)+ibfwl(n)+ibfwe(n)+ibfwbv(n)) !* steps_per_ib_step
         end if
     end do
 !---wiso-code-begin
diff --git a/src/icb_dyn.F90 b/src/icb_dyn.F90
index 7a0fb436..e64a686e 100644
--- a/src/icb_dyn.F90
+++ b/src/icb_dyn.F90
@@ -35,7 +35,7 @@ subroutine iceberg_dyn(mesh, partit, ice, dynamics, ib, new_u_ib, new_v_ib, u_ib
  use g_forcing_arrays 	!for u_wind, v_wind or u_wind_ib, v_wind_ib respectively
  use o_arrays, only: Tsurf_ib, Ssurf_ib
  use o_param		!for dt
- !use iceberg_params,only: l_melt, coriolis_scale !are icebergs allowed to melt?
+ use iceberg_params, only: T_ave !l_melt, coriolis_scale !are icebergs allowed to melt?
 
  integer, intent(IN) 	:: ib !current iceberg's index
  real,    intent(OUT)	:: new_u_ib, new_v_ib
@@ -90,16 +90,20 @@ type(t_dyn)   , intent(inout), target :: dynamics
  ! - (uo_skin_ib, vo_skin_ib)	: velocity below the draft of the iceberg
  ! call iceberg_avvelo_ufkeel(uo_dz,vo_dz, uo_keel,vo_keel, depth_ib,iceberg_elem) 
  call iceberg_average_andkeel(mesh, partit, dynamics, uo_dz,vo_dz, uo_keel,vo_keel, T_dz,S_dz, T_keel,S_keel, depth_ib,iceberg_elem, ib)
+ write(*,*) "LA DEBUG: dyn 1"
  call FEM_3eval(mesh, partit, uo_ib,vo_ib,lon,lat,uo_dz,vo_dz,iceberg_elem)
+ write(*,*) "LA DEBUG: dyn 2"
  call FEM_3eval(mesh, partit, uo_skin_ib,vo_skin_ib,lon,lat,uo_keel,vo_keel,iceberg_elem)
  
 
  !TEMPERATURE AND SALINITY:
  ! - T_ave_ib, S_ave_ib		: Mean T & S (integrated) at location of iceberg
  ! - T_keel_ib, S_keel_ib	: T & S below the draft of the iceberg (depth_ib)
+ write(*,*) "LA DEBUG: dyn 3"
  call FEM_3eval(mesh, partit, T_ave_ib,S_ave_ib,lon,lat,T_dz,S_dz,iceberg_elem)
+ write(*,*) "LA DEBUG: dyn 4"
  call FEM_3eval(mesh, partit, T_keel_ib,S_keel_ib,lon,lat,T_keel,S_keel,iceberg_elem)
-
+ T_ave(ib) = T_ave_ib
 
  !ATMOSPHERIC VELOCITY ua_ib, va_ib
  call FEM_eval(mesh, partit, ua_ib,va_ib,lon,lat,u_wind_ib,v_wind_ib,iceberg_elem)      
@@ -110,6 +114,7 @@ type(t_dyn)   , intent(inout), target :: dynamics
  !ICE THICKNESS (CONCENTRATION) hi_ib, conci_ib
  hi_ib3    => ice%data(size(ice%data))%values(:) !ice%m_ice_ib(tmp_arr)
  conci_ib3 => ice%data(size(ice%data)-1)%values(:) !ice%a_ice_ib(tmp_arr) 
+ write(*,*) "LA DEBUG: dyn 5"
  call FEM_3eval(mesh, partit, hi_ib,conci_ib,lon,lat,hi_ib3,conci_ib3,iceberg_elem)
  P_ib = 20000. * hi_ib * exp(-20.*(1-conci_ib))
  
@@ -135,11 +140,12 @@ type(t_dyn)   , intent(inout), target :: dynamics
  if(l_melt) then
 
   call FEM_eval(mesh, partit, sst_ib,sss_ib,lon,lat,Tsurf_ib,Ssurf_ib,iceberg_elem)
-  call iceberg_meltrates(	M_b, M_v, M_e, M_bv, &
+  
+  call iceberg_meltrates(partit, M_b, M_v, M_e, M_bv, &
 				u_ib,v_ib, uo_ib,vo_ib, ua_ib,va_ib, &
 				sst_ib, length_ib, conci_ib, &
-				uo_skin_ib, vo_skin_ib, T_keel_ib, S_keel_ib, depth_ib, &
-				T_ave_ib, S_ave_ib, ib)
+				uo_skin_ib, vo_skin_ib, T_keel_ib, S_keel_ib, depth_ib, height_ib, &
+				T_ave_ib, S_ave_ib, ib, rho_icb)
 
   call iceberg_newdimensions(partit, ib, depth_ib,height_ib,length_ib,width_ib,M_b,M_v,M_e,M_bv, &
 			     rho_h2o, rho_icb, file3)
@@ -644,19 +650,26 @@ type(t_partit), intent(inout), target :: partit
    ! below n2..
    !innerloop: do k=1, nl+1
    innerloop: do k=1, nlevels_nod2D(n2)
-
-    if( k==1 ) then
-        lev_up = 0.0
-    else
-        lev_up  = mesh%Z_3d_n_ib(k-1, n2)
-        !lev_up  = mesh%Z_3d_n_ib(k-1, n2)
-    end if
+    lev_up  = mesh%zbar_3d_n(k, n2)
 
     if( k==nlevels_nod2D(n2) ) then
         lev_low = mesh%zbar_n_bot(n2)
     else
-        lev_low = mesh%Z_3d_n_ib(k, n2)
+        lev_low = mesh%zbar_3d_n(k+1, n2)
     end if
+
+    !if( k==1 ) then
+    !    lev_up = 0.0
+    !else
+    !    lev_up  = mesh%Z_3d_n_ib(k-1, n2)
+    !    !lev_up  = mesh%Z_3d_n_ib(k-1, n2)
+    !end if
+
+    !if( k==nlevels_nod2D(n2) ) then
+    !    lev_low = mesh%zbar_n_bot(n2)
+    !else
+    !    lev_low = mesh%Z_3d_n_ib(k, n2)
+    !end if
     dz = abs( lev_low - lev_up )
     
     !if( abs(lev_up)>=abs(depth_ib) ) then
@@ -694,7 +707,7 @@ type(t_partit), intent(inout), target :: partit
     if( abs(lev_low)>=abs(depth_ib) ) then !.AND. (abs(lev_up)<=abs(depth_ib)) ) then
 #endif
       if( abs(lev_up)<abs(depth_ib) ) then
-        dz = abs ( lev_up - depth_ib )
+        dz = abs(lev_up - depth_ib)
       else
         ! LA: Never go here, when starting with k=1
         dz = abs(depth_ib)
diff --git a/src/icb_elem.F90 b/src/icb_elem.F90
index c989f24d..7729b757 100644
--- a/src/icb_elem.F90
+++ b/src/icb_elem.F90
@@ -63,6 +63,7 @@ eta_n_ib => dynamics%eta_n_ib(:)
   nablaeta(1) = sum( eta_n_ib(elem2D_nodes(:,elem)) * gradient_sca(1:3, elem)) 
   nablaeta(2) = sum( eta_n_ib(elem2D_nodes(:,elem)) * gradient_sca(4:6, elem)) 
  else
+ write(*,*) "LA DEBUG: elem 1"
   call FEM_3eval(mesh, partit, nablaeta(1),nablaeta(2),lon_rad,lat_rad,gradientx,gradienty,elem)
  end if
   
@@ -175,6 +176,7 @@ type(t_partit), intent(inout), target :: partit
   
   !values of the 3 local basisfunctions at the 
   !position 'coords'
+!  write(*,*) "LA DEBUG: 1 - elem=",elem,", coords_tmp=",coords_tmp,", phi=",phi
   call locbafu_2D(mesh, partit, phi,elem,coords_tmp)
 
   values_u = field_u(elem2D_nodes(:,elem))
@@ -353,6 +355,7 @@ type(t_partit), intent(inout), target :: partit
   !values of the 3 local basisfunctions at the 
   !position 'coords'
   coords_tmp = [lon_deg, lat_deg]
+!  write(*,*) "LA DEBUG: 2 - elem=",elem,", coords_tmp=",coords_tmp,", phi=",phi
   call locbafu_2D(mesh, partit, phi,elem, coords_tmp)
   
   u_at_ib = sum( ocean_u(:) * phi(:))
@@ -376,9 +379,6 @@ end subroutine FEM_3eval
 subroutine iceberg_elem4all(mesh, partit, elem, lon_deg, lat_deg) 
  USE MOD_MESH
  use MOD_PARTIT		!for myDim_nod2D, myList_elem2D
-!#ifdef use_cavity
-! use iceberg_params, only: reject_elem
-!#endif
  
  implicit none
  
@@ -392,36 +392,28 @@ type(t_partit), intent(inout), target :: partit
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
- 
+
+  write(*,*) "LA DEBUG: point_in_triangle 2 - lon/lat", lon_deg, "/", lat_deg
   call point_in_triangle(mesh, partit, elem, (/lon_deg, lat_deg/)) !all PEs search here
   i_have_element= (elem .ne. 0) !up to 3 PEs .true.
   
   if(i_have_element) then
    i_have_element= elem2D_nodes(1,elem) <= myDim_nod2D !1 PE still .true.
-#ifdef use_cavity
-   if( reject_elem(mesh, elem) ) then
+   if( (use_cavity) .and. (reject_elem(mesh, partit, elem) )) then
     elem=0 !reject element
     i_have_element=.false.
-    !write(*,*) 'elem4all: iceberg found in shelf region: elem = 0'
    else 
     elem=myList_elem2D(elem) !global now
    end if 
-#else
-   elem=myList_elem2D(elem) !global now
-#endif 
   end if
   call com_integer(partit, i_have_element,elem) !max 1 PE sends element here; 
 end subroutine iceberg_elem4all
 
 
  !***************************************************************************************************************************
- !***************************************************************************************************************************
 
 subroutine find_new_iceberg_elem(mesh, partit, old_iceberg_elem, pt, left_mype)
   use o_param
-!#ifdef use_cavity
-!  use iceberg_params, only: reject_elem
-!#endif
 
   implicit none
   
@@ -456,19 +448,17 @@ do m=1, 3
   !elem_containing_n2 = nod_in_elem2D(n2)%addresses(idx_elem_containing_n2)
   elem_containing_n2 = nod_in_elem2D(idx_elem_containing_n2,n2) 
     
+!  write(*,*) "LA DEBUG: 3 - elem=",elem_containing_n2,", coords_tmp=",pt,", phi=",werte2D
   call locbafu_2D(mesh, partit, werte2D, elem_containing_n2, pt)
    
   if (ALL(werte2D <= 1.+ 1.0e-07) .AND. ALL(werte2D >= 0.0- 1.0e-07) ) then
    old_iceberg_elem=elem_containing_n2
    
-#ifdef use_cavity
-   if( reject_elem(mesh, old_iceberg_elem) ) then
+   if( (use_cavity) .and. (reject_elem(mesh, partit, old_iceberg_elem) )) then
       left_mype=1.0
       !write(*,*) 'iceberg found in shelf region: left_mype = 1'
       old_iceberg_elem=ibelem_tmp
    end if
-#endif
-
    RETURN 
   end if
  end do
@@ -511,7 +501,7 @@ type(t_partit), intent(inout), target :: partit
   el2D=0
   !DO l=1,elem2D
   DO l=1,partit%myDim_elem2D
- 
+     !write(*,*) "LA DEBUG: 4 - coords_tmp=",pt,", phi=",werte2D, ", l=", l, ", nodes=",mesh%elem2D_nodes(:,l)
      call locbafu_2D(mesh, partit, werte2D, l, pt)
      
      if (ALL(werte2D <= 1.+ 1.0e-07) .AND. ALL(werte2D >= 0.0- 1.0e-07) ) then
diff --git a/src/icb_modules.F90 b/src/icb_modules.F90
index cda937e6..54b07794 100644
--- a/src/icb_modules.F90
+++ b/src/icb_modules.F90
@@ -1,4 +1,10 @@
 module iceberg_params
+use MOD_PARTIT
+USE MOD_MESH
+use g_config, only: use_cavity, use_cavityonelem
+use, intrinsic :: iso_fortran_env, only: real64
+USE o_PARAM, only: WP
+
 implicit none
 save
   !integer,parameter :: ib_num               ! realistic dataset comprising 6912 icebergs
@@ -74,7 +80,7 @@ save
   character(100):: scaling_file='icb_scaling.dat' !scaling factor
   
   !===== OUTPUT RELATED SETTINGS  =====
-  integer :: icb_outfreq = 180           ! 180; for FESOM_dt=2min this is 6 hourly output !120; for FESOM_dt=3min this is 6 hourly output
+  integer :: icb_outfreq           ! 180; for FESOM_dt=2min this is 6 hourly output !120; for FESOM_dt=3min this is 6 hourly output
   logical :: l_geo_out = .true.         ! output in unrotated (.true.) or rotated coordinates
   logical :: ascii_out = .false.        ! old ascii output (slow, more detailed); false: faster nc output
   !===== NUMERICS (DONT HAVE TO BE CHANGED) =====
@@ -82,16 +88,19 @@ save
   real :: semiimplicit_coeff = 1.0      !1. fully implicit, 0.5 no damping
   real :: AB_coeff = 1.53               !1.5 original AB (amplifying), 1.6 stabilized
   !===== NOTHING MUST BE CHANGED BELOW THIS LINE =====
+  real,dimension(:), allocatable:: T_ave
   real,dimension(:), allocatable:: u_ib, v_ib
   integer,dimension(:), allocatable:: iceberg_elem
   logical,dimension(:), allocatable:: find_iceberg_elem
   real,dimension(:), allocatable:: f_u_ib_old, f_v_ib_old
   real,dimension(:), allocatable:: bvl_mean, lvlv_mean, lvle_mean, lvlb_mean !averaged volume losses
-  !real,dimension(:), allocatable:: fw_flux_ib, heat_flux_ib
-  real,dimension(:), allocatable:: fwe_flux_ib, fwl_flux_ib, fwb_flux_ib, fwbv_flux_ib, heat_flux_ib, lheat_flux_ib
+  !real,dimension(:), allocatable:: fw_flux_ib, hfb_flux_ib
+  real,dimension(:), allocatable:: fwe_flux_ib, fwl_flux_ib, fwb_flux_ib, fwbv_flux_ib
+  real,dimension(:), allocatable:: hfe_flux_ib, hfl_flux_ib, hfb_flux_ib, hfbv_flux_ib, lhfb_flux_ib
   
   !===== FRESHWATER AND HEAT ARRAYS ON FESOM GRID =====
   real,dimension(:), allocatable:: ibhf    !icb heat flux into ocean 
+  real(kind=WP),dimension(:,:), allocatable:: ibhf_n    !icb heat flux into ocean 
   real,dimension(:), allocatable:: ibfwb   !freshwater flux into ocean from basal melting
   real,dimension(:), allocatable:: ibfwbv   !freshwater flux into ocean from basal melting
   real,dimension(:), allocatable:: ibfwl   !freshwater flux into ocean from lateral melting
@@ -104,6 +113,8 @@ save
   !for communication
   real,dimension(:), allocatable:: arr_block
   integer,dimension(:), allocatable:: elem_block
+  integer,dimension(:), allocatable:: pe_block
+  real(real64), dimension(:), allocatable:: elem_area_glob
   real,dimension(:), allocatable:: vl_block
 
   !array for output in netcdf
@@ -111,35 +122,50 @@ save
   integer:: save_count_buoys
   real:: prev_sec_in_year
 !****************************************************************************************************************************
-!****************************************************************************************************************************
-#ifdef use_cavity
  contains
  ! true if all nodes of the element are either "real" model boundary nodes or shelf nodes
- logical function reject_elem(mesh, elem)
- USE MOD_MESH
+ logical function reject_elem(mesh, partit, elem)
  implicit none
  integer, intent(in) :: elem
 type(t_mesh), intent(in) , target :: mesh
+type(t_partit), intent(inout), target :: partit
+#include "associate_part_def.h"
 #include "associate_mesh_def.h"
+#include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
 
+if (use_cavity) then
 ! kh 09.08.21 change index_nod2d -> bc_index_nod2d?
- reject_elem = all( (cavity_flag_nod2d(elem2D_nodes(:,elem))==1) .OR. (index_nod2d(elem2D_nodes(:,elem))==1) )
+ if (.not. use_cavityonelem) then
+   reject_elem = all( (mesh%cavity_depth(mesh%elem2D_nodes(:,elem))/=0.0) .OR. (mesh%bc_index_nod2D(mesh%elem2D_nodes(:,elem))==0.0) )
+ !else
+ end if
+else
+ reject_elem = all( (mesh%bc_index_nod2D(mesh%elem2D_nodes(:,elem))==0.0) )
+endif
  end function reject_elem
  
  ! gives number of "coastal" nodes in cavity setup, i.e. number of nodes that are
  ! either "real" model boundary nodes or shelf nodes
- integer function coastal_nodes(mesh, elem)
- USE MOD_MESH
+ integer function coastal_nodes(mesh, partit, elem)
  implicit none
  integer, intent(in) :: elem
 type(t_mesh), intent(in) , target :: mesh
+type(t_partit), intent(inout), target :: partit
 #include "associate_part_def.h"
+#include "associate_mesh_def.h"
 #include "associate_part_ass.h"
+#include "associate_mesh_ass.h"
 
+if (use_cavity) then
 ! kh 09.08.21 change index_nod2d -> bc_index_nod2d?
- coastal_nodes = count( (cavity_flag_nod2d(elem2D_nodes(:,elem))==1) .OR. (index_nod2d(elem2D_nodes(:,elem))==1) )
+ if (.not. use_cavityonelem) then
+   coastal_nodes = count( (mesh%cavity_depth(mesh%elem2D_nodes(:,elem))/=0.0) .OR. (mesh%bc_index_nod2D(mesh%elem2D_nodes(:,elem))==0.0) )
+ !else
+ end if
+else 
+ coastal_nodes = count( (mesh%bc_index_nod2D(mesh%elem2D_nodes(:,elem))==0.0) )
+endif
  end function coastal_nodes
-#endif
 
 end module iceberg_params
diff --git a/src/icb_step.F90 b/src/icb_step.F90
index 03457192..8f9f88e2 100644
--- a/src/icb_step.F90
+++ b/src/icb_step.F90
@@ -11,7 +11,6 @@ implicit none
 
   public    ::  iceberg_calculation
   public    ::  iceberg_step1
-  public    ::  get_total_iceberg_area
   public    ::  iceberg_step2
   public    ::  initialize_velo
   public    ::  trajectory
@@ -53,12 +52,14 @@ subroutine iceberg_calculation(ice, mesh, partit, dynamics, istep)
  real(kind=8) 	:: t0, t1, t2, t3, t4, t0_restart, t1_restart   	!=
  logical	:: firstcall=.true. 					!=
  logical	:: lastsubstep  					!=
- 									!=
+       
  real		:: arr_from_block(15)					!=
  integer	:: elem_from_block					!=  
  real		:: vl_from_block(4)					!=	
  real,dimension(15*ib_num):: arr_block_red				!=
  integer,dimension(ib_num):: elem_block_red				!=
+ integer,dimension(ib_num):: pe_block_red				!=
+ integer    :: n
  real, dimension(4*ib_num):: vl_block_red				!=
 
 type(t_ice), intent(inout), target :: ice
@@ -73,7 +74,6 @@ type(t_dyn)   , intent(inout), target :: dynamics
 
 ! kh 16.03.21 (asynchronous) iceberg computation starts with the content in common arrays at istep and will merge its results at istep_end_synced
  istep_end_synced = istep + steps_per_ib_step - 1
-
  if(firstcall) then
   !overwrite icb_modules if restart, initialize netcdf output if no restart:
   
@@ -97,6 +97,7 @@ type(t_dyn)   , intent(inout), target :: dynamics
  !faster communication via ALLREDUCE
  arr_block = 0.0
  elem_block = 0
+ pe_block = 0
  !for communication of averaged volume losses
  vl_block = 0.0
 
@@ -138,6 +139,7 @@ type(t_dyn)   , intent(inout), target :: dynamics
  
  arr_block_red = 0.0
  elem_block_red= 0
+ pe_block_red= 0
  vl_block_red = 0.0
 
 !$omp critical 
@@ -162,6 +164,29 @@ completed = .false.
 !$omp end critical
  end do
 
+!$omp critical 
+ call MPI_IAllREDUCE(pe_block, pe_block_red, ib_num, MPI_INTEGER, MPI_SUM, partit%MPI_COMM_FESOM_IB, req, partit%MPIERR_IB)  
+!$omp end critical
+
+completed = .false.
+ do while (.not. completed)
+!$omp critical
+  CALL MPI_TEST(req, completed, status, partit%MPIERR_IB)
+!$omp end critical
+ end do
+
+!!$omp critical 
+! call MPI_IAllREDUCE(elem_area_block, elem_area_block_red, ib_num, MPI_REAL, MPI_SUM, partit%MPI_COMM_FESOM_IB, req, partit%MPIERR_IB)  
+!!$omp end critical
+!
+!completed = .false.
+! do while (.not. completed)
+!!$omp critical
+!  CALL MPI_TEST(req, completed, status, partit%MPIERR_IB)
+!!$omp end critical
+! end do
+
+
 !$omp critical 
  call MPI_IAllREDUCE(vl_block, vl_block_red, 4*ib_num, MPI_DOUBLE_PRECISION, MPI_SUM, partit%MPI_COMM_FESOM_IB, req, partit%MPIERR_IB)
 !$omp end critical
@@ -206,7 +231,8 @@ completed = .false.
                             conc_sill(ib),P_sill(ib), rho_h2o(ib),rho_air(ib),rho_ice(ib),	   	& 
                             u_ib(ib),v_ib(ib), iceberg_elem(ib), find_iceberg_elem(ib), lastsubstep,&
                             f_u_ib_old(ib), f_v_ib_old(ib), l_semiimplicit,   &
-                            semiimplicit_coeff, AB_coeff, istep)	
+                            semiimplicit_coeff, AB_coeff, istep, elem_block_red, &
+                            pe_block_red)	
     end if
   end if
 end do
@@ -243,6 +269,7 @@ end do
    write(*,*) 'reading restart took', t1_restart-t0_restart
    write(*,*) '*************************************************************'
  end if
+
 end subroutine iceberg_calculation
 
 
@@ -250,7 +277,7 @@ end subroutine iceberg_calculation
 !****************************************************************************************************************************
 
 
-subroutine iceberg_step1(ice, mesh, partit, dynamics, ib, height_ib,length_ib,width_ib, lon_deg,lat_deg, &
+subroutine iceberg_step1(ice, mesh, partit, dynamics, ib, height_ib_single,length_ib_single,width_ib_single, lon_deg,lat_deg, &
 			Co,Ca,Ci, Cdo_skin,Cda_skin, rho_icb, 		   &
 			conc_sill,P_sill, rho_h2o,rho_air,rho_ice,	   & 
 			u_ib,v_ib, iceberg_elem, find_iceberg_elem, 	   &
@@ -264,8 +291,7 @@ subroutine iceberg_step1(ice, mesh, partit, dynamics, ib, height_ib,length_ib,wi
  use g_rotate_grid	!for subroutine g2r, logfile_outfreq					!=
  use g_config, only: steps_per_ib_step
  !=
-!#ifdef use_cavity
-! use iceberg_params, only: smallestvol_icb, arr_block, elem_block, l_geo_out, icb_outfreq, l_allowgrounding, draft_scale, reject_elem, melted, grounded, scaling !, length_ib, width_ib, scaling
+use iceberg_params, only: length_ib, width_ib, scaling, elem_block, elem_area_glob !smallestvol_icb, arr_block, elem_block, l_geo_out, icb_outfreq, l_allowgrounding, draft_scale, reject_elem, melted, grounded, scaling !, length_ib, width_ib, scaling
 !#else
 ! use iceberg_params, only: smallestvol_icb, arr_block, elem_block, l_geo_out, icb_outfreq, l_allowgrounding, draft_scale, melted, grounded, scaling !, length_ib, width_ib, scaling
 !#endif
@@ -274,7 +300,7 @@ subroutine iceberg_step1(ice, mesh, partit, dynamics, ib, height_ib,length_ib,wi
  
  
  integer, intent(in)	:: ib, istep
- real,    intent(inout)	:: height_ib,length_ib,width_ib
+ real,    intent(inout)	:: height_ib_single,length_ib_single,width_ib_single
  real,    intent(inout)	:: lon_deg,lat_deg
  real, 	  intent(in)	:: Co,Ca,Ci, Cdo_skin,Cda_skin
  real,	  intent(in)	:: rho_icb, conc_sill,P_sill, rho_h2o,rho_air,rho_ice
@@ -334,17 +360,20 @@ type(t_dyn)   , intent(inout), target :: dynamics
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
- 
+
+
+!write(*,*) "LA DEBUG: 1"
  mype          =>partit%mype
 
  istep_end_synced = istep + steps_per_ib_step - 1
 
- depth_ib = -height_ib * rho_icb/rho_h2o
- volume_ib= length_ib * width_ib * height_ib  
+ depth_ib = -height_ib_single * rho_icb/rho_h2o
+ volume_ib= length_ib_single * width_ib_single * height_ib_single  
  mass_ib = volume_ib * rho_icb	 !less mass 
  lon_rad = lon_deg*rad
  lat_rad = lat_deg*rad
  
+!write(*,*) "LA DEBUG: 2"
  if(volume_ib .le. smallestvol_icb) then
   melted(ib) = .true.
 
@@ -354,6 +383,7 @@ type(t_dyn)   , intent(inout), target :: dynamics
 
   return
  end if 
+!write(*,*) "LA DEBUG: 3"
  
  if (firstcall) then
   if(mype==0) write(*,*) 'Preparing local_idx_of array...'
@@ -363,6 +393,7 @@ type(t_dyn)   , intent(inout), target :: dynamics
   firstcall=.false.
   if(mype==0) write(*,*) 'Preparing local_idx_of done.' 
  end if 
+!write(*,*) "LA DEBUG: 4"
  
  if (find_iceberg_elem) then
   lon_rad = lon_deg*rad
@@ -373,25 +404,27 @@ type(t_dyn)   , intent(inout), target :: dynamics
   
   !find LOCAL element where the iceberg starts:
   coords_tmp = [lon_deg, lat_deg]
+  write(*,*) "LA DEBUG: point_in_triangle 1 - lon/lat", lon_deg, "/", lat_deg
   call point_in_triangle(mesh, partit, iceberg_elem, coords_tmp)
   !call point_in_triangle(mesh, iceberg_elem, (/lon_deg, lat_deg/))
   i_have_element= (iceberg_elem .ne. 0) !up to 3 PEs possible
+!write(*,*) "LA DEBUG: 4a"
   
   if(i_have_element) then
+!write(*,*) "LA DEBUG: 4a1"
    i_have_element= mesh%elem2D_nodes(1,iceberg_elem) <= partit%myDim_nod2D !1 PE still .true.
-#ifdef use_cavity
-   if(reject_elem(mesh, partit, iceberg_elem)) then
+!write(*,*) "LA DEBUG: 4a2"
+   if( (use_cavity) .and. (reject_elem(mesh, partit, iceberg_elem))) then
     iceberg_elem=0 !reject element
     i_have_element=.false.
    else 
     iceberg_elem=partit%myList_elem2D(iceberg_elem) !global now
    end if
-#else
-   
-   iceberg_elem=partit%myList_elem2D(iceberg_elem) !global now
-#endif 
+!write(*,*) "LA DEBUG: 4a3"
   end if
+!write(*,*) "LA DEBUG: 4b"
   call com_integer(partit, i_have_element,iceberg_elem)
+!write(*,*) "LA DEBUG: 4c"
  
   if(iceberg_elem .EQ. 0) then
         write(*,*) 'IB ',ib,' rot. coords:', lon_deg, lat_deg !,lon_rad, lat_rad
@@ -400,7 +433,9 @@ type(t_dyn)   , intent(inout), target :: dynamics
   end if
   
   ! initialize the iceberg velocity
+!write(*,*) "LA DEBUG: 4d"
   call initialize_velo(mesh, partit, dynamics, i_have_element, ib, u_ib, v_ib, lon_rad, lat_rad, depth_ib, local_idx_of(iceberg_elem))
+!write(*,*) "LA DEBUG: 4e"
 
   !iceberg elem of ib is found
   find_iceberg_elem = .false.
@@ -417,8 +452,10 @@ type(t_dyn)   , intent(inout), target :: dynamics
       
     endif
   endif
+!write(*,*) "LA DEBUG: 4f"
  end if
  
+!write(*,*) "LA DEBUG: 5"
  
  ! ================== START ICEBERG CALCULATION ====================
  
@@ -428,7 +465,10 @@ type(t_dyn)   , intent(inout), target :: dynamics
  !if the first node belongs to this processor.. (just one processor enters here!)
  !if( local_idx_of(iceberg_elem) > 0 .and. elem2D_nodes(1,local_idx_of(iceberg_elem)) <= myDim_nod2D ) then
 if( local_idx_of(iceberg_elem) > 0 ) then 
+!write(*,*) "LA DEBUG: 5a"
+
   if( elem2D_nodes(1,local_idx_of(iceberg_elem)) <= partit%myDim_nod2D ) then
+!write(*,*) "LA DEBUG: 5b"
 
   i_have_element=.true. 
 
@@ -440,14 +480,13 @@ if( local_idx_of(iceberg_elem) > 0 ) then
   
 
   call iceberg_dyn(mesh, partit, ice, dynamics, ib, new_u_ib, new_v_ib, u_ib, v_ib, lon_rad,lat_rad, depth_ib, &
-                   height_ib, length_ib, width_ib, local_idx_of(iceberg_elem), &
+                   height_ib_single, length_ib_single, width_ib_single, local_idx_of(iceberg_elem), &
   		   mass_ib, Ci, Ca, Co, Cda_skin, Cdo_skin, &
   		   rho_ice, rho_air, rho_h2o, P_sill,conc_sill, frozen_in, &
   		   file_forces_u, file_forces_v, P_ib, conci_ib, &
 		   dt*REAL(steps_per_ib_step), l_output, f_u_ib_old, &
 		   f_v_ib_old, l_semiimplicit, semiimplicit_coeff, &
 		   AB_coeff, file_meltrates, rho_icb)
-  call prepare_icb2fesom(mesh,partit,ib,i_have_element,local_idx_of(iceberg_elem),depth_ib)
 
   dudt = (new_u_ib-u_ib)/REAL(steps_per_ib_step) / dt
   dvdt = (new_v_ib-v_ib)/REAL(steps_per_ib_step) / dt
@@ -456,11 +495,13 @@ if( local_idx_of(iceberg_elem) > 0 ) then
  
   call depth_bathy(mesh,partit, Zdepth3, local_idx_of(iceberg_elem))
   !interpolate depth to location of iceberg (2 times because FEM_3eval expects a 2 component vector...)
+ write(*,*) "LA DEBUG: step 1"
   call FEM_3eval(mesh,partit, Zdepth,Zdepth,lon_rad,lat_rad,Zdepth3,Zdepth3,local_idx_of(iceberg_elem))
   !write(*,*) 'nodal depth in iceberg ', ib,'s element:', Zdepth3
   !write(*,*) 'depth at iceberg ', ib, 's location:', Zdepth
   
   !=================CHECK IF ICEBERG IS GROUNDED...===================
+ old_element = iceberg_elem !save if iceberg left model domain
  if((draft_scale(ib)*abs(depth_ib) .gt. Zdepth) .and. l_allowgrounding ) then 
  !if((draft_scale(ib)*abs(depth_ib) .gt. minval(Zdepth3)) .and. l_allowgrounding ) then 
    !icebergs remains stationary (iceberg can melt above in iceberg_dyn!)
@@ -469,25 +510,20 @@ if( local_idx_of(iceberg_elem) > 0 ) then
     v_ib = 0.0
     old_lon = lon_rad
     old_lat = lat_rad
-
-    !!###########################################
-    !! LA: prevent too many icebergs in one element
-    old_element = iceberg_elem !save if iceberg left model domain
-    !!###########################################
  
 ! kh 16.03.21 (asynchronous) iceberg calculation starts with the content in common arrays at istep and will merge its results at istep_end_synced
-    if (mod(istep_end_synced,logfile_outfreq)==0) then 
-        write(*,*) 'iceberg ib ', ib, 'is grounded'
-        grounded(ib) = .true.
-    end if
+    grounded(ib) = .true.
+    !if (mod(istep_end_synced,logfile_outfreq)==0) then 
+! if(ib==148) write(*,*) "LA DEBUG: 3 - elem ", iceberg_elem
+    write(*,*) 'iceberg ib ', ib, 'is grounded'
+    !end if
  	
  else 
   !===================...ELSE CALCULATE TRAJECTORY====================
 
- ! LA: prevent too many icebergs in one element
- old_element = iceberg_elem !save if iceberg left model domain
  
  t0=MPI_Wtime()
+!write(*,*) "LA DEBUG: before_trajectory"
   call trajectory( lon_rad,lat_rad, u_ib,v_ib, new_u_ib,new_v_ib, &
 		   lon_deg,lat_deg,old_lon,old_lat, dt*REAL(steps_per_ib_step))
   	   
@@ -514,6 +550,7 @@ if( local_idx_of(iceberg_elem) > 0 ) then
    iceberg_elem=local_idx_of(iceberg_elem)  	!local
  t7=MPI_Wtime()
    call find_new_iceberg_elem(mesh,partit, iceberg_elem, (/lon_deg, lat_deg/), left_mype)
+
  t8=MPI_Wtime()
    iceberg_elem=partit%myList_elem2D(iceberg_elem)  	!global
   end if		   
@@ -522,35 +559,53 @@ if( local_idx_of(iceberg_elem) > 0 ) then
 
   !-----------------------------
   ! LA 2022-11-30
-  do idx = 1, size(elem_block)
-      if (elem_block(idx) == iceberg_elem) then
-          area_ib_tot = area_ib_tot + length_ib * width_ib * scaling(idx)
-      end if
-  end do
+!write(*,*) "LA DEBUG: before_saturation"
+  if(lcell_saturation) then
+!write(*,*) "LA DEBUG: start_saturation"
+    area_ib_tot = length_ib_single*width_ib_single*scaling(ib)
+!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(idx, area_ib_tot)
+!$OMP DO
+    do idx = 1, size(elem_block)
+        if (elem_block(idx) == iceberg_elem) then
+            area_ib_tot = area_ib_tot + length_ib(idx) * width_ib(idx) * scaling(idx)
+        end if
+    end do
+!write(*,*) "LA DEBUG: end_loop_saturation"
+!$OMP END DO
+!$OMP END PARALLEL
   !-----------------------------
 
-  if((area_ib_tot > elem_area(local_idx_of(iceberg_elem))) .and. &  
-                (iceberg_elem .ne. old_element) .and. &
-                (old_element .ne. 0) .and. &
-                (.not.grounded(ib))) then
-      lon_rad = old_lon
-      lat_rad = old_lat 
-      lon_deg = lon_rad/rad
-      lat_deg = lat_rad/rad
-      iceberg_elem = old_element
-      u_ib    = 0.
-      v_ib    = 0.  
+    if ((area_ib_tot > elem_area_glob(iceberg_elem)) .and. (old_element.ne.0) .and. (left_mype == 0)) then 
+        write(*,*) " *******************************************************"
+        write(*,*) " * set iceberg ", ib, " back to elem ", old_element, " from elem ", iceberg_elem
+        write(*,*) " * area_ib_tot = ", area_ib_tot, "; elem_area = ", elem_area(local_idx_of(iceberg_elem))
+        i_have_element = .true.
+        left_mype = 0.0
+        lon_rad = old_lon
+        lat_rad = old_lat 
+        lon_deg = lon_rad/rad
+        lat_deg = lat_rad/rad
+        iceberg_elem = old_element
+        u_ib    = 0.
+        v_ib    = 0.  
+    end if
+!write(*,*) "LA DEBUG: after_cell_saturation"
   end if
   !###########################################
  
   !values for communication
-  arr= (/ height_ib,length_ib,width_ib, u_ib,v_ib, lon_rad,lat_rad, &
+  arr= (/ height_ib_single,length_ib_single,width_ib_single, u_ib,v_ib, lon_rad,lat_rad, &
           left_mype, old_lon,old_lat, frozen_in, dudt, dvdt, P_ib, conci_ib/) 
 
   !save in larger array	  
   arr_block((ib-1)*15+1 : ib*15)=arr
   elem_block(ib)=iceberg_elem
-  	  
+  pe_block(ib)=mype
+
+  volume_ib=height_ib_single*length_ib_single*width_ib_single
+!write(*,*) "LA DEBUG: before_prepare"
+  call prepare_icb2fesom(mesh,partit,ib,i_have_element,local_idx_of(iceberg_elem),depth_ib,height_ib_single)
+!write(*,*) "LA DEBUG: after_prepare"
  end if !processor has element?
 end if !... and first node belongs to processor?
 
@@ -567,54 +622,25 @@ end if !... and first node belongs to processor?
  
  end subroutine iceberg_step1
 
-subroutine get_total_iceberg_area(mesh, partit,iceberg_elem, area_ib_tot)
- 
- use o_param 		!for rad								!=
- USE MOD_MESH
- use MOD_PARTIT		!for myDim_elem2D, myList_nod2D						!=
- use g_rotate_grid	!for subroutine g2r, logfile_outfreq					!=
- !use iceberg_params, only: arr_block, elem_block, length_ib, width_ib, scaling
- 
- implicit none											!=
- 
- integer, intent(inout) :: iceberg_elem !global
- real, intent(inout)    :: area_ib_tot
- integer                :: idx
 
-type(t_mesh), intent(in) , target :: mesh
-type(t_partit), intent(inout), target :: partit
-!========================= MODULES & DECLARATIONS =====================================!=
-#include "associate_part_def.h"
-#include "associate_mesh_def.h"
-#include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
-
-  area_ib_tot = 0.0
-  do idx = 1, size(elem_block)
-      if (elem_block(idx) == iceberg_elem) then
-          area_ib_tot = area_ib_tot + length_ib(idx) * width_ib(idx) * scaling(idx)
-      end if
-  end do
-  !###########################################
-end subroutine get_total_iceberg_area
  
- 
-subroutine iceberg_step2(mesh, partit,arr, elem_from_block, ib, height_ib,length_ib,width_ib, lon_deg,lat_deg, &
+subroutine iceberg_step2(mesh, partit,arr, elem_from_block, ib, height_ib_single, length_ib_single, width_ib_single, lon_deg,lat_deg, &
 			Co,Ca,Ci, Cdo_skin,Cda_skin, rho_icb, 		   &
 			conc_sill,P_sill, rho_h2o,rho_air,rho_ice,	   & 
 			u_ib,v_ib, iceberg_elem, find_iceberg_elem, 	   &
 			lastsubstep, f_u_ib_old,	   &
 			f_v_ib_old, l_semiimplicit, semiimplicit_coeff,    &
-			AB_coeff, istep)
+			AB_coeff, istep, elem_block_red, pe_block_red)
 			
  !============================= MODULES & DECLARATIONS =========================================!=
  												!=
  use o_param 		!for rad								!=
  use g_rotate_grid	!for subroutine g2r, logfile_outfreq					!=
  use g_config, only: steps_per_ib_step
+ use g_comm_auto
 !=
-!#ifdef use_cavity
-! use iceberg_params, only: smallestvol_icb, buoy_props, bvl_mean, lvlv_mean, lvle_mean, lvlb_mean, ascii_out, l_geo_out, icb_outfreq, l_allowgrounding, draft_scale, reject_elem, elem_block
+use g_comm
+use iceberg_params, only: length_ib, width_ib, scaling !smallestvol_icb, arr_block, elem_block, l_geo_out, icb_outfreq, l_allowgrounding, draft_scale, reject_elem, melted, grounded, scaling !, length_ib, width_ib, scaling
 !#else
 ! use iceberg_params, only: smallestvol_icb, buoy_props, bvl_mean, lvlv_mean, lvle_mean, lvlb_mean, ascii_out, l_geo_out, icb_outfreq, l_allowgrounding, draft_scale, elem_block
 !#endif
@@ -624,7 +650,7 @@ subroutine iceberg_step2(mesh, partit,arr, elem_from_block, ib, height_ib,length
  real, 	  intent(in)	:: arr(15)
  integer, intent(in)	:: elem_from_block
  integer, intent(in)	:: ib
- real,    intent(inout)	:: height_ib,length_ib,width_ib
+ real,    intent(inout)	:: height_ib_single, length_ib_single, width_ib_single
  real,    intent(inout)	:: lon_deg,lat_deg
  real, 	  intent(in)	:: Co,Ca,Ci, Cdo_skin,Cda_skin
  real,	  intent(in)	:: rho_icb, conc_sill,P_sill, rho_h2o,rho_air,rho_ice
@@ -636,7 +662,8 @@ subroutine iceberg_step2(mesh, partit,arr, elem_from_block, ib, height_ib,length
  logical, intent(in)	:: l_semiimplicit
  real,    intent(in)	:: semiimplicit_coeff
  real,    intent(in)	:: AB_coeff						
- 
+ integer, intent(in), dimension(ib_num):: elem_block_red				!=
+ integer, intent(in), dimension(ib_num):: pe_block_red				!=
  
  integer, dimension(:), save, allocatable :: local_idx_of
  real      			:: depth_ib, volume_ib, mass_ib
@@ -653,9 +680,11 @@ subroutine iceberg_step2(mesh, partit,arr, elem_from_block, ib, height_ib,length
  integer :: istep_end_synced
  
 ! LA: add threshold for number of icebergs in one elemt
+ integer status(MPI_STATUS_SIZE)
  integer                        :: num_ib_in_elem, idx
  real                           :: area_ib_tot
- real                           :: local_elem_area
+ !real(real64), dimension(:), allocatable    :: rbuffer, local_elem_area
+ real(real64)                   :: elem_area_tmp 
 
  !iceberg output 
  character 			:: ib_char*10
@@ -701,9 +730,9 @@ type(t_partit), intent(inout), target :: partit
 
  iceberg_elem= elem_from_block !update element as before in com_values
  old_element = elem_from_block !save if iceberg left model domain
- height_ib= arr(1)
- length_ib= arr(2)
- width_ib = arr(3)
+ height_ib_single= arr(1)
+ length_ib_single= arr(2)
+ width_ib_single = arr(3)
  u_ib     = arr(4)
  v_ib     = arr(5)
  lon_rad  = arr(6)
@@ -718,9 +747,9 @@ type(t_partit), intent(inout), target :: partit
  dvdt	  = arr(13)
  P_ib	  = arr(14)
  conci_ib = arr(15) 
- 
+
  !**** check if iceberg melted in step 1 ****! 
- volume_ib = height_ib * length_ib * width_ib ! * rho_icb
+ volume_ib = height_ib_single * length_ib_single * width_ib_single ! * rho_icb
  if(volume_ib .le. smallestvol_icb) then
   buoy_props(ib, :) = 0. ! for output: NaN or MissVal could be written here
   return
@@ -738,10 +767,11 @@ type(t_partit), intent(inout), target :: partit
       call global2local(mesh, partit, local_idx_of, elem2D)
       firstcall=.false.
     end if 
- 
+
  if(left_mype > 0.) then
    call iceberg_elem4all(mesh, partit, iceberg_elem, lon_deg, lat_deg) !Just PE changed?
    if(iceberg_elem == 0 ) then
+           left_mype = 0.0
            lon_rad = old_lon
            lat_rad = old_lat 
            lon_deg = lon_rad/rad
@@ -749,18 +779,38 @@ type(t_partit), intent(inout), target :: partit
            iceberg_elem = old_element
            u_ib    = 0.
            v_ib    = 0.
-   else
+   else if(lcell_saturation) then
      if (mype==0) write(*,*) 'iceberg ',ib, ' changed PE or was very fast'
-     call get_total_iceberg_area(mesh, partit, iceberg_elem, area_ib_tot)
-     if(area_ib_tot > elem_area(local_idx_of(iceberg_elem))) then
+     elem_area_tmp = elem_area_glob(iceberg_elem)
+     area_ib_tot = length_ib_single * width_ib_single * scaling(ib)
+!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(idx, area_ib_tot)
+!$OMP DO
+     do idx = 1, size(elem_block_red)
+         if (elem_block_red(idx) == iceberg_elem) then
+     !        write(*,*) " * Found element ",elem_block_red(idx), " for ib ",idx, ", elem_area=",elem_area_tmp
+             area_ib_tot = area_ib_tot + length_ib(idx) * width_ib(idx) * scaling(idx)
+         end if
+     end do
+!$OMP END DO
+!$OMP END PARALLEL
+     if((area_ib_tot > elem_area_tmp) .and. (elem_area_tmp > 0.0)) then
+         if(mype==pe_block_red(ib)) then
+            write(*,*) " *******************************************************"
+            write(*,*) " * iceberg changed PE and saturation"
+            write(*,*) " * set iceberg ", ib, " back to elem ", old_element, " from elem ", iceberg_elem
+            write(*,*) " * area_ib = ", length_ib_single * width_ib_single, "; area_ib_tot = ", area_ib_tot, "; elem_area = ", elem_area_tmp
+         end if
+         left_mype = 0.0
          lon_rad = old_lon
-         lat_rad = old_lat 
+         lat_rad = old_lat
          lon_deg = lon_rad/rad
          lat_deg = lat_rad/rad
          iceberg_elem = old_element
          u_ib    = 0.
-         v_ib    = 0.  
+         v_ib    = 0.
      end if
+   else if(lcell_saturation) then
+     if (mype==0) write(*,*) 'iceberg ',ib, ' changed PE or was very fast'
    end if
  end if
  
@@ -822,9 +872,9 @@ type(t_partit), intent(inout), target :: partit
   buoy_props(ib, 7) = dvdt_out
   buoy_props(ib, 8) = u_ib_out
   buoy_props(ib, 9) = v_ib_out
-  buoy_props(ib,10) = height_ib
-  buoy_props(ib,11) = length_ib 
-  buoy_props(ib,12) = width_ib
+  buoy_props(ib,10) = height_ib_single
+  buoy_props(ib,11) = length_ib_single 
+  buoy_props(ib,12) = width_ib_single
   buoy_props(ib,13) = iceberg_elem
 
 ! end if
@@ -832,14 +882,6 @@ type(t_partit), intent(inout), target :: partit
  end if 
  
  t4=MPI_Wtime()
- 
-! if (mod(istep,logfile_outfreq)==0 .and. mype==0 .and. lastsubstep) then 
-!  write(*,*) '*** step2 ***'
-!  write(*,*) 'comvalues took', t2-t1
-!  write(*,*) 'left mype took', t3-t2
-!  write(*,*) 'track out took', t4-t3
-! end if
-
 end subroutine iceberg_step2
 
 
@@ -887,6 +929,7 @@ type(t_partit), intent(inout), target :: partit
 	else
    		!OCEAN VELOCITY uo_ib, voib is start velocity
    		call iceberg_avvelo(mesh, partit, dynamics, startu,startv,depth_ib,localelem)
+ write(*,*) "LA DEBUG: step 2"
         call FEM_3eval(mesh, partit,u_ib,v_ib,lon_rad,lat_rad,startu,startv,localelem)
 	end if
  end if
@@ -983,15 +1026,14 @@ end subroutine depth_bathy
 !****************************************************************************************************************************
 
 subroutine parallel2coast(mesh, partit,u, v, lon,lat, elem)
-!#ifdef use_cavity
-! use iceberg_params, only: coastal_nodes
-!#endif
+ use iceberg_params, only: coastal_nodes
  implicit none
  
  real, intent(inout) 	:: u, v 	!velocity
  real, intent(in)	:: lon, lat 	!radiant
  integer, intent(in)	:: elem
  
+ integer :: fld_tmp
  integer, dimension(3) :: n
  integer :: node, m, i
  real, dimension(2) :: velocity, velocity1, velocity2
@@ -1004,12 +1046,13 @@ type(t_partit), intent(inout), target :: partit
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
 
-#ifdef use_cavity
-  SELECT CASE ( coastal_nodes(mesh, elem) ) !num of "coastal" points
-#else
-  SELECT CASE ( sum( mesh%bc_index_nod2D(mesh%elem2D_nodes(:,elem)) ) ) !num of coastal points
-  !SELECT CASE ( sum( bc_index_nod2D(elem2D_nodes(:,elem)) ) ) !num of coastal points
-#endif
+  if( use_cavity ) then
+    fld_tmp = coastal_nodes(mesh, partit, elem)
+  else
+    fld_tmp = sum( mesh%bc_index_nod2D(mesh%elem2D_nodes(:,elem)) )
+  end if
+  
+  SELECT CASE ( fld_tmp ) !num of coastal points
    CASE (0) !...coastal points: do nothing
     return
     
@@ -1020,14 +1063,18 @@ type(t_partit), intent(inout), target :: partit
     do m = 1, 3
       node = mesh%elem2D_nodes(m,elem)
       !write(*,*) 'index ', m, ':', index_nod2D(node)
-#ifdef use_cavity
-      if( mesh%bc_index_nod2D(node)==1 .OR. cavity_flag_nod2d(node)==1 ) then
-#else
-      if( mesh%bc_index_nod2D(node)==1 ) then
-#endif
-       n(i) = node
-       exit
-      end if 
+      if( use_cavity ) then
+        !if( mesh%bc_index_nod2D(node)==1 .OR. cavity_flag_nod2d(node)==1 ) then
+        if( mesh%bc_index_nod2D(node)==0.0 .OR.  (mesh%cavity_depth(node)/=0.0) ) then
+         n(i) = node
+         exit
+        end if 
+      else
+        if( mesh%bc_index_nod2D(node)==1 ) then
+         n(i) = node
+         exit
+        end if 
+      end if
     end do 
     
    !write(*,*) 'one coastal node ', n(1)
@@ -1075,13 +1122,17 @@ type(t_partit), intent(inout), target :: partit
     velocity = [ u, v ]
     do m = 1, 3
       node = mesh%elem2D_nodes(m,elem) 
-#ifdef use_cavity
-      if( (mesh%bc_index_nod2D(node)==1) .OR. (cavity_flag_nod2d(node)==1)) then
-#else
-      if( mesh%bc_index_nod2D(node)==1 ) then
-#endif
-       n(i) = node
-       i = i+1
+      if( use_cavity ) then
+        !if( (mesh%bc_index_nod2D(node)==1) .OR. (cavity_flag_nod2d(node)==1)) then
+        if( mesh%bc_index_nod2D(node)==0.0 .OR.  (mesh%cavity_depth(node)/=0.0) ) then
+         n(i) = node
+         i = i+1
+        end if
+      else
+        if( mesh%bc_index_nod2D(node)==1 ) then
+         n(i) = node
+         i = i+1
+        end if
       end if
     end do   
     call projection(mesh,partit,  velocity, n(1), n(2))
diff --git a/src/icb_thermo.F90 b/src/icb_thermo.F90
index 624f2a75..090cf075 100644
--- a/src/icb_thermo.F90
+++ b/src/icb_thermo.F90
@@ -1,4 +1,4 @@
-!==============================================================================
+!!=============================================================================
 ! calculates the empirical melt rates of the iceberg as in 
 ! Martin: 'Parameterizing the fresh-water flux from land ice to ocean
 !          with interactive icebergs in a coupled climate model'(2010)
@@ -16,51 +16,74 @@
 !				use 3D information for T,S and velocities
 !				instead of SSTs; M_v depends on 'thermal driving')
 !==============================================================================
-subroutine iceberg_meltrates(   M_b, M_v, M_e, M_bv, &
+subroutine iceberg_meltrates(partit, M_b, M_v, M_e, M_bv, &
 				u_ib,v_ib, uo_ib,vo_ib, ua_ib,va_ib, &
 				sst_ib, length_ib, conci_ib, &
-				uo_keel_ib, vo_keel_ib, T_keel_ib, S_keel_ib, depth_ib, &
-				T_ave_ib, S_ave_ib, ib)
+				uo_keel_ib, vo_keel_ib, T_keel_ib, S_keel_ib, depth_ib, height_ib, &
+				T_ave_ib, S_ave_ib, ib, rho_icb)
   
   use o_param
+  use MOD_PARTIT
   use g_clock
   use g_forcing_arrays
   use g_rotate_grid
 
-  use iceberg_params, only: fwe_flux_ib, fwl_flux_ib, fwb_flux_ib, fwbv_flux_ib, heat_flux_ib
-  
+  use iceberg_params, only: fwe_flux_ib, fwl_flux_ib, fwb_flux_ib, fwbv_flux_ib, hfb_flux_ib, hfbv_flux_ib, hfe_flux_ib, hfl_flux_ib, scaling
   implicit none
   
+  ! LA: include latent heat 2023-04-04
+  real(kind=8),parameter ::  L                  = 334000.                   ! [J/Kg]
+  
   real, intent(IN)	:: u_ib,v_ib, uo_ib,vo_ib, ua_ib,va_ib	!iceberg velo, (int.) ocean & atm velo
   real, intent(IN)	:: uo_keel_ib, vo_keel_ib		!ocean velo at iceberg's draft
-  real, intent(IN)	:: sst_ib, length_ib, conci_ib 		!SST, length and sea ice conc.
-  real, intent(IN)	:: T_keel_ib, S_keel_ib, depth_ib	!T & S at depth 'depth_ib'
+  real, intent(IN)	:: sst_ib, length_ib, conci_ib, rho_icb 		!SST, length and sea ice conc.
+  real, intent(IN)	:: T_keel_ib, S_keel_ib, depth_ib, height_ib !T & S at depth 'depth_ib'
   real, intent(IN)	:: T_ave_ib, S_ave_ib			!T & S averaged, i.e. at 'depth_ib/2'
   integer, intent(IN)	:: ib					!iceberg ID
   real, intent(OUT)	:: M_b, M_v, M_e, M_bv			!melt rates [m (ice) per s]	
+  real 	            :: H_b, H_v, H_e, H_bv			!melt rates [m (ice) per s]	
   	
   
   real			:: absamino, damping, sea_state, v_ibmino
   real			:: tf, T_d 				!freezing temp. and 'thermal driving'
+type(t_partit), intent(inout), target :: partit
+#include "associate_part_def.h"
+#include "associate_part_ass.h"
+  
+  !bottom melt (basal turbulent melting rate)
+  !M_b = 0.58 * sqrt( (u_ib - uo_ib)**2 + (v_ib - vo_ib)**2 )**0.8 &
+  !     	* (sst_ib + 4.0) / length_ib**0.2
+  !M_b = M_b/86400. !conversion of m/day to m/s	
 
   !3-eq. formulation for bottom melting [m/s]    
   v_ibmino  = sqrt( (u_ib - uo_keel_ib)**2 + (v_ib - vo_keel_ib)**2 )
-  call iceberg_heat_water_fluxes_3eq(ib, M_b, T_keel_ib,S_keel_ib,v_ibmino, depth_ib, tf)
+  call iceberg_heat_water_fluxes_3eq(partit, ib, M_b, H_b, T_keel_ib,S_keel_ib,v_ibmino, depth_ib, tf)
+  hfb_flux_ib(ib) = H_b * length_ib*length_ib*scaling(ib)
+!  write(*,*) "LA DEBUG: 1"
 
   !3-eq. formulation for lateral 'basal' melting [m/s]
   v_ibmino  = sqrt( (u_ib - uo_ib)**2 + (v_ib - vo_ib)**2 ) ! depth-average rel. velocity
-  call iceberg_heat_water_fluxes_3eq(ib, M_bv, T_ave_ib,S_ave_ib,v_ibmino, depth_ib/2.0, tf)
+  call iceberg_heat_water_fluxes_3eq(partit, ib, M_bv, H_bv, T_ave_ib,S_ave_ib,v_ibmino, depth_ib/2.0, tf)
+  hfbv_flux_ib(ib) = H_bv * (2*length_ib*abs(depth_ib)  + 2*length_ib*abs(depth_ib) ) * scaling(ib)
+!  write(*,*) "LA DEBUG: 2"
   
   !'thermal driving', defined as the elevation of ambient water 
   !temperature above freezing point' (Neshyba and Josberger, 1979).
   T_d = T_ave_ib - tf
   if(T_d < 0.) T_d = 0.
-  !write(*,*) 'thermal driving:',T_d,'; Tf:',tf,'T_ave:',T_ave_ib
 
   !lateral melt (buoyant convection)
+  !M_v = 0.00762 * sst_ib + 0.00129 * sst_ib**2
+  !M_v = M_v/86400.
   !M_v is a function of the 'thermal driving', NOT just sst! Cf. Neshyba and Josberger (1979)
   M_v = 0.00762 * T_d + 0.00129 * T_d**2
   M_v = M_v/86400.
+  H_v = M_v * rho_icb * L
+!  write(*,*) "!LA DEBUG: H_v=",H_v
+  hfl_flux_ib(ib) = H_v * (2*length_ib*abs(depth_ib)  + 2*length_ib*abs(depth_ib) ) * scaling(ib)
+!  write(*,*) "LA DEBUG: hfl_flux_ib=",hfl_flux_ib
+!  write(*,*) "LA DEBUG: 3"
+  !fwl_flux_ib = M_v
 
   !wave erosion
   absamino = sqrt( (ua_ib - uo_ib)**2 + (va_ib - vo_ib)**2 )
@@ -68,6 +91,12 @@ subroutine iceberg_meltrates(   M_b, M_v, M_e, M_bv, &
   damping = 0.5 * (1.0 + cos(conci_ib**3 * Pi))
   M_e = 1./6. * sea_state * (sst_ib + 2.0) * damping
   M_e = M_e/86400.
+  H_e = M_e * rho_icb * L
+!  write(*,*) "LA DEBUG: H_e=",H_e
+!  write(*,*) "LA DEBUG: height=",height_ib
+  hfe_flux_ib(ib) = H_e * (length_ib*abs(height_ib)  + length_ib*abs(height_ib) ) * scaling(ib)
+!  write(*,*) "LA DEBUG: hfe_flux_ib=",hfe_flux_ib
+!  write(*,*) "LA DEBUG: 4"
   !fwe_flux_ib = M_e  
 end subroutine iceberg_meltrates
 
@@ -90,7 +119,7 @@ subroutine iceberg_newdimensions(partit, ib, depth_ib,height_ib,length_ib,width_
   use g_clock
   use g_forcing_arrays
   use g_rotate_grid
-  use iceberg_params, only: l_weeksmellor, ascii_out, icb_outfreq, vl_block, bvl_mean, lvlv_mean, lvle_mean, lvlb_mean, smallestvol_icb, fwb_flux_ib, fwe_flux_ib, fwbv_flux_ib, fwl_flux_ib, scaling, heat_flux_ib, lheat_flux_ib
+  use iceberg_params, only: l_weeksmellor, ascii_out, icb_outfreq, vl_block, bvl_mean, lvlv_mean, lvle_mean, lvlb_mean, smallestvol_icb, fwb_flux_ib, fwe_flux_ib, fwbv_flux_ib, fwl_flux_ib, scaling, hfb_flux_ib, hfbv_flux_ib, hfe_flux_ib, hfl_flux_ib, lhfb_flux_ib
   use g_config, only: steps_per_ib_step
 
   implicit none  
@@ -98,6 +127,7 @@ subroutine iceberg_newdimensions(partit, ib, depth_ib,height_ib,length_ib,width_
   integer, intent(IN)	:: ib
   real, intent(INOUT)	:: depth_ib, height_ib, length_ib, width_ib
   real, intent(IN)	:: M_b, M_v, M_e, M_bv, rho_h2o, rho_icb
+  real              :: hf_tot_tmp
   character, intent(IN)	:: file_meltrates*80
   
   real			:: dh_b, dh_v, dh_e, dh_bv, bvl, lvl_b, lvl_v, lvl_e, tvl, volume_before, volume_after
@@ -140,7 +170,7 @@ type(t_partit), intent(inout), target :: partit
 
     if((tvl .ge. volume_before) .OR. (volume_before .le. smallestvol_icb)) then
     	volume_after=0.0    	
-	depth_ib = 0.0
+	    depth_ib = 0.0
     	height_ib= 0.0
     	length_ib= 0.0
     	width_ib = 0.0
@@ -170,7 +200,7 @@ type(t_partit), intent(inout), target :: partit
         !iceberg smaller than critical value after melting?
         if (volume_after .le. smallestvol_icb) then
             volume_after=0.0    	
-	    depth_ib = 0.0
+	        depth_ib = 0.0
     	    height_ib= 0.0
     	    length_ib= 0.0
     	    width_ib = 0.0
@@ -226,9 +256,18 @@ type(t_partit), intent(inout), target :: partit
     ! -----------------------
     ! LA: set iceberg heatflux at least to latent heat 2023-04-04
     ! Latent heat flux at base and sides also changes lines 475/476
-    lheat_flux_ib(ib) = rho_icb*L*tvl*scaling(ib)/dt/REAL(steps_per_ib_step)
-    if( (heat_flux_ib(ib).gt.0.0) .and. (heat_flux_ib(ib).lt.lheat_flux_ib(ib))) then
-        heat_flux_ib(ib)=lheat_flux_ib(ib)
+    ! Lateral heat flux set to latent heat and basal heat flux set to zero
+    if( lmin_latent_hf ) then
+        lhfb_flux_ib(ib) = rho_icb*L*tvl*scaling(ib)/dt/REAL(steps_per_ib_step)
+
+        hf_tot_tmp = hfb_flux_ib(ib)+hfbv_flux_ib(ib)+hfl_flux_ib(ib)+hfe_flux_ib(ib)
+
+        if( (hf_tot_tmp >= 0.0) .and. (abs(hf_tot_tmp) < abs(lhfb_flux_ib(ib)))) then
+            hfe_flux_ib(ib)=-lhfb_flux_ib(ib) * abs(hfe_flux_ib(ib)/hf_tot_tmp)
+            hfl_flux_ib(ib)=-lhfb_flux_ib(ib) * abs(hfl_flux_ib(ib)/hf_tot_tmp)
+            hfb_flux_ib(ib)=-lhfb_flux_ib(ib) * abs(hfb_flux_ib(ib)/hf_tot_tmp)
+            hfbv_flux_ib(ib)=-lhfb_flux_ib(ib) * abs(hfbv_flux_ib(ib)/hf_tot_tmp)
+        end if
     end if
     ! -----------------------
 end subroutine iceberg_newdimensions
@@ -263,20 +302,20 @@ end subroutine weeksmellor
  !***************************************************************************************************************************
  !***************************************************************************************************************************
 
-subroutine iceberg_heat_water_fluxes_3eq(ib, M_b, T_ib,S_ib,v_rel, depth_ib, t_freeze)
+subroutine iceberg_heat_water_fluxes_3eq(partit, ib, M_b, H_b, T_ib,S_ib,v_rel, depth_ib, t_freeze)
   ! The three-equation model of ice-shelf ocean interaction (Hellmer et al., 1997)
   ! Code derived from BRIOS subroutine iceshelf (which goes back to H.Hellmer's 2D ice shelf model code)
   ! adjusted for use in FESOM by Ralph Timmermann, 16.02.2011
   ! adopted and modified for iceberg basal melting by Thomas Rackow, 11.06.2014
   !----------------------------------------------------------------
-  
+
   use iceberg_params
   use g_config
 
   implicit none
 
   integer, INTENT(IN)	  :: ib
-  real(kind=8),INTENT(OUT) :: M_b, t_freeze
+  real(kind=8),INTENT(OUT) :: M_b, H_b, t_freeze
   real(kind=8),INTENT(IN) :: T_ib, S_ib 	! ocean temperature & salinity (at depth 'depth_ib')
   real(kind=8),INTENT(IN) :: v_rel, depth_ib 	! relative velocity iceberg-ocean (at depth 'depth_ib')
 
@@ -312,6 +351,10 @@ subroutine iceberg_heat_water_fluxes_3eq(ib, M_b, T_ib,S_ib,v_rel, depth_ib, t_f
   real(kind=8),parameter ::  cpi =  152.5+7.122*(atk+tob)     !Paterson:"The Physics of Glaciers"
 
   real(kind=8),parameter ::  L    = 334000.                   ! [J/Kg]
+type(t_partit), intent(inout), target :: partit
+!==================== MODULES & DECLARATIONS ==========================!= 
+#include "associate_part_def.h"
+#include "associate_part_ass.h"
 
      temp = T_ib
      sal = S_ib
@@ -324,7 +367,7 @@ subroutine iceberg_heat_water_fluxes_3eq(ib, M_b, T_ib,S_ib,v_rel, depth_ib, t_f
      ! Calculate or prescribe the turbulent heat and salt transfer coeff. GAT and GAS
      ! velocity-dependent approach of Jenkins (1991)
 
-     vt1  = v_rel ! relative velocity iceberg-ocean (at depth 'depth_ib')
+     vt1 = v_rel ! relative velocity iceberg-ocean (at depth 'depth_ib')
      vt1  = max(vt1,0.005)       ! RG44030
 
      re   = 10./un                   !vt1*re (=velocity times length scale over kinematic viscosity) is the Reynolds number
@@ -404,17 +447,18 @@ subroutine iceberg_heat_water_fluxes_3eq(ib, M_b, T_ib,S_ib,v_rel, depth_ib, t_f
      endif
 
      t_freeze = tf ! output of freezing temperature
-
      ! Calculate the melting/freezing rate [m/s]
      ! seta = ep5*(1.0-sal/sf)     !rt thinks this is not needed; TR: Why different to M_b? LIQUID vs. ICE
 
      !rt  t_surf_flux(i,j)=gat*(tf-tin)
      !rt  s_surf_flux(i,j)=gas*(sf-(s(i,j,N,lrhs)+35.0))
 
-     !heat_flux_ib(ib)  = rhow*cpw*gat*(tin-tf)*scaling(ib)      ! [W/m2]  ! positive for upward
-     heat_flux_ib(ib)  = rhow*cpw*gat*(tin-tf)*length_ib(ib)*width_ib(ib)*scaling(ib)      ! [W]  ! positive for upward
+     !hfb_flux_ib(ib)  = rhow*cpw*gat*(tin-tf)*scaling(ib)      ! [W/m2]  ! positive for upward
+     !hfb_flux_ib(ib)  = rhow*cpw*gat*(tin-tf)*length_ib(ib)*width_ib(ib)*scaling(ib)      ! [W]  ! positive for upward
+     H_b  = rhow*cpw*gat*(tin-tf) !*length_ib(ib)*width_ib(ib)*scaling(ib)      ! [W]  ! positive for upward
+     
      !fw_flux_ib(ib) =          gas*(sf-sal)/sf   ! [m/s]   !
-      M_b 	    =          gas*(sf-sal)/sf   ! [m/s]   ! m freshwater per second
+     M_b 	    =          gas*(sf-sal)/sf   ! [m/s]   ! m freshwater per second
      !fw_flux_ib(ib) = M_b
      !fw = -M_b
      M_b = - (rhow / rhoi) * M_b 		 ! [m (ice) per second], positive for melting? NOW positive for melting
@@ -468,112 +512,6 @@ subroutine potit_ib(ib,salz,pt,pres,rfpres,tin)
   return
 end subroutine potit_ib
 
-! if the underlying FESOM is run without cavities, the following routines might be 
-! missing, so put them here:
-#ifndef use_cavity
-!
-!-------------------------------------------------------------------------------------
-!
-!subroutine potit(salz,pt,pres,rfpres,tin)
-!  ! Berechnet aus dem Salzgehalt[psu] (SALZ), der pot. Temperatur[oC]
-!  ! (PT) und dem Referenzdruck[dbar] (REFPRES) die in-situ Temperatur
-!  ! [oC] (TIN) bezogen auf den in-situ Druck[dbar] (PRES) mit Hilfe
-!  ! eines Iterationsverfahrens aus.
-!
-!  integer iter
-!  real salz,pt,pres,rfpres,tin
-!  real epsi,tpmd,pt1,ptd,pttmpr
-!
-!  data tpmd / 0.001 /
-!
-!  epsi = 0.
-!  do iter=1,100
-!     tin  = pt+epsi
-!     pt1  = pttmpr(salz,tin,pres,rfpres)
-!     ptd  = pt1-pt
-!     if(abs(ptd).lt.tpmd) return
-!     epsi = epsi-ptd
-!  enddo
-!  write(6,*) ' WARNING!'
-!  write(6,*) ' in-situ temperature calculation has not converged.'
-!  stop
-!  return
-!end subroutine potit
-!
-!-------------------------------------------------------------------------------------
-!
-!real function pttmpr(salz,temp,pres,rfpres)
-!  ! Berechnet aus dem Salzgehalt/psu (SALZ), der in-situ Temperatur/degC
-!  ! (TEMP) und dem in-situ Druck/dbar (PRES) die potentielle Temperatur/
-!  ! degC (PTTMPR) bezogen auf den Referenzdruck/dbar (RFPRES). Es wird
-!  ! ein Runge-Kutta Verfahren vierter Ordnung verwendet.
-!  ! Checkwert: PTTMPR = 36.89073 DegC
-!  !       fuer SALZ   =    40.0 psu
-!  !            TEMP   =    40.0 DegC
-!  !            PRES   = 10000.000 dbar
-!  !            RFPRES =     0.000 dbar
-!
-!  data ct2 ,ct3  /0.29289322 ,  1.707106781/
-!  data cq2a,cq2b /0.58578644 ,  0.121320344/
-!  data cq3a,cq3b /3.414213562, -4.121320344/
-!
-!  real salz,temp,pres,rfpres
-!  real p,t,dp,dt,q,ct2,ct3,cq2a,cq2b,cq3a,cq3b
-!  real adlprt
-!
-!  p  = pres
-!  t  = temp
-!  dp = rfpres-pres
-!  dt = dp*adlprt(salz,t,p)
-!  t  = t +0.5*dt
-!  q = dt
-!  p  = p +0.5*dp
-!  dt = dp*adlprt(salz,t,p)
-!  t  = t + ct2*(dt-q)
-!  q  = cq2a*dt + cq2b*q
-!  dt = dp*adlprt(salz,t,p)
-!  t  = t + ct3*(dt-q)
-!  q  = cq3a*dt + cq3b*q
-!  p  = rfpres
-!  dt = dp*adlprt(salz,t,p)
-!
-!  pttmpr = t + (dt-q-q)/6.0
-!
-!end function pttmpr
-!
-!-------------------------------------------------------------------------------------
-!
-!real function adlprt(salz,temp,pres)
-!  ! Berechnet aus dem Salzgehalt/psu (SALZ), der in-situ Temperatur/degC
-!  ! (TEMP) und dem in-situ Druck/dbar (PRES) den adiabatischen Temperatur-
-!  ! gradienten/(K Dbar^-1) ADLPRT.
-!  ! Checkwert: ADLPRT =     3.255976E-4 K dbar^-1
-!  !       fuer SALZ   =    40.0 psu
-!  !            TEMP   =    40.0 DegC
-!  !            PRES   = 10000.000 dbar
-!
-!  real salz,temp,pres
-!  real s0,a0,a1,a2,a3,b0,b1,c0,c1,c2,c3,d0,d1,e0,e1,e2,ds
-!
-!  data s0 /35.0/
-!  data a0,a1,a2,a3 /3.5803E-5, 8.5258E-6, -6.8360E-8, 6.6228E-10/
-!  data b0,b1       /1.8932E-6, -4.2393E-8/
-!  data c0,c1,c2,c3 /1.8741E-8, -6.7795E-10, 8.7330E-12, -5.4481E-14/
-!  data d0,d1       /-1.1351E-10, 2.7759E-12/
-!  data e0,e1,e2    /-4.6206E-13,  1.8676E-14, -2.1687E-16/
-!
-!  ds = salz-s0
-!  adlprt = ( ( (e2*temp + e1)*temp + e0 )*pres                     &
-!       + ( (d1*temp + d0)*ds                                  &
-!       + ( (c3*temp + c2)*temp + c1 )*temp + c0 ) )*pres   &
-!       + (b1*temp + b0)*ds +  ( (a3*temp + a2)*temp + a1 )*temp + a0
-!
-!END function adlprt
-!
-!----------------------------------------------------------------------------------------
-!
-#endif
-
 
 ! LA from oce_dens_press for iceberg coupling
 subroutine fcn_density(t,s,z,rho)
diff --git a/src/ice_EVP.F90 b/src/ice_EVP.F90
index b646bc6d..7a1f0ccd 100755
--- a/src/ice_EVP.F90
+++ b/src/ice_EVP.F90
@@ -19,7 +19,7 @@ module ice_EVP_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-    end interface
+    end interface  
 end module
 
 module ice_EVPdynamics_interface
@@ -33,14 +33,14 @@ module ice_EVPdynamics_interface
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-    end interface
+    end interface  
 end module
 
 !
 ! Contains routines of EVP dynamics
 !
 !_______________________________________________________________________________
-! EVP rheology. The routine computes stress tensor components based on ice
+! EVP rheology. The routine computes stress tensor components based on ice 
 ! velocity field. They are stored as elemental arrays (sigma11, sigma22 and
 ! sigma12). The ocean velocity is at nodal locations.
 subroutine stress_tensor(ice, partit, mesh)
@@ -84,96 +84,88 @@ subroutine stress_tensor(ice, partit, mesh)
     vale = 1.0_WP/(ice%ellipse**2)
     dte  = ice%ice_dt/(1.0_WP*ice%evp_rheol_steps)
     det1 = 1.0_WP/(1.0_WP + 0.5_WP*ice%Tevp_inv*dte)
-    det2 = 1.0_WP/(1.0_WP + 0.5_WP*ice%Tevp_inv*dte) !*ellipse**2
-
-#ifndef ENABLE_OPENACC
+    det2 = 1.0_WP/(1.0_WP + 0.5_WP*ice%Tevp_inv*dte) !*ellipse**2 
+ 
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(el, r1, r2, r3, si1, si2, zeta, delta, delta_inv, d1, d2)
-#else
-!$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do el=1,myDim_elem2D
         !_______________________________________________________________________
-        ! if element contains cavity node skip it
+        ! if element contains cavity node skip it 
         if (ulevels(el) > 1) cycle
-
+        
         ! ===== Check if there is ice on elem
-        ! There is no ice in elem
-        ! if (any(m_ice(elnodes)<= 0.) .or. any(a_ice(elnodes) <=0.)) CYCLE
+        ! There is no ice in elem 
+        ! if (any(m_ice(elnodes)<= 0.) .or. any(a_ice(elnodes) <=0.)) CYCLE     
         if (ice_strength(el) > 0.) then
-            ! =====
+            ! =====	
             ! ===== Deformation rate tensor on element elem:
                 !du/dx
             eps11(el) = sum(gradient_sca(1:3,el)*U_ice(elem2D_nodes(1:3,el))) &
                 - metric_factor(el) * sum(V_ice(elem2D_nodes(1:3,el)))/3.0_WP
-
+            
             eps22(el) = sum(gradient_sca(4:6, el)*V_ice(elem2D_nodes(1:3,el)))
-
+            
             eps12(el) = 0.5_WP*(sum(gradient_sca(4:6,el)*U_ice(elem2D_nodes(1:3,el))) &
                         + sum(gradient_sca(1:3,el)*V_ice(elem2D_nodes(1:3,el))) &
                         + metric_factor(el) * sum(U_ice(elem2D_nodes(1:3,el)))/3.0_WP)
             ! ===== moduli:
             delta = sqrt((eps11(el)*eps11(el) + eps22(el)*eps22(el))*(1.0_WP+vale) + 4.0_WP*vale*eps12(el)*eps12(el) + &
                                 2.0_WP*eps11(el)*eps22(el)*(1.0_WP-vale))
-
+            
             ! =======================================
             ! ===== Here the EVP rheology piece starts
             ! =======================================
-
+            
             ! ===== viscosity zeta should exceed zeta_min
             ! (done via limiting delta from above)
-
+            
             !if(delta>pressure/ice%zeta_min) delta=pressure/ice%zeta_min
-                !It does not work properly by
+                !It does not work properly by 
             !creating response where ice_strength is small
                 ! Uncomment and test if necessary
-
+            
             ! ===== if delta is too small or zero, viscosity will too large (unlimited)
             ! (limit delta_inv)
-            delta_inv = 1.0_WP/max(delta,ice%delta_min)
-            zeta = ice_strength(el)*delta_inv
-            ! ===== Limiting pressure/Delta  (zeta): it may still happen that pressure/Delta
+            delta_inv = 1.0_WP/max(delta,ice%delta_min) 
+            zeta = ice_strength(el)*delta_inv			     
+            ! ===== Limiting pressure/Delta  (zeta): it may still happen that pressure/Delta 
             ! is too large in some regions and CFL criterion is violated.
-            ! The regularization below was introduced by Hunke,
-            ! but seemingly is not used in the current CICE.
-            ! Without it divergence and zeta can be noisy (but code
+            ! The regularization below was introduced by Hunke, 
+            ! but seemingly is not used in the current CICE. 
+            ! Without it divergence and zeta can be noisy (but code 
             ! remains stable), using it reduces viscosities too strongly.
             ! It is therefore commented
-
+            
             !if (zeta>ice%clim_evp*voltriangle(el)) then
             !zeta=ice%clim_evp*voltriangle(el)
-            !end if
-
+            !end if 
+            
             zeta = zeta*ice%Tevp_inv
-
+                            
             r1  = zeta*(eps11(el)+eps22(el)) - ice_strength(el)*ice%Tevp_inv
             r2  = zeta*(eps11(el)-eps22(el))*vale
             r3  = zeta*eps12(el)*vale
-
+            
             si1 = det1*(sigma11(el) + sigma22(el) + dte*r1)
             si2 = det2*(sigma11(el) - sigma22(el) + dte*r2)
-
+            
             sigma12(el) = det2*(sigma12(el)+dte*r3)
             sigma11(el) = 0.5_WP*(si1+si2)
             sigma22(el) = 0.5_WP*(si1-si2)
-
+            
 #if defined (__icepack)
             rdg_conv_elem(el)  = -min((eps11(el)+eps22(el)),0.0_WP)
             rdg_shear_elem(el) = 0.5_WP*(delta - abs(eps11(el)+eps22(el)))
 #endif
         endif
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-!$ACC END PARALLEL LOOP
-#endif
 end subroutine stress_tensor
 !
 !
 !_______________________________________________________________________________
 ! EVP implementation:
 ! Computes the divergence of stress tensor and puts the result into the
-! rhs vectors
+! rhs vectors 
 subroutine stress2rhs(ice, partit, mesh)
     USE MOD_ICE
     USE MOD_PARTIT
@@ -205,100 +197,58 @@ subroutine stress2rhs(ice, partit, mesh)
     rhs_m        => ice%data(2)%values_rhs(:)
     inv_areamass => ice%work%inv_areamass(:)
     ice_strength => ice%work%ice_strength(:)
-
-    !___________________________________________________________________________
+    
+    !___________________________________________________________________________    
     val3=1/3.0_WP
 
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, el, k)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     DO  n=1, myDim_nod2D
         U_rhs_ice(n)=0.0_WP
         V_rhs_ice(n)=0.0_WP
     END DO
-
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#else
-    !$ACC UPDATE SELF(u_rhs_ice, v_rhs_ice, sigma11, sigma12, sigma22)
-#endif
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#endif
     do el=1,myDim_elem2D
         ! ===== Skip if ice is absent
-        !   if (any(m_ice(elnodes)<= 0.) .or. any(a_ice(elnodes) <=0.)) CYCLE
+        !   if (any(m_ice(elnodes)<= 0.) .or. any(a_ice(elnodes) <=0.)) CYCLE 
         !_______________________________________________________________________
-        ! if element contains cavity node skip it
+        ! if element contains cavity node skip it 
         if (ulevels(el) > 1) cycle
-
+        
         !_______________________________________________________________________
         if (ice_strength(el) > 0._WP) then
             DO k=1,3
-
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP) && !defined(__openmp_reproducible)
         call omp_set_lock  (partit%plock(elem2D_nodes(k,el)))
 #else
 !$OMP ORDERED
-#endif
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
 #endif
                 U_rhs_ice(elem2D_nodes(k,el)) = U_rhs_ice(elem2D_nodes(k,el)) &
                 - elem_area(el) * &
                     (sigma11(el)*gradient_sca(k,el) + sigma12(el)*gradient_sca(k+3,el) &
                     +sigma12(el)*val3*metric_factor(el))            !metrics
-
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
-#endif
-                V_rhs_ice(elem2D_nodes(k,el)) = V_rhs_ice(elem2D_nodes(k,el)) &
+                
+                V_rhs_ice(elem2D_nodes(k,el)) = V_rhs_ice(elem2D_nodes(k,el)) & 
                     - elem_area(el) * &
-                    (sigma12(el)*gradient_sca(k,el) + sigma22(el)*gradient_sca(k+3,el) &
+                    (sigma12(el)*gradient_sca(k,el) + sigma22(el)*gradient_sca(k+3,el) &   
                     -sigma11(el)*val3*metric_factor(el))
 
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP) && !defined(__openmp_reproducible)
         call omp_unset_lock(partit%plock(elem2D_nodes(k,el)))
 #else
 !$OMP END ORDERED
-#endif
 #endif
             END DO
         endif
-    end do
-#ifdef ENABLE_OPENACC
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC END PARALLEL LOOP
-#else
-    !$ACC UPDATE DEVICE(u_rhs_ice, v_rhs_ice)
-#endif
-#endif 
-
-#ifndef ENABLE_OPENACC
+    end do 
 !$OMP END DO
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     DO n=1, myDim_nod2D
         !_______________________________________________________________________
-        ! if cavity node skip it
+        ! if cavity node skip it 
         if (ulevels_nod2d(n)>1) cycle
-
+        
         !_______________________________________________________________________
         if (inv_areamass(n) > 0._WP) then
             U_rhs_ice(n) = U_rhs_ice(n)*inv_areamass(n) + rhs_a(n)
@@ -308,17 +258,13 @@ subroutine stress2rhs(ice, partit, mesh)
             V_rhs_ice(n) = 0._WP
         endif
     END DO
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-    !$ACC END PARALLEL LOOP
-#endif
 end subroutine stress2rhs
 !
 !
 !_______________________________________________________________________________
-! EVP implementation. Does subcycling and boundary conditions.
+! EVP implementation. Does subcycling and boundary conditions.  
 ! Velocities at nodes
 subroutine EVPdynamics(ice, partit, mesh)
     USE MOD_ICE
@@ -332,7 +278,7 @@ subroutine EVPdynamics(ice, partit, mesh)
     use ice_EVP_interfaces
 #if defined (__icepack)
     use icedrv_main,   only: rdg_conv_elem, rdg_shear_elem, strength
-    use icedrv_main,   only: icepack_to_fesom
+    use icedrv_main,   only: icepack_to_fesom   
 #endif
     IMPLICIT NONE
     type(t_ice)   , intent(inout), target :: ice
@@ -351,7 +297,7 @@ subroutine EVPdynamics(ice, partit, mesh)
     real(kind=WP)   :: eta, delta
     integer         :: k
     real(kind=WP)   :: vale, dx(3), dy(3), val3
-    real(kind=WP)   :: det1, det2, r1, r2, r3, si1, si2, dte
+    real(kind=WP)   :: det1, det2, r1, r2, r3, si1, si2, dte 
     real(kind=WP)   :: zeta, delta_inv, d1, d2
     INTEGER         :: elem
     !_______________________________________________________________________________
@@ -361,7 +307,7 @@ subroutine EVPdynamics(ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: u_ice_old, v_ice_old
     real(kind=WP), dimension(:), pointer  :: u_rhs_ice, v_rhs_ice, rhs_a, rhs_m
     real(kind=WP), dimension(:), pointer  :: u_w, v_w, elevation
-    real(kind=WP), dimension(:), pointer  :: stress_atmice_x, stress_atmice_y
+    real(kind=WP), dimension(:), pointer  :: stress_atmice_x, stress_atmice_y 
     real(kind=WP), dimension(:), pointer  :: inv_areamass, inv_mass, ice_strength
 #if defined (__icepack)
     real(kind=WP), dimension(:), pointer  :: a_ice_old, m_ice_old, m_snow_old
@@ -395,11 +341,11 @@ subroutine EVPdynamics(ice, partit, mesh)
     rhosno          => ice%thermo%rhosno
     rhoice          => ice%thermo%rhoice
     inv_rhowat      => ice%thermo%inv_rhowat
-
+    
     inv_areamass    => ice%work%inv_areamass(:)
     inv_mass        => ice%work%inv_mass(:)
     ice_strength    => ice%work%ice_strength(:)
-
+    
     !___________________________________________________________________________
     ! If Icepack is used, always update the tracers
 #if defined (__icepack)
@@ -416,39 +362,27 @@ subroutine EVPdynamics(ice, partit, mesh)
     rdt=ice%ice_dt/(1.0*ice%evp_rheol_steps)
     ax=cos(ice%theta_io)
     ay=sin(ice%theta_io)
-
+    
     !___________________________________________________________________________
     ! Precompute values that are never changed during the iteration
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do n=1, myDim_nod2D+eDim_nod2D
        inv_areamass(n) =0.0_WP
        inv_mass(n)     =0.0_WP
        rhs_a(n)        =0.0_WP
        rhs_m(n)        =0.0_WP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
 
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(n)
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
-    do n=1,myDim_nod2D
+    do n=1,myDim_nod2D 
         !_______________________________________________________________________
-        ! if cavity node skip it
+        ! if cavity node skip it 
         if (ulevels_nod2d(n)>1) cycle
 
         !_______________________________________________________________________
         if ((rhoice*m_ice(n)+rhosno*m_snow(n)) > 1.e-3_WP) then
-            inv_areamass(n) = 1._WP/(area(1,n)*(rhoice*m_ice(n)+rhosno*m_snow(n)))
+            inv_areamass(n) = 1._WP/(area(1,n)*(rhoice*m_ice(n)+rhosno*m_snow(n))) 
         else
             inv_areamass(n) = 0._WP
         endif
@@ -458,53 +392,40 @@ subroutine EVPdynamics(ice, partit, mesh)
             inv_mass(n) = 0._WP
         else
             inv_mass(n) = (rhoice*m_ice(n)+rhosno*m_snow(n))/a_ice(n)
-            inv_mass(n) = 1.0_WP/max(inv_mass(n), 9.0_WP)        ! Limit the mass
+            inv_mass(n) = 1.0_WP/max(inv_mass(n), 9.0_WP)        ! Limit the mass 
                                         ! if it is too small
         endif
         rhs_a(n)=0.0_WP       ! these are used as temporal storage here
         rhs_m(n)=0.0_WP       ! for the contribution due to ssh
     enddo
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
+
     !___________________________________________________________________________
     use_pice=0
     if (use_floatice .and.  .not. trim(which_ale)=='linfs') use_pice=1
     if ( .not. trim(which_ALE)=='linfs') then
         ! for full free surface include pressure from ice mass
-
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(el, elnodes, msum, asum, aa, p_ice, elevation_elem, elevation_dx, elevation_dy)
 !$OMP DO
-#else
-
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes, elevation_elem, p_ice) DEFAULT(PRESENT)
-#else
-        !$ACC UPDATE SELF(rhs_a, rhs_m, m_ice, a_ice)
-#endif
-#endif
         do el = 1,myDim_elem2D
             elnodes = elem2D_nodes(:,el)
             ice_strength(el)=0.0_WP
             !___________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels(el) > 1) cycle
-
+            
             !___________________________________________________________________
             if (any(m_ice(elnodes)<=0._WP) .or. &
                 any(a_ice(elnodes)<=0._WP)) then
-
+                
                 ! There is no ice in elem
                 ice_strength(el) = 0._WP
-
+                
             !___________________________________________________________________
             else
                 msum = sum(m_ice(elnodes))/3.0_WP
                 asum = sum(a_ice(elnodes))/3.0_WP
-
+                
                 !_______________________________________________________________
                 ! Hunke and Dukowicz c*h*p*
 #if defined (__icepack)
@@ -513,11 +434,11 @@ subroutine EVPdynamics(ice, partit, mesh)
                 ice_strength(el) = ice%pstar*msum*exp(-ice%c_pressure*(1.0_WP-asum))
 #endif
                 ice_strength(el) = 0.5_WP*ice_strength(el)
-
+                
                 !_______________________________________________________________
                 ! use rhs_m and rhs_a for storing the contribution from elevation:
                 aa = 9.81_WP*elem_area(el)/3.0_WP
-
+                
                 !_______________________________________________________________
                 ! add and limit pressure from ice weight in case of floating ice
                 ! like in FESOM 1.4
@@ -526,63 +447,39 @@ subroutine EVPdynamics(ice, partit, mesh)
                     p_ice(n)=min(p_ice(n),max_ice_loading)
                 end do
                 !!PS p_ice= 0.0_WP
-
+                
                 !_______________________________________________________________
                 elevation_elem = elevation(elnodes)
-                elevation_dx   = sum(gradient_sca(1:3,el)*(elevation_elem+p_ice*use_pice))
+                elevation_dx   = sum(gradient_sca(1:3,el)*(elevation_elem+p_ice*use_pice))   
                 elevation_dy   = sum(gradient_sca(4:6,el)*(elevation_elem+p_ice*use_pice))
-
+                
                 !_______________________________________________________________
-                do k = 1, 3
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                    !$ACC ATOMIC UPDATE
-#endif
-                    rhs_a(elnodes(k)) = rhs_a(elnodes(k))-aa*elevation_dx
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                    !$ACC ATOMIC UPDATE
-#endif
-                    rhs_m(elnodes(k)) = rhs_m(elnodes(k))-aa*elevation_dy
-                end do
+                rhs_a(elnodes) = rhs_a(elnodes)-aa*elevation_dx
+                rhs_m(elnodes) = rhs_m(elnodes)-aa*elevation_dy
             end if
         enddo
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END PARALLEL LOOP
-#else
-        !$ACC UPDATE DEVICE(rhs_a, rhs_m, ice_strength)
-#endif
-#endif
     else
         ! for linear free surface
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(el, elnodes, msum, asum, aa, elevation_elem, elevation_dx, elevation_dy)
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#else
-            !$ACC UPDATE SELF(rhs_a, rhs_m, ice_strength, m_ice, a_ice)
-#endif
-#endif
         do el = 1,myDim_elem2D
             ice_strength(el)=0.0_WP
             elnodes = elem2D_nodes(:,el)
             !___________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels(el) > 1) cycle
-
+            
             !___________________________________________________________________
             if (any(m_ice(elnodes) <= 0._WP) .or. &
                 any(a_ice(elnodes) <=0._WP)) then
-
+            
                 ! There is no ice in elem
                 ice_strength(el) = 0._WP
             else
                 msum = sum(m_ice(elnodes))/3.0_WP
                 asum = sum(a_ice(elnodes))/3.0_WP
-
+                
                 ! ===== Hunke and Dukowicz c*h*p*
 #if defined (__icepack)
                 ice_strength(el) = ice%pstar*msum*exp(-ice%c_pressure*(1.0_WP-asum))
@@ -590,130 +487,75 @@ subroutine EVPdynamics(ice, partit, mesh)
                 ice_strength(el) = ice%pstar*msum*exp(-ice%c_pressure*(1.0_WP-asum))
 #endif
                 ice_strength(el) = 0.5_WP*ice_strength(el)
-
+                
                 ! use rhs_m and rhs_a for storing the contribution from elevation:
                 aa = 9.81_WP*elem_area(el)/3.0_WP
-
+                
                 elevation_dx = sum(gradient_sca(1:3,el)*elevation(elnodes))
                 elevation_dy = sum(gradient_sca(4:6,el)*elevation(elnodes))
-
-                do k = 1, 3
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                    !$ACC ATOMIC UPDATE
-#endif
-                    rhs_a(elnodes(k)) = rhs_a(elnodes(k))-aa*elevation_dx
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                    !$ACC ATOMIC UPDATE
-#endif
-                    rhs_m(elnodes(k)) = rhs_m(elnodes(k))-aa*elevation_dy
-                end do
+                
+                rhs_a(elnodes) = rhs_a(elnodes)-aa*elevation_dx
+                rhs_m(elnodes) = rhs_m(elnodes)-aa*elevation_dy
             end if
         enddo
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC END PARALLEL LOOP
-#else
-            !$ACC UPDATE DEVICE(rhs_a, rhs_m, ice_strength)
-#endif
-#endif
     endif ! --> if ( .not. trim(which_ALE)=='linfs') then
-#ifndef ENABLE_OPENACC
-!$OMP PARALLEL DO
-#else
-
+!$OMP PARALLEL DO    
     !___________________________________________________________________________
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
-    do n=1,myDim_nod2D
+    do n=1,myDim_nod2D 
         if (ulevels_nod2d(n)>1) cycle
         rhs_a(n) = rhs_a(n)/area(1,n)
         rhs_m(n) = rhs_m(n)/area(1,n)
     enddo
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     !___________________________________________________________________________
     ! End of Precomputing --> And the ice stepping starts
 #if defined (__icepack)
     rdg_conv_elem(:)  = 0.0_WP
     rdg_shear_elem(:) = 0.0_WP
 #endif
-    do shortstep=1, ice%evp_rheol_steps
+    do shortstep=1, ice%evp_rheol_steps 
         !_______________________________________________________________________
-        !TODO: temporary workaround for cray16.0.1.1 bug
-#if defined(_CRAYFTN)
-	!dir$ noinline
-#endif
         call stress_tensor(ice, partit, mesh)
-#if defined(_CRAYFTN)
-	!dir$ noinline
-#endif
-        call stress2rhs(ice, partit, mesh)
-
+        call stress2rhs(ice, partit, mesh) 
         !_______________________________________________________________________
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, ed, umod, drag, rhsu, rhsv, r_a, r_b, det)
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do n=1,myDim_nod2D+eDim_nod2D
            U_ice_old(n) = U_ice(n) !PS
            V_ice_old(n) = V_ice(n) !PS
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
-        do n=1,myDim_nod2D
+        do n=1,myDim_nod2D 
             !___________________________________________________________________
-            ! if cavity node skip it
+            ! if cavity node skip it 
             if ( ulevels_nod2d(n)>1 ) cycle
-
+            
             !___________________________________________________________________
             if (a_ice(n) >= 0.01_WP) then               ! Skip if ice is absent
                 umod = sqrt((U_ice(n)-U_w(n))**2+(V_ice(n)-V_w(n))**2)
                 drag = ice%cd_oce_ice*umod*density_0*inv_mass(n)
-
+                
                 rhsu = U_ice(n) +rdt*(drag*(ax*U_w(n) - ay*V_w(n))+ &
                         inv_mass(n)*stress_atmice_x(n) + U_rhs_ice(n))
                 rhsv = V_ice(n) +rdt*(drag*(ax*V_w(n) + ay*U_w(n))+ &
                         inv_mass(n)*stress_atmice_y(n) + V_rhs_ice(n))
-
+                
                 r_a      = 1._WP + ax*drag*rdt
                 r_b      = rdt*(mesh%coriolis_node(n) + ay*drag)
                 det      = 1.0_WP/(r_a*r_a + r_b*r_b)
                 U_ice(n) = det*(r_a*rhsu +r_b*rhsv)
                 V_ice(n) = det*(r_a*rhsv -r_b*rhsu)
-            else  ! Set velocities to 0 if ice is absent
+            else  ! Set velocities to 0 if ice is absent 
                 U_ice(n) = 0.0_WP
                 V_ice(n) = 0.0_WP
             end if
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
         !_______________________________________________________________________
         ! apply sea ice velocity boundary condition
-
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        ! With the binary data of np2 goes only inside the first if
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         DO  ed=1,myDim_edge2D
             !___________________________________________________________________
             ! apply coastal sea ice velocity boundary conditions
@@ -721,10 +563,10 @@ subroutine EVPdynamics(ice, partit, mesh)
                 U_ice(edges(1:2,ed))=0.0_WP
                 V_ice(edges(1:2,ed))=0.0_WP
             endif
-
+            
             !___________________________________________________________________
             ! apply sea ice velocity boundary conditions at cavity-ocean edge
-            if (use_cavity) then
+            if (use_cavity) then 
 !                 if ( (ulevels(edge_tri(1,ed))>1) .or. &
 !                     ( edge_tri(2,ed)>0 .and. ulevels(edge_tri(2,ed))>1) ) then
 ! #if defined(_OPENMP)  && !defined(__openmp_reproducible)
@@ -734,14 +576,14 @@ subroutine EVPdynamics(ice, partit, mesh)
 ! #endif
 !                     U_ice(edges(1,ed))=0.0_WP
 !                     V_ice(edges(1,ed))=0.0_WP
-!
+! 
 ! #if defined(_OPENMP) && !defined(__openmp_reproducible)
 !                     call omp_unset_lock(partit%plock(edges(1,ed)))
 !                     call omp_set_lock  (partit%plock(edges(2,ed)))
 ! #endif
 !                     U_ice(edges(2,ed))=0.0_WP
 !                     V_ice(edges(2,ed))=0.0_WP
-!
+! 
 ! #if defined(_OPENMP)  && !defined(__openmp_reproducible)
 !                     call omp_unset_lock(partit%plock(edges(2,ed)))
 ! #else
@@ -749,79 +591,58 @@ subroutine EVPdynamics(ice, partit, mesh)
 ! #endif
 !                 end if
                 if (ulevels(edge_tri(1,ed))>1) then
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                     call omp_set_lock  (partit%plock(edges(1,ed)))
 #else
 !$OMP ORDERED
-#endif
 #endif
                     U_ice(edges(1,ed))=0.0_WP
                     V_ice(edges(1,ed))=0.0_WP
 
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP) && !defined(__openmp_reproducible)
                     call omp_unset_lock(partit%plock(edges(1,ed)))
                     call omp_set_lock  (partit%plock(edges(2,ed)))
-#endif
 #endif
                     U_ice(edges(2,ed))=0.0_WP
                     V_ice(edges(2,ed))=0.0_WP
 
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                     call omp_unset_lock(partit%plock(edges(2,ed)))
 #else
 !$OMP END ORDERED
 #endif
-#endif 
-                elseif ( edge_tri(2,ed)>0) then
+                elseif ( edge_tri(2,ed)>0) then 
                     if (ulevels(edge_tri(2,ed))>1) then
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                     call omp_set_lock  (partit%plock(edges(1,ed)))
 #else
 !$OMP ORDERED
-#endif
 #endif
                     U_ice(edges(1,ed))=0.0_WP
                     V_ice(edges(1,ed))=0.0_WP
 
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP) && !defined(__openmp_reproducible)
                     call omp_unset_lock(partit%plock(edges(1,ed)))
                     call omp_set_lock  (partit%plock(edges(2,ed)))
-#endif
 #endif
                     U_ice(edges(2,ed))=0.0_WP
                     V_ice(edges(2,ed))=0.0_WP
 
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                     call omp_unset_lock(partit%plock(edges(2,ed)))
 #else
 !$OMP END ORDERED
-#endif
-#endif
-
-                    end if
-                end if
-            end if
+#endif                    
+                    
+                    end if 
+                end if 
+            end if 
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-
 !write(*,*) partit%mype, shortstep, 'CP4'
         !_______________________________________________________________________
-        call exchange_nod(U_ice,V_ice,partit, luse_g2g = .true.)
-
-!ifndef ENABLE_OPENACC
+        call exchange_nod(U_ice,V_ice,partit)
 !$OMP BARRIER
-!endif
-    END DO !--> do shortstep=1, ice%evp_rheol_steps
-
+    END DO !--> do shortstep=1, ice%evp_rheol_steps 
 end subroutine EVPdynamics
diff --git a/src/ice_fct.F90 b/src/ice_fct.F90
index d60bf352..f6c7230f 100755
--- a/src/ice_fct.F90
+++ b/src/ice_fct.F90
@@ -29,7 +29,7 @@ module ice_fct_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh),   intent(in),    target :: mesh
         end subroutine
-
+        
         subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -40,7 +40,7 @@ module ice_fct_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh),   intent(in),    target :: mesh
         end subroutine
-
+        
         subroutine ice_TG_rhs_div(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -50,7 +50,7 @@ module ice_fct_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh),   intent(in),    target :: mesh
         end subroutine
-
+        
         subroutine ice_TG_rhs(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -60,7 +60,7 @@ module ice_fct_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh),   intent(in),    target :: mesh
         end subroutine
-
+        
         subroutine ice_update_for_div(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -80,7 +80,7 @@ end module
 ! There is a tunable parameter ice_gamma_fct.
 ! Increasing it leads to positivity preserving solution.
 !
-! Driving routine is fct_ice_solve. It calles other routines
+! Driving routine is fct_ice_solve. It calles other routines 
 ! that do low-order and figh order solutions and then combine them in a flux
 ! corrected way. Taylor-Galerkin scheme is used as a high-order one.
 !
@@ -95,12 +95,12 @@ subroutine ice_TG_rhs(ice, partit, mesh)
     USE MOD_ICE
     use o_PARAM
     USE g_CONFIG
-    implicit none
+    implicit none 
     type(t_ice),    intent(inout), target :: ice
     type(t_partit), intent(inout), target :: partit
     type(t_mesh),   intent(in),    target :: mesh
     !___________________________________________________________________________
-    real(kind=WP)   :: diff, entries(3),  um, vm, vol, dx(3), dy(3)
+    real(kind=WP)   :: diff, entries(3),  um, vm, vol, dx(3), dy(3) 
     integer         :: n, q, row, elem, elnodes(3)
     !___________________________________________________________________________
     ! pointer on necessary derived types
@@ -109,7 +109,7 @@ subroutine ice_TG_rhs(ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: rhs_a, rhs_m, rhs_ms
 #if defined (__oifs) || defined (__ifsinterface)
     real(kind=WP), dimension(:), pointer  :: ice_temp, rhs_temp
-#endif
+#endif 
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -125,43 +125,28 @@ subroutine ice_TG_rhs(ice, partit, mesh)
 #if defined (__oifs) || defined (__ifsinterface)
     ice_temp => ice%data(4)%values(:)
     rhs_temp => ice%data(4)%values_rhs(:)
-#endif
+#endif    
     !___________________________________________________________________________
     ! Taylor-Galerkin (Lax-Wendroff) rhs
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, q, row, elem, elnodes, diff, entries,  um, vm, vol, dx, dy)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     DO row=1, myDim_nod2D
         rhs_m(row)=0._WP
         rhs_a(row)=0._WP
-        rhs_ms(row)=0._WP
+        rhs_ms(row)=0._WP        
 #if defined (__oifs) || defined (__ifsinterface)
         rhs_temp(row)=0._WP
-#endif
+#endif /* (__oifs) */
     END DO
-
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     ! Velocities at nodes
-
-
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT) private(n, q, row, elem, elnodes, diff, entries,  um, vm, vol, dx, dy)
-#endif
     do elem=1,myDim_elem2D          !assembling rhs over elements
         elnodes=elem2D_nodes(:,elem)
         !_______________________________________________________________________
-        ! if cavity element skip it
+        ! if cavity element skip it 
         if (ulevels(elem)>1) cycle
-
+        
         !derivatives
         dx=gradient_sca(1:3,elem)
         dy=gradient_sca(4:6,elem)
@@ -170,40 +155,31 @@ subroutine ice_TG_rhs(ice, partit, mesh)
         !vm=sum(V_ice(elnodes))/3.0_WP
         um=sum(U_ice(elnodes))
         vm=sum(V_ice(elnodes))
-
+     
         !diffusivity
         diff=ice%ice_diff*sqrt(elem_area(elem)/scale_area)
-        !$ACC LOOP SEQ
         DO n=1,3
             row=elnodes(n)
-	    !$ACC LOOP SEQ
-            DO q = 1,3
+            DO q = 1,3 
                 !entries(q)= vol*dt*((dx(n)*um+dy(n)*vm)/3.0_WP - &
                 !            diff*(dx(n)*dx(q)+ dy(n)*dy(q))- &
-                !	       0.5*dt*(um*dx(n)+vm*dy(n))*(um*dx(q)+vm*dy(q)))
+                !	       0.5*dt*(um*dx(n)+vm*dy(n))*(um*dx(q)+vm*dy(q))) 
                 entries(q)= vol*ice%ice_dt*((dx(n)*(um+u_ice(elnodes(q)))+ &
                             dy(n)*(vm+v_ice(elnodes(q))))/12.0_WP - &
                             diff*(dx(n)*dx(q)+ dy(n)*dy(q))- &
-                            0.5_WP*ice%ice_dt*(um*dx(n)+vm*dy(n))*(um*dx(q)+vm*dy(q))/9.0_WP)
+                            0.5_WP*ice%ice_dt*(um*dx(n)+vm*dy(n))*(um*dx(q)+vm*dy(q))/9.0_WP)    
             END DO
-	    !$ACC END LOOP
             rhs_m(row)=rhs_m(row)+sum(entries*m_ice(elnodes))
             rhs_a(row)=rhs_a(row)+sum(entries*a_ice(elnodes))
             rhs_ms(row)=rhs_ms(row)+sum(entries*m_snow(elnodes))
 #if defined (__oifs) || defined (__ifsinterface)
             rhs_temp(row)=rhs_temp(row)+sum(entries*ice_temp(elnodes))
-#endif
+#endif /* (__oifs) */
         END DO
-	!$ACC END LOOP
     end do
-
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-end subroutine ice_TG_rhs
+end subroutine ice_TG_rhs   
 !
 !
 !_______________________________________________________________________________
@@ -217,41 +193,36 @@ subroutine ice_fct_solve(ice, partit, mesh)
   type(t_ice),    intent(inout), target :: ice
   type(t_partit), intent(inout), target :: partit
   type(t_mesh),   intent(in),    target :: mesh
-#include "associate_part_def.h"
-#include "associate_mesh_def.h"
-#include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
   !_____________________________________________________________________________
   ! Driving routine
   call ice_solve_high_order(ice, partit, mesh)   ! uses arrays of low-order solutions as temp
                                     ! storage. It should preceed the call of low
-                                    ! order solution.
+                                    ! order solution.  
   call ice_solve_low_order(ice, partit, mesh)
 
   call ice_fem_fct(1, ice, partit, mesh)    ! m_ice
   call ice_fem_fct(2, ice, partit, mesh)    ! a_ice
   call ice_fem_fct(3, ice, partit, mesh)    ! m_snow
-
 #if defined (__oifs) || defined (__ifsinterface)
   call ice_fem_fct(4, ice, partit, mesh)    ! ice_temp
-#endif
+#endif /* (__oifs) */
 
 end subroutine ice_fct_solve
 !
 !
 !_______________________________________________________________________________
 subroutine ice_solve_low_order(ice, partit, mesh)
-
+ 
     !============================
     ! Low-order solution
     !============================
     !
-    ! It is assumed that m_ice, a_ice and m_snow from the previous time step
+    ! It is assumed that m_ice, a_ice and m_snow from the previous time step 
     ! are known at 1:myDim_nod2D+eDim_nod2D.
     ! We add diffusive contribution to the rhs. The diffusion operator
     ! is implemented as the difference between the consistent and lumped mass
-    ! matrices acting on the field from the previous time step. The consistent
-    ! mass matrix on the lhs is replaced with the lumped one.
+    ! matrices acting on the field from the previous time step. The consistent 
+    ! mass matrix on the lhs is replaced with the lumped one.       
     USE MOD_ICE
     USE MOD_PARTIT
     USE MOD_PARSUP
@@ -272,7 +243,7 @@ subroutine ice_solve_low_order(ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: mass_matrix
 #if defined (__oifs) || defined (__ifsinterface)
     real(kind=WP), dimension(:), pointer  :: ice_temp, rhs_temp, m_templ
-#endif
+#endif 
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -291,21 +262,16 @@ subroutine ice_solve_low_order(ice, partit, mesh)
     ice_temp     => ice%data(4)%values(:)
     rhs_temp     => ice%data(4)%values_rhs(:)
     m_templ      => ice%data(4)%valuesl(:)
-#endif
+#endif       
     !___________________________________________________________________________
     gamma=ice%ice_gamma_fct         ! Added diffusivity parameter
                                 ! Adjust it to ensure posivity of solution
-
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(row, clo, clo2, cn, location)
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR PRESENT(ssh_stiff, ssh_stiff%rowptr) PRIVATE(location) DEFAULT(PRESENT)
-#endif
     do row=1,myDim_nod2D
         !_______________________________________________________________________
         ! if there is cavity no ice fxt low order
         if (ulevels_nod2D(row)>1) cycle
-
+        
         !_______________________________________________________________________
         clo=ssh_stiff%rowptr(row)-ssh_stiff%rowptr(1)+1
         clo2=ssh_stiff%rowptr(row+1)-ssh_stiff%rowptr(1)
@@ -324,23 +290,16 @@ subroutine ice_solve_low_order(ice, partit, mesh)
         m_templ(row)=(rhs_temp(row)+gamma*sum(mass_matrix(clo:clo2)* &
                   ice_temp(location(1:cn))))/area(1,row) + &
                   (1.0_WP-gamma)*ice_temp(row)
-#endif
+#endif /* (__oifs) */
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else 
-   !$ACC END PARALLEL LOOP
-#endif
     ! Low-order solution must be known to neighbours
-    call exchange_nod(m_icel,a_icel,m_snowl, partit, luse_g2g = .true.)
+    call exchange_nod(m_icel,a_icel,m_snowl, partit)
 #if defined (__oifs) || defined (__ifsinterface)
-    call exchange_nod(m_templ, partit, luse_g2g = .true.)
-#endif
-
-#ifndef ENABLE_OPENACC
+    call exchange_nod(m_templ, partit)
+#endif /* (__oifs) */
 !$OMP BARRIER
-#endif
-end subroutine ice_solve_low_order
+end subroutine ice_solve_low_order     
 !
 !
 !_______________________________________________________________________________
@@ -368,7 +327,7 @@ subroutine ice_solve_high_order(ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: mass_matrix
 #if defined (__oifs) || defined (__ifsinterface)
     real(kind=WP), dimension(:), pointer  :: rhs_temp, m_templ, dm_temp
-#endif
+#endif 
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -387,51 +346,36 @@ subroutine ice_solve_high_order(ice, partit, mesh)
     rhs_temp     => ice%data(4)%values_rhs(:)
     m_templ      => ice%data(4)%valuesl(:)
     dm_temp      => ice%data(4)%dvalues(:)
-#endif
+#endif 
     !___________________________________________________________________________
     ! Does Taylor-Galerkin solution
     !
     !the first approximation
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(row)
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do row=1,myDim_nod2D
-        ! if cavity node skip it
+        ! if cavity node skip it 
         if (ulevels_nod2d(row)>1) cycle
-
+        
         dm_ice(row)=rhs_m(row)/area(1,row)
         da_ice(row)=rhs_a(row)/area(1,row)
         dm_snow(row)=rhs_ms(row)/area(1,row)
 #if defined (__oifs) || defined (__ifsinterface)
         dm_temp(row)=rhs_temp(row)/area(1,row)
-#endif
+#endif /* (__oifs) */
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-    call exchange_nod(dm_ice, da_ice, dm_snow, partit, luse_g2g = .true.)
+    call exchange_nod(dm_ice, da_ice, dm_snow, partit)
 #if defined (__oifs) || defined (__ifsinterface)
-    call exchange_nod(dm_temp, partit, luse_g2g = .true.)
+    call exchange_nod(dm_temp, partit)
 #endif /* (__oifs) */
-#ifndef ENABLE_OPENACC
 !$OMP BARRIER
-#endif
     !___________________________________________________________________________
     !iterate
-
     do n=1,num_iter_solve-1
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, clo, clo2, cn, location, row, rhs_new)
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR PRESENT(ssh_stiff, ssh_stiff%rowptr) PRIVATE(location) DEFAULT(PRESENT)
-#endif
         do row=1,myDim_nod2D
-            ! if cavity node skip it
+            ! if cavity node skip it 
             if (ulevels_nod2d(row)>1) cycle
             !___________________________________________________________________
             clo  = ssh_stiff%rowptr(row)-ssh_stiff%rowptr(1)+1
@@ -444,57 +388,43 @@ subroutine ice_solve_high_order(ice, partit, mesh)
             rhs_new     = rhs_a(row) - sum(mass_matrix(clo:clo2)*da_ice(location(1:cn)))
             a_icel(row) = da_ice(row)+rhs_new/area(1,row)
             rhs_new     = rhs_ms(row) - sum(mass_matrix(clo:clo2)*dm_snow(location(1:cn)))
-            m_snowl(row)= dm_snow(row)+rhs_new/area(1,row)
+            m_snowl(row)= dm_snow(row)+rhs_new/area(1,row) 
 #if defined (__oifs) || defined (__ifsinterface)
             rhs_new     = rhs_temp(row) - sum(mass_matrix(clo:clo2)*dm_temp(location(1:cn)))
             m_templ(row)= dm_temp(row)+rhs_new/area(1,row)
-#endif
+#endif /* (__oifs) */
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
         !_______________________________________________________________________
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do row=1,myDim_nod2D
-            ! if cavity node skip it
+            ! if cavity node skip it 
             if (ulevels_nod2d(row)>1) cycle
             dm_ice(row)=m_icel(row)
             da_ice(row)=a_icel(row)
             dm_snow(row)=m_snowl(row)
 #if defined (__oifs) || defined (__ifsinterface)
             dm_temp(row)=m_templ(row)
-#endif
+#endif /* (__oifs) */
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-        !$ACC END PARALLEL LOOP
-#endif
         !_______________________________________________________________________
-        call exchange_nod(dm_ice, da_ice, dm_snow, partit, luse_g2g = .true.)
+        call exchange_nod(dm_ice, da_ice, dm_snow, partit)
 #if defined (__oifs) || defined (__ifsinterface)
-        call exchange_nod(dm_temp, partit, luse_g2g = .true.)
+        call exchange_nod(dm_temp, partit)
 #endif /* (__oifs) */
-#ifndef ENABLE_OPENACC
 !$OMP BARRIER
-#endif
     end do
 end subroutine ice_solve_high_order
 !
 !
 !_______________________________________________________________________________
 ! Flux corrected transport algorithm for tracer advection
-! It is based on Loehner et al. (Finite-element flux-corrected
-! transport (FEM-FCT) for the Euler and Navier-Stokes equation,
+! It is based on Loehner et al. (Finite-element flux-corrected 
+! transport (FEM-FCT) for the Euler and Navier-Stokes equation, 
 ! Int. J. Numer. Meth. Fluids, 7 (1987), 1093--1109) as described by Kuzmin and
-! Turek. (kuzmin@math.uni-dortmund.de)
+! Turek. (kuzmin@math.uni-dortmund.de) 
 subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
     USE MOD_ICE
     USE MOD_PARTIT
@@ -519,7 +449,7 @@ subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
     real(kind=WP), dimension(:,:), pointer  :: icefluxes
 #if defined (__oifs) || defined (__ifsinterface)
     real(kind=WP), dimension(:)  , pointer  :: ice_temp, m_templ, dm_temp
-#endif
+#endif     
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -542,111 +472,81 @@ subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
     ice_temp  => ice%data(4)%values(:)
     m_templ   => ice%data(4)%valuesl(:)
     dm_temp   => ice%data(4)%dvalues(:)
-#endif
+#endif   
     !___________________________________________________________________________
-    ! It should coinside with gamma in ts_solve_low_order
-    gamma=ice%ice_gamma_fct
-
+    ! It should coinside with gamma in ts_solve_low_order  
+    gamma=ice%ice_gamma_fct        
+  
     !___________________________________________________________________________
     ! Compute elemental antidiffusive fluxes to nodes
-    ! This is the most unpleasant part ---
-    ! it takes memory and time. For every element
-    ! we need its antidiffusive contribution to
+    ! This is the most unpleasant part --- 
+    ! it takes memory and time. For every element 
+    ! we need its antidiffusive contribution to 
     ! each of its 3 nodes
-#ifndef ENABLE_OPENACC
-!$OMP PARALLEL DO
-#else
-    !$ACC DATA CREATE(icoef, elnodes)
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
-    do n = 1, myDim_nod2D + eDim_nod2D
-        tmax(n) = 0.0_WP
-        tmin(n) = 0.0_WP
-    end do
-#ifndef ENABLE_OPENACC
-!$OMP END PARALLEL  DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
+    tmax = 0.0_WP
+    tmin = 0.0_WP
+    
     ! Auxiliary elemental operator (mass matrix- lumped mass matrix)
-
-    !$ACC KERNELS
-    icoef = 1
-    !$ACC END KERNELS
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
+    icoef=1
     do n=1,3   ! three upper nodes
         ! Cycle over rows  row=elnodes(n)
         icoef(n,n)=-2
-    end do
-    !$ACC END PARALLEL LOOP
-
+    end do	    
 
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, q, elem, elnodes, row, vol, flux, ae)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#endif
     do elem=1, myDim_elem2D
         !_______________________________________________________________________
         elnodes=elem2D_nodes(:,elem)
-
+        
         !_______________________________________________________________________
         ! if cavity cycle over
         if(ulevels(elem)>1) cycle !LK89140
-
+        
         !_______________________________________________________________________
         vol=elem_area(elem)
         if (tr_array_id==1) then
-            do q=1,3
-                icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*m_ice(elnodes) + &
+            do q=1,3       
+            icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*m_ice(elnodes) + &
                             dm_ice(elnodes)))*(vol/area(1,elnodes(q)))/12.0_WP
             end do
         end if
-
+        
         if (tr_array_id==2) then
-            do q=1,3
-                icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*a_ice(elnodes) + &
+            do q=1,3       
+            icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*a_ice(elnodes) + &
                             da_ice(elnodes)))*(vol/area(1,elnodes(q)))/12.0_WP
             end do
         end if
-
+        
         if (tr_array_id==3) then
-            do q=1,3
-                icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*m_snow(elnodes) + &
+            do q=1,3       
+            icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*m_snow(elnodes) + &
                             dm_snow(elnodes)))*(vol/area(1,elnodes(q)))/12.0_WP
             end do
         end if
-
+        
 #if defined (__oifs) || defined (__ifsinterface)
         if (tr_array_id==4) then
             do q=1,3
-                icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*ice_temp(elnodes) + &
+            icefluxes(elem,q)=-sum(icoef(:,q)*(gamma*ice_temp(elnodes) + &
                             dm_temp(elnodes)))*(vol/area(1,elnodes(q)))/12.0_WP
             end do
         end if
-#endif
+#endif /* (__oifs) */
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     !___________________________________________________________________________
     ! Screening the low-order solution
     ! TO BE ADDED IF FOUND NECESSARY
     ! Screening means comparing low-order solutions with the
-    ! solution on the previous time step and using whichever
+    ! solution on the previous time step and using whichever 
     ! is greater/smaller in computations of max/min below
-
+    
     !___________________________________________________________________________
     ! Cluster min/max
     if (tr_array_id==1) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do row=1, myDim_nod2D
             if (ulevels_nod2d(row)>1) cycle
             n=nn_num(row)
@@ -656,19 +556,11 @@ subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
             tmax(row)=tmax(row)-m_icel(row)
             tmin(row)=tmin(row)-m_icel(row)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
     end if
-
+    
     if (tr_array_id==2) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do row=1, myDim_nod2D
             if (ulevels_nod2d(row)>1) cycle
             n=nn_num(row)
@@ -678,19 +570,11 @@ subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
             tmax(row)=tmax(row)-a_icel(row)
             tmin(row)=tmin(row)-a_icel(row)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
     end if
-
+ 
     if (tr_array_id==3) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do row=1, myDim_nod2D
             if (ulevels_nod2d(row)>1) cycle
             n=nn_num(row)
@@ -700,20 +584,12 @@ subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
             tmax(row)=tmax(row)-m_snowl(row)
             tmin(row)=tmin(row)-m_snowl(row)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
     end if
 
 #if defined (__oifs) || defined (__ifsinterface)
     if (tr_array_id==4) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do row=1, myDim_nod2D
             if (ulevels_nod2d(row)>1) cycle
             n=nn_num(row)
@@ -723,407 +599,224 @@ subroutine ice_fem_fct(tr_array_id, ice, partit, mesh)
             tmax(row)=tmax(row)-m_templ(row)
             tmin(row)=tmin(row)-m_templ(row)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
     end if
-#endif
-
+#endif /* (__oifs) */
+ 
     !___________________________________________________________________________
     ! Sums of positive/negative fluxes to node row
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do n=1, myDim_nod2D+eDim_nod2D
        icepplus (n)=0._WP
        icepminus(n)=0._WP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#else
-    !$ACC UPDATE SELF(icefluxes, icepplus, icepminus)
-#endif
-#endif
     do elem=1, myDim_elem2D
         ! if cavity cycle over
         if(ulevels(elem)>1) cycle !LK89140
-
+        
         !_______________________________________________________________________
         elnodes=elem2D_nodes(:,elem)
         do q=1,3
-            n=elnodes(q)
+            n=elnodes(q) 
             flux=icefluxes(elem,q)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
         call omp_set_lock  (partit%plock(n))
 #else
 !$OMP ORDERED
-#endif
 #endif
             if (flux>0) then
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
-#endif
                 icepplus(n)=icepplus(n)+flux
             else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
-#endif
                 icepminus(n)=icepminus(n)+flux
             end if
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
         call omp_unset_lock(partit%plock(n))
 #else
 !$OMP END ORDERED
 #endif
-#endif
-        end do
-    end do
-#ifndef ENABLE_OPENACC
+        end do  
+    end do   
 !$OMP END DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC END PARALLEL LOOP
-#else
-    !$ACC UPDATE DEVICE(icepplus, icepminus)
-#endif
-#endif
-
-
     !___________________________________________________________________________
     ! The least upper bound for the correction factors
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR PRESENT(icepplus, icepminus) DEFAULT(PRESENT)
-#endif
     do n=1,myDim_nod2D
         ! if cavity cycle over
         if(ulevels_nod2D(n)>1) cycle !LK89140
-
+        
         flux=icepplus(n)
         if (abs(flux)>0) then
-            icepplus(n)=min(1.0_WP,tmax(n)/max(flux,1.e-12))
+            icepplus(n)=min(1.0_WP,tmax(n)/flux)
         else
             icepplus(n)=0._WP
         end if
-
+        
         flux=icepminus(n)
         if (abs(flux)>0) then
-            icepminus(n)=min(1.0_WP,tmin(n)/min(flux,-1.e-12))
+            icepminus(n)=min(1.0_WP,tmin(n)/flux)
         else
             icepminus(n)=0._WP
         end if
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif 
-   ! pminus and pplus are to be known to neighbouting PE
-!$ACC wait
-
-#if defined(_OPENMP)
+    ! pminus and pplus are to be known to neighbouting PE
 !$OMP MASTER
-#endif
-    call exchange_nod(icepminus, icepplus, partit, luse_g2g = .true.)
-#if defined(_OPENMP)
+    call exchange_nod(icepminus, icepplus, partit)
 !$OMP END MASTER
 !$OMP BARRIER
-#endif
     !___________________________________________________________________________
     ! Limiting
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR PRESENT(icepplus, icepminus) PRIVATE(elnodes) DEFAULT(PRESENT)
-#endif
     do elem=1, myDim_elem2D
         ! if cavity cycle over
         if(ulevels(elem)>1) cycle !LK89140
-
+        
         !_______________________________________________________________________
         elnodes=elem2D_nodes(:,elem)
         ae=1.0_WP
         do q=1,3
-            n=elnodes(q)
+            n=elnodes(q)  
             flux=icefluxes(elem,q)
             if(flux>=0._WP) ae=min(ae,icepplus(n))
             if(flux<0._WP) ae=min(ae,icepminus(n))
         end do
         icefluxes(elem,:)=ae*icefluxes(elem,:)
-    end do
-#ifndef ENABLE_OPENACC
-!$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
+    end do   
+!$OMP END DO  
     !___________________________________________________________________________
-    ! Update the solution
+    ! Update the solution 
     if(tr_array_id==1) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do n=1,myDim_nod2D
             if(ulevels_nod2D(n)>1) cycle !LK89140
             m_ice(n)=m_icel(n)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#else
-        !$ACC UPDATE SELF(m_ice, icefluxes)
-#endif
-#endif
         do elem=1, myDim_elem2D
             ! if cavity cycle over
             if(ulevels(elem)>1) cycle !LK89140
-
+            
             elnodes=elem2D_nodes(:,elem)
             do q=1,3
-                n=elnodes(q)
-#ifndef ENABLE_OPENACC
+                n=elnodes(q)  
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_set_lock  (partit%plock(n))
 #else
 !$OMP ORDERED
-#endif
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
 #endif
                 m_ice(n)=m_ice(n)+icefluxes(elem,q)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_unset_lock(partit%plock(n))
 #else
 !$OMP END ORDERED
-#endif
 #endif
             end do
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END PARALLEL LOOP
-#else
-        !$ACC UPDATE DEVICE(m_ice)
-#endif
-#endif
     end if
-
+    
     if(tr_array_id==2) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do n=1,myDim_nod2D
             if(ulevels_nod2D(n)>1) cycle !LK89140
             a_ice(n)=a_icel(n)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP DO
-#else
-        !$ACC END PARALLEL LOOP
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#else
-        !$ACC UPDATE SELF(a_ice, icefluxes)
-#endif
-#endif
         do elem=1, myDim_elem2D
             ! if cavity cycle over
             if(ulevels(elem)>1) cycle !LK89140
-
+            
             elnodes=elem2D_nodes(:,elem)
             do q=1,3
                 n=elnodes(q)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_set_lock  (partit%plock(n))
 #else
 !$OMP ORDERED
-#endif
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
 #endif
                 a_ice(n)=a_ice(n)+icefluxes(elem,q)
-#ifndef ENABLE_OPENACC
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
+#if defined(_OPENMP) && !defined(__openmp_reproducible) 
                 call omp_unset_lock(partit%plock(n))
 #else
 !$OMP END ORDERED
-#endif
 #endif
             end do
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END PARALLEL LOOP
-#else
-        !$ACC UPDATE DEVICE(a_ice)
-#endif
-#endif
     end if
-
+    
     if(tr_array_id==3) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do n=1,myDim_nod2D
             if(ulevels_nod2D(n)>1) cycle !LK89140
             m_snow(n)=m_snowl(n)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP DO
-#else
-        !$ACC END PARALLEL LOOP
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#else
-        !$ACC UPDATE SELF(m_snow, icefluxes)
-#endif
-#endif
         do elem=1, myDim_elem2D
             ! if cavity cycle over
             if(ulevels(elem)>1) cycle !LK89140
-
+            
             elnodes=elem2D_nodes(:,elem)
             do q=1,3
-                n=elnodes(q)
-#ifndef ENABLE_OPENACC
+                n=elnodes(q)  
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_set_lock  (partit%plock(n))
 #else
 !$OMP ORDERED
-#endif
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
 #endif
                 m_snow(n)=m_snow(n)+icefluxes(elem,q)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP) && !defined(__openmp_reproducible)
                 call omp_unset_lock(partit%plock(n))
 #else
 !$OMP END ORDERED
-#endif
 #endif
             end do
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END PARALLEL LOOP
-#else
-        !$ACC UPDATE DEVICE(m_snow)
-#endif
-#endif
     end if
-
+    
 #if defined (__oifs) || defined (__ifsinterface)
     if(tr_array_id==4) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do n=1,myDim_nod2D
             if(ulevels_nod2D(n)>1) cycle !LK89140
             ice_temp(n)=m_templ(n)
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes) DEFAULT(PRESENT)
-#else
-        !$ACC UPDATE SELF(ice_temp, icefluxes)
-#endif
         do elem=1, myDim_elem2D
             ! if cavity cycle over
             if(ulevels(elem)>1) cycle !LK89140
-
+            
             elnodes=elem2D_nodes(:,elem)
             do q=1,3
                 n=elnodes(q)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_set_lock  (partit%plock(n))
 #else
 !$OMP ORDERED
-#endif
-#endif
-#if !defined(DISABLE_OPENACC_ATOMICS)
-                !$ACC ATOMIC UPDATE
 #endif
                 ice_temp(n)=ice_temp(n)+icefluxes(elem,q)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_unset_lock(partit%plock(n))
 #else
 !$OMP END ORDERED
-#endif
 #endif
             end do
         end do
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END PARALLEL LOOP
-#else
-        !$ACC UPDATE DEVICE(ice_temp)
-#endif
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#endif
     end if
-#endif
-#ifndef ENABLE_OPENACC
-!$OMP END PARALLEL
-#endif
-    call exchange_nod(m_ice, a_ice, m_snow, partit, luse_g2g = .true.)
+#endif /* (__oifs) */ || defined (__ifsinterface)
+!$OMP END PARALLEL    
+    call exchange_nod(m_ice, a_ice, m_snow, partit)
 #if defined (__oifs) || defined (__ifsinterface)
-    call exchange_nod(ice_temp, partit, luse_g2g = .true.)
-#endif
-
-!$ACC END DATA
-
+    call exchange_nod(ice_temp, partit)
+#endif /* (__oifs) */
 !$OMP BARRIER
 end subroutine ice_fem_fct
 !
@@ -1141,7 +834,7 @@ SUBROUTINE ice_mass_matrix_fill(ice, partit, mesh)
     type(t_mesh)  , intent(in)   , target :: mesh
     !___________________________________________________________________________
     integer                             :: n, k, row
-    integer                             :: elem, elnodes(3), q, offset, ipos
+    integer                             :: elem, elnodes(3), q, offset, ipos 
     real(kind=WP)                       :: aa
     integer                             :: flag=0, iflag=0
     !___________________________________________________________________________
@@ -1157,14 +850,14 @@ SUBROUTINE ice_mass_matrix_fill(ice, partit, mesh)
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, k, row, elem, elnodes, q, offset, ipos, aa)
 !$OMP DO
     DO elem=1,myDim_elem2D
-        elnodes=elem2D_nodes(:,elem)
-
+        elnodes=elem2D_nodes(:,elem) 
+        
         !_______________________________________________________________________
         do n=1,3
             row=elnodes(n)
             if(row>myDim_nod2D) cycle
             !___________________________________________________________________
-            ! Global-to-local neighbourhood correspondence
+            ! Global-to-local neighbourhood correspondence  
             ! we have to modify col_pos construction for OMP compatibility. The MPI version might become a bit slower :(
             ! loop over number of neghbouring nodes of node-row
             offset=ssh_stiff%rowptr(row)-ssh_stiff%rowptr(1)
@@ -1190,7 +883,7 @@ SUBROUTINE ice_mass_matrix_fill(ice, partit, mesh)
                end if
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                call omp_unset_lock(partit%plock(row))
-#else
+#else 
 !$OMP END ORDERED
 #endif
            END DO
@@ -1202,11 +895,11 @@ SUBROUTINE ice_mass_matrix_fill(ice, partit, mesh)
     DO q=1,myDim_nod2D
         ! if cavity cycle over
         if(ulevels_nod2d(q)>1) cycle
-
+        
         !_______________________________________________________________________
         offset=ssh_stiff%rowptr(q)-ssh_stiff%rowptr(1)+1
         n=ssh_stiff%rowptr(q+1)-ssh_stiff%rowptr(1)
-        aa=sum(mass_matrix(offset:n))
+        aa=sum(mass_matrix(offset:n))  
         !!PS if(abs(area(1,q)-aa)>.1_WP) then
         if(abs(area(ulevels_nod2d(q),q)-aa)>.1_WP) then
 !$OMP CRITICAL
@@ -1245,14 +938,14 @@ subroutine ice_TG_rhs_div(ice, partit, mesh)
     USE MOD_MESH
     use o_PARAM
     USE g_CONFIG
-    implicit none
+    implicit none 
     type(t_ice)   , intent(inout), target :: ice
     type(t_partit), intent(inout), target :: partit
     type(t_mesh)  , intent(in)   , target :: mesh
     !___________________________________________________________________________
-    real(kind=WP)            :: diff, entries(3),  um, vm, vol, dx(3), dy(3), tmp_sum
+    real(kind=WP)            :: diff, entries(3),  um, vm, vol, dx(3), dy(3) 
     integer                  :: n, q, row, elem, elnodes(3)
-    real(kind=WP)            :: c1, c2, c3, c4, cx1, cx2, cx3, cx4, entries2(3)
+    real(kind=WP)            :: c1, c2, c3, c4, cx1, cx2, cx3, cx4, entries2(3) 
     !___________________________________________________________________________
     ! pointer on necessary derived types
     real(kind=WP), dimension(:), pointer  :: u_ice, v_ice
@@ -1261,7 +954,7 @@ subroutine ice_TG_rhs_div(ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: rhs_adiv, rhs_mdiv, rhs_msdiv
 #if defined (__oifs) || defined (__ifsinterface)
     real(kind=WP), dimension(:), pointer  :: ice_temp, rhs_temp, rhs_tempdiv
-#endif
+#endif     
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -1281,16 +974,11 @@ subroutine ice_TG_rhs_div(ice, partit, mesh)
     ice_temp    => ice%data(4)%values(:)
     rhs_temp    => ice%data(4)%values_rhs(:)
     rhs_tempdiv => ice%data(4)%values_div_rhs(:)
-#endif
+#endif    
     !___________________________________________________________________________
-    ! Computes the rhs in a Taylor-Galerkin way (with upwind type of
+    ! Computes the rhs in a Taylor-Galerkin way (with upwind type of 
     ! correction for the advection operator)
     ! In this version I tr to split divergent term off, so that FCT works without it.
-#ifndef ENABLE_OPENACC
-!$OMP PARALLEL DO
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do row=1, myDim_nod2D
                     !! row=myList_nod2D(m)
         rhs_m(row)=0.0_WP
@@ -1298,142 +986,85 @@ subroutine ice_TG_rhs_div(ice, partit, mesh)
         rhs_ms(row)=0.0_WP
 #if defined (__oifs) || defined (__ifsinterface)
         rhs_temp(row)=0.0_WP
-#endif
+#endif /* (__oifs) */
         rhs_mdiv(row)=0.0_WP
         rhs_adiv(row)=0.0_WP
         rhs_msdiv(row)=0.0_WP
 #if defined (__oifs) || defined (__ifsinterface)
-        rhs_tempdiv(row)=0.0_WP
-#endif
+        rhs_tempdiv(row)=0.0_WP        
+#endif /* (__oifs) */
     end do
-#ifndef ENABLE_OPENACC
-!$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(diff, entries, um, vm, vol, dx, dy, n, q, row, elem, elnodes, c1, c2, c3, c4, cx1, cx2, cx3, cx4, entries2)
 !$OMP DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(elnodes, dx, dy, entries, entries2) DEFAULT(PRESENT)
-#else
-    !$ACC UPDATE SELF(rhs_a, rhs_m, rhs_ms, rhs_adiv, rhs_mdiv, rhs_msdiv, u_ice, v_ice, m_ice, a_ice, m_snow)
-#endif
-#endif
     do elem=1,myDim_elem2D          !assembling rhs over elements
         elnodes=elem2D_nodes(:,elem)
-
-        ! if cavity element skip it
+        
+        ! if cavity element skip it 
         if (ulevels(elem)>1) cycle
-
+        
         !derivatives
         dx=gradient_sca(1:3,elem)
         dy=gradient_sca(4:6,elem)
         vol=elem_area(elem)
         um=sum(u_ice(elnodes))
         vm=sum(v_ice(elnodes))
-        ! this is exact computation (no assumption of u=const on elements used
+        ! this is exact computation (no assumption of u=const on elements used 
         ! in the standard version)
-        c1=(um*um+sum(u_ice(elnodes)*u_ice(elnodes)))/12.0_WP
+        c1=(um*um+sum(u_ice(elnodes)*u_ice(elnodes)))/12.0_WP 
         c2=(vm*vm+sum(v_ice(elnodes)*v_ice(elnodes)))/12.0_WP
         c3=(um*vm+sum(v_ice(elnodes)*u_ice(elnodes)))/12.0_WP
         c4=sum(dx*u_ice(elnodes)+dy*v_ice(elnodes))
         do n=1,3
             row=elnodes(n)
                 !!PS         if(ulevels_nod2D(row)>1) cycle !LK89140
-            do q = 1,3
+            do q = 1,3 
                 entries(q)= vol*ice%ice_dt*((1.0_WP-0.5_WP*ice%ice_dt*c4)*(dx(n)*(um+u_ice(elnodes(q)))+ &
                             dy(n)*(vm+v_ice(elnodes(q))))/12.0_WP - &
                             0.5_WP*ice%ice_dt*(c1*dx(n)*dx(q)+c2*dy(n)*dy(q)+c3*(dx(n)*dy(q)+dx(q)*dy(n))))
                         !um*dx(n)+vm*dy(n))*(um*dx(q)+vm*dy(q))/9.0)
                 entries2(q)=0.5_WP*ice%ice_dt*(dx(n)*(um+u_ice(elnodes(q)))+ &
                             dy(n)*(vm+v_ice(elnodes(q)))-dx(q)*(um+u_ice(row))- &
-                            dy(q)*(vm+v_ice(row)))
+                            dy(q)*(vm+v_ice(row)))  
             end do
-
+            
             !___________________________________________________________________
             cx1=vol*ice%ice_dt*c4*(sum(m_ice(elnodes))+m_ice(elnodes(n))+sum(entries2*m_ice(elnodes)))/12.0_WP
             cx2=vol*ice%ice_dt*c4*(sum(a_ice(elnodes))+a_ice(elnodes(n))+sum(entries2*a_ice(elnodes)))/12.0_WP
             cx3=vol*ice%ice_dt*c4*(sum(m_snow(elnodes))+m_snow(elnodes(n))+sum(entries2*m_snow(elnodes)))/12.0_WP
 #if defined (__oifs) || defined (__ifsinterface)
             cx4=vol*ice%ice_dt*c4*(sum(ice_temp(elnodes))+ice_temp(elnodes(n))+sum(entries2*ice_temp(elnodes)))/12.0_WP
-#endif
+#endif /* (__oifs) */
 
             !___________________________________________________________________
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_set_lock  (partit%plock(row))
 #else
 !$OMP ORDERED
 #endif
-#endif
-            tmp_sum = sum(entries*m_ice(elnodes))
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC WRITE
-#endif
-            rhs_m(row)=rhs_m(row)+tmp_sum+cx1
-
-            tmp_sum = sum(entries*a_ice(elnodes))
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC WRITE
-#endif
-            rhs_a(row)=rhs_a(row)+tmp_sum+cx2
-
-            tmp_sum = sum(entries*m_snow(elnodes))
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC WRITE
-#endif
-            rhs_ms(row)=rhs_ms(row)+tmp_sum+cx3
-
+            rhs_m(row)=rhs_m(row)+sum(entries*m_ice(elnodes))+cx1
+            rhs_a(row)=rhs_a(row)+sum(entries*a_ice(elnodes))+cx2
+            rhs_ms(row)=rhs_ms(row)+sum(entries*m_snow(elnodes))+cx3
 #if defined (__oifs) || defined (__ifsinterface)
-            tmp_sum = sum(entries*ice_temp(elnodes))
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
-            rhs_temp(row)=rhs_temp(row)+tmp_sum+cx4
-#endif
-
+            rhs_temp(row)=rhs_temp(row)+sum(entries*ice_temp(elnodes))+cx4
+#endif /* (__oifs) */
+        
             !___________________________________________________________________
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
             rhs_mdiv(row)=rhs_mdiv(row)-cx1
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
             rhs_adiv(row)=rhs_adiv(row)-cx2
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
             rhs_msdiv(row)=rhs_msdiv(row)-cx3
 #if defined (__oifs) || defined (__ifsinterface)
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
             rhs_tempdiv(row)=rhs_tempdiv(row)-cx4
 #endif /* (__oifs) */
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
                 call omp_unset_lock(partit%plock(row))
 #else
 !$OMP END ORDERED
-#endif
 #endif
         end do
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC END PARALLEL LOOP
-#else
-    !$ACC UPDATE DEVICE(rhs_a, rhs_m, rhs_ms, rhs_adiv, rhs_mdiv, rhs_msdiv)
-#endif
-#endif
-end subroutine ice_TG_rhs_div
+end subroutine ice_TG_rhs_div 
 !
 !
 !_______________________________________________________________________________
@@ -1450,7 +1081,7 @@ subroutine ice_update_for_div(ice, partit, mesh)
     type(t_partit), intent(inout), target   :: partit
     type(t_mesh)  , intent(in)   , target   :: mesh
     !___________________________________________________________________________
-    integer                                 :: n,clo,clo2,cn,location(100),row, tmp_it
+    integer                                 :: n,clo,clo2,cn,location(100),row
     real(kind=WP)                           :: rhs_new
     integer                                 :: num_iter_solve=3
     !___________________________________________________________________________
@@ -1462,7 +1093,7 @@ subroutine ice_update_for_div(ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: mass_matrix
 #if defined (__oifs) || defined (__ifsinterface)
     real(kind=WP), dimension(:), pointer  :: ice_temp, m_templ, dm_temp, rhs_tempdiv
-#endif
+#endif 
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -1485,62 +1116,46 @@ subroutine ice_update_for_div(ice, partit, mesh)
     m_templ      => ice%data(4)%valuesl(:)
     dm_temp      => ice%data(4)%dvalues(:)
     rhs_tempdiv  => ice%data(4)%values_div_rhs(:)
-#endif
+#endif        
     !___________________________________________________________________________
     ! Does Taylor-Galerkin solution
     ! the first approximation
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(row)
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do row=1,myDim_nod2D
         !! row=myList_nod2D(m)
-        ! if cavity node skip it
+        ! if cavity node skip it 
         if (ulevels_nod2d(row)>1) cycle
-
+        
         dm_ice(row) =rhs_mdiv(row) /area(1,row)
         da_ice(row) =rhs_adiv(row) /area(1,row)
         dm_snow(row)=rhs_msdiv(row)/area(1,row)
 #if defined (__oifs) || defined (__ifsinterface)
         dm_temp(row)=rhs_tempdiv(row)/area(1,row)
-#endif
+#endif /* (__oifs) */
     end do
-#ifndef ENABLE_OPENACC
-!$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-    call exchange_nod(dm_ice, partit, luse_g2g = .true.)
-    call exchange_nod(da_ice, partit, luse_g2g = .true.)
-    call exchange_nod(dm_snow, partit, luse_g2g = .true.)
+!$OMP END PARALLEL DO 
+    call exchange_nod(dm_ice, partit)
+    call exchange_nod(da_ice, partit)
+    call exchange_nod(dm_snow, partit)
 #if defined (__oifs) || defined (__ifsinterface)
-    call exchange_nod(dm_temp, partit, luse_g2g = .true.)
+    call exchange_nod(dm_temp, partit)
 #endif /* (__oifs) */
-#ifndef ENABLE_OPENACC
 !$OMP BARRIER
-#endif
     !___________________________________________________________________________
-    !iterate
+    !iterate 
     do n=1,num_iter_solve-1
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(row, n, clo, clo2, cn, location, rhs_new)
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR PRIVATE(location) DEFAULT(PRESENT)
-#endif
         do row=1,myDim_nod2D
-            ! if cavity node skip it
+            ! if cavity node skip it 
             if (ulevels_nod2d(row)>1) cycle
                   !! row=myList_nod2D(m)
             !___________________________________________________________________
             clo=ssh_stiff%rowptr(row)-ssh_stiff%rowptr(1)+1
             clo2=ssh_stiff%rowptr(row+1)-ssh_stiff%rowptr(1)
             cn=clo2-clo+1
-            do tmp_it = 1, cn
-                location(tmp_it)=nn_pos(tmp_it, row)
-            end do
-
+            location(1:cn)=nn_pos(1:cn, row)
+            
             !___________________________________________________________________
             rhs_new     = rhs_mdiv(row) - sum(mass_matrix(clo:clo2)*dm_ice(location(1:cn)))
             m_icel(row) = dm_ice(row)+rhs_new/area(1,row)
@@ -1551,63 +1166,40 @@ subroutine ice_update_for_div(ice, partit, mesh)
 #if defined (__oifs) || defined (__ifsinterface)
             rhs_new     = rhs_tempdiv(row) - sum(mass_matrix(clo:clo2)*dm_temp(location(1:cn)))
             m_templ(row)= dm_temp(row)+rhs_new/area(1,row)
-#endif
+#endif /* (__oifs) */
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
         do row=1,myDim_nod2D
-            ! if cavity node skip it
+            ! if cavity node skip it 
             if (ulevels_nod2d(row)>1) cycle
             dm_ice(row)  = m_icel(row)
             da_ice(row)  = a_icel(row)
             dm_snow(row) = m_snowl(row)
 #if defined (__oifs) || defined (__ifsinterface)
             dm_temp(row) = m_templ(row)
-#endif
+#endif /* (__oifs) */
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-        call exchange_nod(dm_ice, partit, luse_g2g = .true.)
-        call exchange_nod(da_ice, partit, luse_g2g = .true.)
-        call exchange_nod(dm_snow, partit, luse_g2g = .true.)
+        call exchange_nod(dm_ice, partit)
+        call exchange_nod(da_ice, partit)
+        call exchange_nod(dm_snow, partit)
 #if defined (__oifs) || defined (__ifsinterface)
-        call exchange_nod(dm_temp, partit, luse_g2g = .true.)
+        call exchange_nod(dm_temp, partit)
 #endif /* (__oifs) */
-#ifndef ENABLE_OPENACC
 !$OMP BARRIER
-#endif
     end do
 
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(row)
-#else
-    !$ACC PARALLEL LOOP GANG VECTOR DEFAULT(PRESENT)
-#endif
     do row=1, myDim_nod2D+eDim_nod2D
        m_ice(row)   = m_ice (row)+dm_ice (row)
        a_ice(row)   = a_ice (row)+da_ice (row)
-       m_snow(row)  = m_snow(row)+dm_snow(row)
+       m_snow(row)  = m_snow(row)+dm_snow(row)     
 #if defined (__oifs) || defined (__ifsinterface)
        ice_temp(row)= ice_temp(row)+dm_temp(row)
-#endif
+#endif /* (__oifs) */
     end do
-#ifndef ENABLE_OPENACC
-!$OMP END PARALLEL DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
+!$OMP END PARALLEL DO 
 end subroutine ice_update_for_div
 ! =============================================================
diff --git a/src/ice_maEVP.F90 b/src/ice_maEVP.F90
index 62fa9c0a..c33ab6a4 100644
--- a/src/ice_maEVP.F90
+++ b/src/ice_maEVP.F90
@@ -9,7 +9,7 @@ module ice_maEVP_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-
+        
         subroutine stress_tensor_a(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -19,7 +19,7 @@ module ice_maEVP_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-
+        
         subroutine stress2rhs_m(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -29,7 +29,7 @@ module ice_maEVP_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-
+        
         subroutine find_alpha_field_a(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -39,7 +39,7 @@ module ice_maEVP_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-
+        
         subroutine find_beta_field_a(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -49,7 +49,7 @@ module ice_maEVP_interfaces
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
-   end interface
+   end interface  
 end module
 
 module ice_maEVPdynamics_interface
@@ -63,7 +63,7 @@ module ice_maEVPdynamics_interface
         type(t_partit), intent(inout), target :: partit
         type(t_ice)   , intent(inout), target :: ice
         end subroutine
-
+        
         subroutine EVPdynamics_m(ice, partit, mesh)
         USE MOD_ICE
         USE MOD_PARTIT
@@ -73,8 +73,8 @@ module ice_maEVPdynamics_interface
         type(t_partit), intent(inout), target :: partit
         type(t_ice)   , intent(inout), target :: ice
         end subroutine
-   end interface
-end module
+   end interface  
+end module 
 !
 !
 !_______________________________________________________________________________
@@ -113,7 +113,7 @@ subroutine stress_tensor_m(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     a_ice        => ice%data(1)%values(:)
     m_ice        => ice%data(2)%values(:)
     eps11        => ice%work%eps11(:)
@@ -124,45 +124,44 @@ subroutine stress_tensor_m(ice, partit, mesh)
     sigma22      => ice%work%sigma22(:)
     u_ice_aux    => ice%uice_aux(:)
     v_ice_aux    => ice%vice_aux(:)
-
+    
     !___________________________________________________________________________
     val3=1.0_WP/3.0_WP
     vale=1.0_WP/(ice%ellipse**2)
     det2=1.0_WP/(1.0_WP+ice%alpha_evp)
     det1=ice%alpha_evp*det2
-!$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(elem, elnodes, dx, dy, msum, asum, eps1, eps2, pressure, delta, meancos, usum, vsum, r1, r2, r3, si1, si2)
     do elem=1,myDim_elem2D
         elnodes=elem2D_nodes(:,elem)
         !_______________________________________________________________________
-        ! if element has any cavity node skip it
+        ! if element has any cavity node skip it 
         if (ulevels(elem) > 1) cycle
-
+        
         msum=sum(m_ice(elnodes))*val3
         if(msum<=0.01_WP) cycle !DS
         asum=sum(a_ice(elnodes))*val3
-
+        
         dx=gradient_sca(1:3,elem)
-        dy=gradient_sca(4:6,elem)
+        dy=gradient_sca(4:6,elem)     
         ! METRICS:
             vsum=sum(v_ice_aux(elnodes))
             usum=sum(u_ice_aux(elnodes))
             meancos=metric_factor(elem)
-        !
+        !  
         ! ====== Deformation rate tensor on element elem:
         eps11(elem)=sum(dx*u_ice_aux(elnodes))
         eps11(elem)=eps11(elem)-val3*vsum*meancos                !metrics
         eps22(elem)=sum(dy*v_ice_aux(elnodes))
         eps12(elem)=0.5_WP*sum(dy*u_ice_aux(elnodes) + dx*v_ice_aux(elnodes))
-        eps12(elem)=eps12(elem)+0.5_WP*val3*usum*meancos          !metrics
-
+        eps12(elem)=eps12(elem)+0.5_WP*val3*usum*meancos          !metrics 
+        
         ! ======= Switch to eps1,eps2
         eps1=eps11(elem)+eps22(elem)
-        eps2=eps11(elem)-eps22(elem)
-
+        eps2=eps11(elem)-eps22(elem)   
+        
         ! ====== moduli:
         delta=eps1**2+vale*(eps2**2+4.0_WP*eps12(elem)**2)
         delta=sqrt(delta)
-
+        
 #if defined (__icepack)
         pressure = sum(strength(elnodes))*val3/max(delta,ice%delta_min)
 #else
@@ -173,7 +172,7 @@ subroutine stress_tensor_m(ice, partit, mesh)
         r3=pressure*eps12(elem)*vale
         si1=sigma11(elem)+sigma22(elem)
         si2=sigma11(elem)-sigma22(elem)
-
+        
         si1=det1*si1+det2*r1
         si2=det1*si2+det2*r2
         sigma12(elem)=det1*sigma12(elem)+det2*r3
@@ -185,8 +184,7 @@ subroutine stress_tensor_m(ice, partit, mesh)
         rdg_shear_elem(elem) = 0.5_WP*(delta - abs(eps11(elem)+eps22(elem)))
 #endif
     end do
-!$OMP END PARALLEL DO
-    ! Equations solved in terms of si1, si2, eps1, eps2 are (43)-(45) of
+    ! Equations solved in terms of si1, si2, eps1, eps2 are (43)-(45) of 
     ! Boullion et al Ocean Modelling 2013, but in an implicit mode:
     ! si1_{p+1}=det1*si1_p+det2*r1, where det1=alpha/(1+alpha) and det2=1/(1+alpha),
     ! and similarly for si2 and sigma12
@@ -221,7 +219,7 @@ subroutine ssh2rhs(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     m_ice        => ice%data(2)%values(:)
     m_snow       => ice%data(3)%values(:)
     rhs_a        => ice%data(1)%values_rhs(:)
@@ -230,92 +228,61 @@ subroutine ssh2rhs(ice, partit, mesh)
     rhoice       => ice%thermo%rhoice
     rhosno       => ice%thermo%rhosno
     inv_rhowat   => ice%thermo%inv_rhowat
-
+    
     !___________________________________________________________________________
     val3=1.0_WP/3.0_WP
-
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(row, elem, elnodes, n, dx, dy, vol, meancos, aa, bb, p_ice)
-!$OMP DO
+    
     ! use rhs_m and rhs_a for storing the contribution from elevation:
-    do row=1, myDim_nod2d
+    do row=1, myDim_nod2d 
         rhs_a(row)=0.0_WP
         rhs_m(row)=0.0_WP
     end do
-!$OMP END DO
+  
     !_____________________________________________________________________________
     ! use floating sea ice for zlevel and zstar
     if (use_floatice .and.  .not. trim(which_ale)=='linfs') then
-!$OMP DO
-        do elem=1,myDim_elem2d
+        do elem=1,myDim_elem2d         
             elnodes=elem2D_nodes(:,elem)
             !_______________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels(elem) > 1) cycle
-
+            
             !_______________________________________________________________________
             vol=elem_area(elem)
             dx=gradient_sca(1:3,elem)
-            dy=gradient_sca(4:6,elem)
-
+            dy=gradient_sca(4:6,elem)     
+            
             !_______________________________________________________________________
             ! add pressure gradient from sea ice --> in case of floating sea ice
             p_ice=(rhoice*m_ice(elnodes)+rhosno*m_snow(elnodes))*inv_rhowat
             do n=1,3
                 p_ice(n)=min(p_ice(n),max_ice_loading)
             end do
-
+            
             !_______________________________________________________________________
             bb=g*val3*vol
             aa=bb*sum(dx*(elevation(elnodes)+p_ice))
             bb=bb*sum(dy*(elevation(elnodes)+p_ice))
-            do n=1,3
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                call omp_set_lock  (partit%plock(elnodes(n)))
-#else
-!$OMP ORDERED
-#endif
-               rhs_a(elnodes(n))=rhs_a(elnodes(n))-aa
-               rhs_m(elnodes(n))=rhs_m(elnodes(n))-bb
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-               call omp_unset_lock(partit%plock(elnodes(n)))
-#else
-!$OMP END ORDERED
-#endif
-            end do
+            rhs_a(elnodes)=rhs_a(elnodes)-aa    
+            rhs_m(elnodes)=rhs_m(elnodes)-bb
         end do
-!$OMP END DO
     else
-!$OMP DO
-        do elem=1,myDim_elem2d
+        do elem=1,myDim_elem2d         
             elnodes=elem2D_nodes(:,elem)
             !_______________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels(elem) > 1) cycle
-
+            
             vol=elem_area(elem)
             dx=gradient_sca(1:3,elem)
-            dy=gradient_sca(4:6,elem)
+            dy=gradient_sca(4:6,elem)     
             bb=g*val3*vol
             aa=bb*sum(dx*elevation(elnodes))
             bb=bb*sum(dy*elevation(elnodes))
-            do n=1,3
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                call omp_set_lock  (partit%plock(elnodes(n)))
-#else
-!$OMP ORDERED
-#endif
-               rhs_a(elnodes(n))=rhs_a(elnodes(n))-aa
-               rhs_m(elnodes(n))=rhs_m(elnodes(n))-bb
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-               call omp_unset_lock(partit%plock(elnodes(n)))
-#else
-!$OMP END ORDERED
-#endif
-            end do
+            rhs_a(elnodes)=rhs_a(elnodes)-aa   
+            rhs_m(elnodes)=rhs_m(elnodes)-bb
         end do
-!$OMP END DO
-    end if
-!$OMP END PARALLEL
+    end if 
 end subroutine ssh2rhs
 !
 !
@@ -347,7 +314,7 @@ subroutine stress2rhs_m(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     a_ice        => ice%data(1)%values(:)
     m_ice        => ice%data(2)%values(:)
     m_snow       => ice%data(3)%values(:)
@@ -360,65 +327,49 @@ subroutine stress2rhs_m(ice, partit, mesh)
     rhs_m        => ice%data(2)%values_rhs(:)
     rhoice       => ice%thermo%rhoice
     rhosno       => ice%thermo%rhosno
-
+    
     !___________________________________________________________________________
     val3=1.0_WP/3.0_WP
-!$OMP PARALLEL DO
-    do row=1, myDim_nod2d
+    
+    do row=1, myDim_nod2d 
         u_rhs_ice(row)=0.0_WP
         v_rhs_ice(row)=0.0_WP
     end do
-!$OMP END PARALLEL DO
 
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(elem, elnodes, k, row, dx, dy, vol, mf, aa, bb, mass, cluster_area, elevation_elem)
-!$OMP DO
-    do elem=1,myDim_elem2d
+    do elem=1,myDim_elem2d         
         elnodes=elem2D_nodes(:,elem)
         !_______________________________________________________________________
-        ! if element has any cavity node skip it
+        ! if element has any cavity node skip it 
         if (ulevels(elem) > 1) cycle
 
         if(sum(a_ice(elnodes)) < 0.01_WP) cycle !DS
-
+        
         vol=elem_area(elem)
         dx=gradient_sca(1:3,elem)
-        dy=gradient_sca(4:6,elem)
+        dy=gradient_sca(4:6,elem)     
         mf=metric_factor(elem)                               !metrics
 
         do k=1,3
             row=elnodes(k)
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-            call omp_set_lock  (partit%plock(row))
-#else
-!$OMP ORDERED
-#endif
             u_rhs_ice(row)=u_rhs_ice(row) - vol* &
                 (sigma11(elem)*dx(k)+sigma12(elem)*dy(k))    &
         -vol*sigma12(elem)*val3*mf                         !metrics
             v_rhs_ice(row)=v_rhs_ice(row) - vol* &
                 (sigma12(elem)*dx(k)+sigma22(elem)*dy(k))    &
         +vol*sigma11(elem)*val3*mf                         ! metrics
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-        call omp_unset_lock(partit%plock(row))
-#else
-!$OMP END ORDERED
-#endif
         end do
     end do
-!$OMP END DO
-!$OMP DO
-    do row=1, myDim_nod2d
+  
+    do row=1, myDim_nod2d     
         !_______________________________________________________________________
-        ! if cavity node skip it
+        ! if cavity node skip it 
         if ( ulevels_nod2d(row)>1 ) cycle
-
+        
         mass=(m_ice(row)*rhoice+m_snow(row)*rhosno)
         mass=mass/(1.0_WP+mass*mass)
-        u_rhs_ice(row)=(u_rhs_ice(row)*mass + rhs_a(row))/area(1,row)
-        v_rhs_ice(row)=(v_rhs_ice(row)*mass + rhs_m(row))/area(1,row)
+        u_rhs_ice(row)=(u_rhs_ice(row)*mass + rhs_a(row))/area(1,row) 
+        v_rhs_ice(row)=(v_rhs_ice(row)*mass + rhs_m(row))/area(1,row) 
     end do
-!$OMP END DO
-!$OMP END PARALLEL
 end subroutine stress2rhs_m
 !
 !
@@ -454,30 +405,30 @@ subroutine EVPdynamics_m(ice, partit, mesh)
     real(kind=WP)   :: eps1, eps2, pressure, pressure_fac(partit%myDim_elem2D), delta
     real(kind=WP)   :: val3, meancos, vale
     real(kind=WP)   :: det1, det2, r1, r2, r3, si1, si2
-    !NR for stress2rhs_m
+    !NR for stress2rhs_m  
     integer        :: k, row
     real(kind=WP)  :: vol
     real(kind=WP)  :: mf,aa, bb,p_ice(3)
     real(kind=WP)  :: mass(partit%myDim_nod2D)
     !___________________________________________________________________________
     ! pointer on necessary derived types
-    real(kind=WP), dimension(:), pointer  :: u_ice, v_ice
+    real(kind=WP), dimension(:), pointer  :: u_ice, v_ice  
     real(kind=WP), dimension(:), pointer  :: a_ice, m_ice, m_snow
     real(kind=WP), dimension(:), pointer  :: eps11, eps12, eps22
     real(kind=WP), dimension(:), pointer  :: sigma11, sigma12, sigma22
     real(kind=WP), dimension(:), pointer  :: u_rhs_ice, v_rhs_ice, rhs_a, rhs_m
     real(kind=WP), dimension(:), pointer  :: u_w, v_w
     real(kind=WP), dimension(:), pointer  :: elevation
-    real(kind=WP), dimension(:), pointer  :: stress_atmice_x, stress_atmice_y
+    real(kind=WP), dimension(:), pointer  :: stress_atmice_x, stress_atmice_y  
     real(kind=WP), dimension(:), pointer  :: u_ice_aux, v_ice_aux
 #if defined (__icepack)
     real(kind=WP), dimension(:), pointer  :: a_ice_old, m_ice_old, m_snow_old
-#endif
+#endif    
     real(kind=WP)              , pointer  :: rhoice, rhosno, inv_rhowat
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     u_ice           => ice%uice(:)
     v_ice           => ice%vice(:)
     a_ice           => ice%data(1)%values(:)
@@ -508,7 +459,7 @@ subroutine EVPdynamics_m(ice, partit, mesh)
     rhoice          => ice%thermo%rhoice
     rhosno          => ice%thermo%rhosno
     inv_rhowat      => ice%thermo%inv_rhowat
-
+    
     !___________________________________________________________________________
     val3=1.0_WP/3.0_WP
     vale=1.0_WP/(ice%ellipse**2)
@@ -516,11 +467,11 @@ subroutine EVPdynamics_m(ice, partit, mesh)
     det1=ice%alpha_evp*det2
     rdt=ice%ice_dt
     steps=ice%evp_rheol_steps
-
+    
     !___________________________________________________________________________
     u_ice_aux=u_ice    ! Initialize solver variables
     v_ice_aux=v_ice
-
+    
 #if defined (__icepack)
     a_ice_old(:)  = a_ice(:)
     m_ice_old(:)  = a_ice(:)
@@ -534,143 +485,111 @@ subroutine EVPdynamics_m(ice, partit, mesh)
 
     !NR inlined, to have all initialization in one place.
     !  call ssh2rhs
+  
     ! use rhs_m and rhs_a for storing the contribution from elevation:
-!$OMP PARALLEL DO
-    do row=1, myDim_nod2d
+    do row=1, myDim_nod2d 
         rhs_a(row)=0.0_WP
         rhs_m(row)=0.0_WP
     end do
-!$OMP END PARALLEL DO
+  
     !_____________________________________________________________________________
     ! use floating sea ice for zlevel and zstar
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(el, elnodes, vol, dx, dy, p_ice, n, bb, aa)
     if (use_floatice .and.  .not. trim(which_ale)=='linfs') then
-!$OMP DO
-        do el=1,myDim_elem2d
+        do el=1,myDim_elem2d         
             elnodes=elem2D_nodes(:,el)
-
+            
             !_______________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels(el) > 1) cycle
-
+            
             !_______________________________________________________________________
             vol=elem_area(el)
             dx=gradient_sca(1:3,el)
-            dy=gradient_sca(4:6,el)
-
+            dy=gradient_sca(4:6,el)     
+            
             !_______________________________________________________________________
             ! add pressure gradient from sea ice --> in case of floating sea ice
             p_ice=(rhoice*m_ice(elnodes)+rhosno*m_snow(elnodes))*inv_rhowat
             do n=1,3
                 p_ice(n)=min(p_ice(n),max_ice_loading)
             end do
-
+            
             !_______________________________________________________________________
             bb=g*val3*vol
             aa=bb*sum(dx*(elevation(elnodes)+p_ice))
             bb=bb*sum(dy*(elevation(elnodes)+p_ice))
-            do n=1, 3
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-               call omp_set_lock  (partit%plock(elnodes(n)))
-#else
-!$OMP ORDERED
-#endif
-               rhs_a(elnodes(n))=rhs_a(elnodes(n))-aa
-               rhs_m(elnodes(n))=rhs_m(elnodes(n))-bb
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-               call omp_unset_lock(partit%plock(elnodes(n)))
-#else
-!$OMP END ORDERED
-#endif
-            end do
+            rhs_a(elnodes)=rhs_a(elnodes)-aa    
+            rhs_m(elnodes)=rhs_m(elnodes)-bb
         end do
-!$OMP END DO
     !_____________________________________________________________________________
-    ! use levitating sea ice for linfs, zlevel and zstar
+    ! use levitating sea ice for linfs, zlevel and zstar  
     else
-!$OMP DO
-        do el=1,myDim_elem2d
+        do el=1,myDim_elem2d         
             elnodes=elem2D_nodes(:,el)
             !_______________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels(el) > 1)  cycle
-
+            
             vol=elem_area(el)
             dx=gradient_sca(1:3,el)
             dy=gradient_sca(4:6,el)
             bb=g*val3*vol
             aa=bb*sum(dx*elevation(elnodes))
             bb=bb*sum(dy*elevation(elnodes))
-            do n=1, 3
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-            call omp_set_lock  (partit%plock(elnodes(n)))
-#else
-!$OMP ORDERED
-#endif
-               rhs_a(elnodes(n))=rhs_a(elnodes(n))-aa
-               rhs_m(elnodes(n))=rhs_m(elnodes(n))-bb
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-            call omp_unset_lock(partit%plock(elnodes(n)))
-#else
-!$OMP END ORDERED
-#endif
-            end do
+            rhs_a(elnodes)=rhs_a(elnodes)-aa    
+            rhs_m(elnodes)=rhs_m(elnodes)-bb
         end do
-!$OMP END DO
     end if
-!$OMP END PARALLEL
+
     !___________________________________________________________________________
     ! precompute thickness (the inverse is needed) and mass (scaled by area)
-!$OMP PARALLEL DO
     do i=1,myDim_nod2D
         inv_thickness(i) = 0._WP
         mass(i) = 0._WP
         ice_nod(i) = .false.
         !_______________________________________________________________________
-        ! if cavity ndoe skip it
+        ! if cavity ndoe skip it 
         if ( ulevels_nod2d(i)>1 ) cycle
-
+        
         if (a_ice(i) >= 0.01_WP) then
             inv_thickness(i) = (rhoice*m_ice(i)+rhosno*m_snow(i))/a_ice(i)
             inv_thickness(i) = 1.0_WP/max(inv_thickness(i), 9.0_WP)  ! Limit the mass
-
+            
             mass(i) = (m_ice(i)*rhoice+m_snow(i)*rhosno)
             mass(i) = mass(i)/((1.0_WP+mass(i)*mass(i))*area(1,i))
-
+            
             ! scale rhs_a, rhs_m, too.
-            rhs_a(i) = rhs_a(i)/area(1,i)
-            rhs_m(i) = rhs_m(i)/area(1,i)
-
+            rhs_a(i) = rhs_a(i)/area(1,i) 
+            rhs_m(i) = rhs_m(i)/area(1,i) 
+            
             ice_nod(i) = .true.
         endif
     enddo
-!$OMP END PARALLEL DO
+
     !___________________________________________________________________________
     ! precompute pressure factor
-!$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(el, elnodes, msum, asum)
     do el=1,myDim_elem2D
         elnodes=elem2D_nodes(:,el)
         pressure_fac(el) = 0._WP
         ice_el(el) = .false.
-
+        
         !_______________________________________________________________________
-        ! if element has any cavity node skip it
+        ! if element has any cavity node skip it 
         if (ulevels(el) > 1) cycle
-
+        
         msum=sum(m_ice(elnodes))*val3
         if(msum > 0.01) then
             ice_el(el) = .true.
-            asum=sum(a_ice(elnodes))*val3
+            asum=sum(a_ice(elnodes))*val3          
             pressure_fac(el) = det2*ice%pstar*msum*exp(-ice%c_pressure*(1.0_WP-asum))
         endif
     end do
-!$OMP END PARALLEL DO
-!$OMP PARALLEL DO
-    do row=1, myDim_nod2d
+
+    do row=1, myDim_nod2d 
         u_rhs_ice(row)=0.0_WP
         v_rhs_ice(row)=0.0_WP
     end do
-!$OMP END PARALLEL DO
+
     !___________________________________________________________________________
     ! Ice EVPdynamics Iteration main loop:
 #if defined (__icepack)
@@ -684,36 +603,34 @@ subroutine EVPdynamics_m(ice, partit, mesh)
         ! New implementation following Boullion et al, Ocean Modelling 2013.
         ! SD, 30.07.2014
         !_______________________________________________________________________
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(el, i, ed, row, elnodes, dx, dy, meancos, eps1, eps2, delta, pressure, umod, drag, rhsu, rhsv, det, n)
-!$OMP DO
         do el=1,myDim_elem2D
             if (ulevels(el)>1) cycle
-
+            
             !___________________________________________________________________
             if(ice_el(el)) then
-
+            
                 elnodes=elem2D_nodes(:,el)
                 dx=gradient_sca(1:3,el)
-                dy=gradient_sca(4:6,el)
+                dy=gradient_sca(4:6,el)     
                 ! METRICS:
                 meancos = val3*metric_factor(el)
-                !
+                !  
                 ! ====== Deformation rate tensor on element elem:
                 eps11(el) = sum(dx(:)*u_ice_aux(elnodes)) - sum(v_ice_aux(elnodes))*meancos                !metrics
                 eps22(el) = sum(dy(:)*v_ice_aux(elnodes))
                 eps12(el) = 0.5_WP*(sum(dy(:)*u_ice_aux(elnodes) + dx(:)*v_ice_aux(elnodes)) &
-                                +sum(u_ice_aux(elnodes))*meancos )          !metrics
-
+                                +sum(u_ice_aux(elnodes))*meancos )          !metrics 
+                
                 ! ======= Switch to eps1,eps2
                 eps1 = eps11(el) + eps22(el)
-                eps2 = eps11(el) - eps22(el)
-
+                eps2 = eps11(el) - eps22(el)   
+                
                 ! ====== moduli:
                 delta = sqrt(eps1**2+vale*(eps2**2+4.0_WP*eps12(el)**2))
-
+                
                 pressure = pressure_fac(el)/(delta+ice%delta_min)
-
-                !        si1 = det1*(sigma11(el)+sigma22(el)) + pressure*(eps1-delta)
+                
+                !        si1 = det1*(sigma11(el)+sigma22(el)) + pressure*(eps1-delta) 
                 !        si2 = det1*(sigma11(el)-sigma22(el)) + pressure*eps2*vale
                 !        sigma11(el) = 0.5_WP*(si1+si2)
                 !        sigma22(el) = 0.5_WP*(si1-si2)
@@ -728,7 +645,7 @@ subroutine EVPdynamics_m(ice, partit, mesh)
 #endif
 
                 !  end do   ! fuse loops
-                ! Equations solved in terms of si1, si2, eps1, eps2 are (43)-(45) of
+                ! Equations solved in terms of si1, si2, eps1, eps2 are (43)-(45) of 
                 ! Boullion et al Ocean Modelling 2013, but in an implicit mode:
                 ! si1_{p+1}=det1*si1_p+det2*r1, where det1=alpha/(1+alpha) and det2=1/(1+alpha),
                 ! and similarly for si2 and sigma12
@@ -736,67 +653,36 @@ subroutine EVPdynamics_m(ice, partit, mesh)
                 !NR inlining  call stress2rhs_m
                 ! add internal stress to the rhs
                 ! SD, 30.07.2014
-                !-----------------------------------------------------------------
+                !-----------------------------------------------------------------  
                 if (elnodes(1) <= myDim_nod2D) then
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                    call omp_set_lock  (partit%plock(elnodes(1)))
-#else
-!$OMP ORDERED
-#endif
                     u_rhs_ice(elnodes(1)) = u_rhs_ice(elnodes(1)) - elem_area(el)* &
-                            (sigma11(el)*dx(1)+sigma12(el)*(dy(1) + meancos))                         !metrics
+                            (sigma11(el)*dx(1)+sigma12(el)*(dy(1) + meancos))                         !metrics 
                     v_rhs_ice(elnodes(1)) = v_rhs_ice(elnodes(1)) - elem_area(el)* &
-                            (sigma12(el)*dx(1)+sigma22(el)*dy(1) - sigma11(el)*meancos)               !metrics
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                    call omp_unset_lock(partit%plock(elnodes(1)))
-#else
-!$OMP END ORDERED
-#endif
+                            (sigma12(el)*dx(1)+sigma22(el)*dy(1) - sigma11(el)*meancos)               ! metrics                                              
                 end if
 
                 if (elnodes(2) <= myDim_nod2D) then
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                    call omp_set_lock  (partit%plock(elnodes(2)))
-#else
-!$OMP ORDERED
-#endif
                     u_rhs_ice(elnodes(2)) = u_rhs_ice(elnodes(2)) - elem_area(el)* &
-                            (sigma11(el)*dx(2)+sigma12(el)*(dy(2) + meancos))                         !metrics
+                            (sigma11(el)*dx(2)+sigma12(el)*(dy(2) + meancos))                         !metrics 
                     v_rhs_ice(elnodes(2)) = v_rhs_ice(elnodes(2)) - elem_area(el)* &
-                            (sigma12(el)*dx(2)+sigma22(el)*dy(2) - sigma11(el)*meancos)               !metrics
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                call omp_unset_lock(partit%plock(elnodes(2)))
-#else
-!$OMP END ORDERED
-#endif
+                            (sigma12(el)*dx(2)+sigma22(el)*dy(2) - sigma11(el)*meancos)               ! metrics                                              
                 end if
 
                 if (elnodes(3) <= myDim_nod2D) then
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                    call omp_set_lock  (partit%plock(elnodes(3)))
-#else
-!$OMP ORDERED
-#endif
                     u_rhs_ice(elnodes(3)) = u_rhs_ice(elnodes(3)) - elem_area(el)* &
-                            (sigma11(el)*dx(3)+sigma12(el)*(dy(3) + meancos))                         !metrics
+                            (sigma11(el)*dx(3)+sigma12(el)*(dy(3) + meancos))                         !metrics 
                     v_rhs_ice(elnodes(3)) = v_rhs_ice(elnodes(3)) - elem_area(el)* &
-                            (sigma12(el)*dx(3)+sigma22(el)*dy(3) - sigma11(el)*meancos)               !metrics
-#if defined(_OPENMP) && !defined(__openmp_reproducible)
-                   call omp_unset_lock(partit%plock(elnodes(3)))
-#else
-!$OMP END ORDERED
-#endif
+                            (sigma12(el)*dx(3)+sigma22(el)*dy(3) - sigma11(el)*meancos)               ! metrics                                              
                 end if
             end if
         end do ! --> do el=1,myDim_elem2D
-!$OMP END DO
-!$OMP DO
-        do i=1, myDim_nod2d
+    
+        do i=1, myDim_nod2d 
             !___________________________________________________________________
             if (ulevels_nod2D(i)>1) cycle
-
+            
             !___________________________________________________________________
-            if (ice_nod(i)) then                   ! Skip if ice is absent
+            if (ice_nod(i)) then                   ! Skip if ice is absent              
                 u_rhs_ice(i) = u_rhs_ice(i)*mass(i) + rhs_a(i)
                 v_rhs_ice(i) = v_rhs_ice(i)*mass(i) + rhs_m(i)
                 ! end do   !NR fuse loops
@@ -804,80 +690,54 @@ subroutine EVPdynamics_m(ice, partit, mesh)
                 !    do i=1,myDim_nod2D
                 umod = sqrt((u_ice_aux(i)-u_w(i))**2+(v_ice_aux(i)-v_w(i))**2)
                 drag = rdt*ice%cd_oce_ice*umod*density_0*inv_thickness(i)
-
+                
                 !rhs for water stress, air stress, and u_rhs_ice/v (internal stress + ssh)
                 rhsu = u_ice(i)+drag*u_w(i)+rdt*(inv_thickness(i)*stress_atmice_x(i)+u_rhs_ice(i)) + ice%beta_evp*u_ice_aux(i)
                 rhsv = v_ice(i)+drag*v_w(i)+rdt*(inv_thickness(i)*stress_atmice_y(i)+v_rhs_ice(i)) + ice%beta_evp*v_ice_aux(i)
-
-                !solve (Coriolis and water stress are treated implicitly)
+                
+                !solve (Coriolis and water stress are treated implicitly)        
                 det = bc_index_nod2D(i) / ((1.0_WP+ice%beta_evp+drag)**2 + (rdt*mesh%coriolis_node(i))**2)
-
+                
                 u_ice_aux(i) = det*((1.0_WP+ice%beta_evp+drag)*rhsu +rdt*mesh%coriolis_node(i)*rhsv)
                 v_ice_aux(i) = det*((1.0_WP+ice%beta_evp+drag)*rhsv -rdt*mesh%coriolis_node(i)*rhsu)
             end if
-        end do ! --> do i=1, myDim_nod2d
-!$OMP END DO
+        end do ! --> do i=1, myDim_nod2d 
+
         !_______________________________________________________________________
-        ! apply sea ice velocity boundary condition
-!$OMP DO
+        ! apply sea ice velocity boundary condition 
         do ed=1,myDim_edge2D
             !___________________________________________________________________
             ! apply coastal sea ice velocity boundary conditions
-            if (myList_edge2D(ed) > edge2D_in) then
-               do n=1, 2
-#if defined(_OPENMP)
-                call omp_set_lock  (partit%plock(edges(n, ed)))
-#endif
-                u_ice_aux(edges(n,ed))=0.0_WP
-                v_ice_aux(edges(n,ed))=0.0_WP
-#if defined(_OPENMP)
-                call omp_unset_lock(partit%plock(edges(n,ed)))
-#endif
-               end do
+            if(myList_edge2D(ed) > edge2D_in) then
+                u_ice_aux(edges(:,ed))=0.0_WP
+                v_ice_aux(edges(:,ed))=0.0_WP
             end if
-
+            
             !___________________________________________________________________
             ! apply sea ice velocity boundary conditions at cavity-ocean edge
-            if (use_cavity) then
+            if (use_cavity) then 
                 if ( (ulevels(edge_tri(1,ed))>1) .or. &
-                    ( edge_tri(2,ed)>0 .and. ulevels(edge_tri(2,ed))>1) ) then
-                    do n=1, 2
-#if defined(_OPENMP)
-                       call omp_set_lock  (partit%plock(edges(n, ed)))
-#endif
-                       u_ice_aux(edges(n,ed))=0.0_WP
-                       v_ice_aux(edges(n,ed))=0.0_WP
-#if defined(_OPENMP)
-                       call omp_unset_lock(partit%plock(edges(n,ed)))
-#endif
-                    end do
-                end if
-            end if
+                    ( edge_tri(2,ed)>0 .and. ulevels(edge_tri(2,ed))>1) ) then 
+                    u_ice_aux(edges(1:2,ed))=0.0_WP
+                    v_ice_aux(edges(1:2,ed))=0.0_WP
+                end if 
+            end if 
         end do ! --> do ed=1,myDim_edge2D
-!$OMP END DO
+        
         !_______________________________________________________________________
-!$OMP MASTER
         call exchange_nod_begin(u_ice_aux, v_ice_aux, partit)
-!$OMP END MASTER
-!$OMP BARRIER
-!$OMP DO
-        do row=1, myDim_nod2d
-           u_rhs_ice(row)=0.0_WP
-           v_rhs_ice(row)=0.0_WP
+
+        do row=1, myDim_nod2d 
+            u_rhs_ice(row)=0.0_WP
+            v_rhs_ice(row)=0.0_WP
         end do
-!$OMP END DO
-!$OMP MASTER
+
         call exchange_nod_end(partit)
-!$OMP END MASTER
-!$OMP BARRIER
-!$OMP END PARALLEL
+        
     end do ! --> do shortstep=1, steps
-!$OMP PARALLEL DO
-    do row=1, myDim_nod2d+eDim_nod2D
-       u_ice(row)=u_ice_aux(row)
-       v_ice(row)=v_ice_aux(row)
-    end do
-!$OMP END PARALLEL DO
+    u_ice=u_ice_aux
+    v_ice=v_ice_aux
+  
 end subroutine EVPdynamics_m
 !
 !
@@ -917,7 +777,7 @@ subroutine find_alpha_field_a(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     a_ice           => ice%data(1)%values(:)
     m_ice           => ice%data(2)%values(:)
     eps11           => ice%work%eps11(:)
@@ -930,42 +790,42 @@ subroutine find_alpha_field_a(ice, partit, mesh)
     v_ice_aux       => ice%vice_aux(:)
     alpha_evp_array => ice%alpha_evp_array(:)
     rhoice          => ice%thermo%rhoice
-
+    
     !___________________________________________________________________________
     val3=1.0_WP/3.0_WP
     vale=1.0_WP/(ice%ellipse**2)
     do elem=1,myDim_elem2D
         elnodes=elem2D_nodes(:,elem)
         !_______________________________________________________________________
-        ! if element has any cavity node skip it
+        ! if element has any cavity node skip it 
         if (ulevels(elem) > 1) cycle
-
+            
         msum=sum(m_ice(elnodes))*val3
         if(msum<=0.01_WP) cycle !DS
         asum=sum(a_ice(elnodes))*val3
-
+        
         dx=gradient_sca(1:3,elem)
-        dy=gradient_sca(4:6,elem)
+        dy=gradient_sca(4:6,elem)     
         ! METRICS:
         vsum=sum(v_ice_aux(elnodes))
         usum=sum(u_ice_aux(elnodes))
         meancos=metric_factor(elem)
-        !
+        !  
         ! ====== Deformation rate tensor on element elem:
         eps11(elem)=sum(dx*u_ice_aux(elnodes))
         eps11(elem)=eps11(elem)-val3*vsum*meancos                !metrics
         eps22(elem)=sum(dy*v_ice_aux(elnodes))
         eps12(elem)=0.5_WP*sum(dy*u_ice_aux(elnodes) + dx*v_ice_aux(elnodes))
-        eps12(elem)=eps12(elem)+0.5_WP*val3*usum*meancos          !metrics
-
+        eps12(elem)=eps12(elem)+0.5_WP*val3*usum*meancos          !metrics 
+        
         ! ======= Switch to eps1,eps2
         eps1=eps11(elem)+eps22(elem)
-        eps2=eps11(elem)-eps22(elem)
-
+        eps2=eps11(elem)-eps22(elem)   
+        
         ! ====== moduli:
         delta=eps1**2+vale*(eps2**2+4.0_WP*eps12(elem)**2)
         delta=sqrt(delta)
-
+         
 #if defined (__icepack)
         pressure = sum(strength(elnodes))*val3/(delta+ice%delta_min)/msum
 #else
@@ -977,7 +837,7 @@ subroutine find_alpha_field_a(ice, partit, mesh)
         ! /voltriangle(elem) for FESOM1.4
         ! We do not allow alpha to be too small!
     end do !--> do elem=1,myDim_elem2D
-end subroutine find_alpha_field_a
+end subroutine find_alpha_field_a  
 !
 !
 !_______________________________________________________________________________
@@ -1015,7 +875,7 @@ subroutine stress_tensor_a(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     a_ice           => ice%data(1)%values(:)
     m_ice           => ice%data(2)%values(:)
     eps11           => ice%work%eps11(:)
@@ -1027,54 +887,54 @@ subroutine stress_tensor_a(ice, partit, mesh)
     u_ice_aux       => ice%uice_aux(:)
     v_ice_aux       => ice%vice_aux(:)
     alpha_evp_array => ice%alpha_evp_array(:)
-
+    
     !___________________________________________________________________________
     val3=1.0_WP/3.0_WP
     vale=1.0_WP/(ice%ellipse**2)
     do elem=1,myDim_elem2D
         !_______________________________________________________________________
-        ! if element has any cavity node skip it
+        ! if element has any cavity node skip it 
         if (ulevels(elem) > 1) cycle
-
+        
         !_______________________________________________________________________
         det2=1.0_WP/(1.0_WP+alpha_evp_array(elem))     ! Take alpha from array
         det1=alpha_evp_array(elem)*det2
-
+    
         elnodes=elem2D_nodes(:,elem)
-
+        
         msum=sum(m_ice(elnodes))*val3
         if(msum<=0.01_WP) cycle !DS
         asum=sum(a_ice(elnodes))*val3
-
+        
         dx=gradient_sca(1:3,elem)
-        dy=gradient_sca(4:6,elem)
+        dy=gradient_sca(4:6,elem)     
         ! METRICS:
         vsum=sum(v_ice_aux(elnodes))
         usum=sum(u_ice_aux(elnodes))
         meancos=metric_factor(elem)
-        !
+        !  
         ! ====== Deformation rate tensor on element elem:
         eps11(elem)=sum(dx*u_ice_aux(elnodes))
         eps11(elem)=eps11(elem)-val3*vsum*meancos                !metrics
         eps22(elem)=sum(dy*v_ice_aux(elnodes))
         eps12(elem)=0.5_WP*sum(dy*u_ice_aux(elnodes) + dx*v_ice_aux(elnodes))
-        eps12(elem)=eps12(elem)+0.5_WP*val3*usum*meancos          !metrics
-
+        eps12(elem)=eps12(elem)+0.5_WP*val3*usum*meancos          !metrics 
+        
         ! ======= Switch to eps1,eps2
         eps1=eps11(elem)+eps22(elem)
-        eps2=eps11(elem)-eps22(elem)
-
+        eps2=eps11(elem)-eps22(elem)   
+        
         ! ====== moduli:
         delta=eps1**2+vale*(eps2**2+4.0_WP*eps12(elem)**2)
         delta=sqrt(delta)
-
+   
 #if defined (__icepack)
         pressure = sum(strength(elnodes))*val3/(delta+ice%delta_min)
 #else
         pressure=ice%pstar*msum*exp(-ice%c_pressure*(1.0_WP-asum))/(delta+ice%delta_min)
 #endif
-
-        r1=pressure*(eps1-delta)
+    
+        r1=pressure*(eps1-delta) 
         r2=pressure*eps2*vale
         r3=pressure*eps12(elem)*vale
         si1=sigma11(elem)+sigma22(elem)
@@ -1091,7 +951,7 @@ subroutine stress_tensor_a(ice, partit, mesh)
         rdg_shear_elem(elem) = 0.5_WP*(delta - abs(eps11(elem)+eps22(elem)))
 #endif
     end do ! --> do elem=1,myDim_elem2D
-    ! Equations solved in terms of si1, si2, eps1, eps2 are (43)-(45) of
+    ! Equations solved in terms of si1, si2, eps1, eps2 are (43)-(45) of 
     ! Boullion et al Ocean Modelling 2013, but in an implicit mode:
     ! si1_{p+1}=det1*si1_p+det2*r1, where det1=alpha/(1+alpha) and det2=1/(1+alpha),
     ! and similarly for si2 and sigma12
@@ -1101,7 +961,7 @@ end subroutine stress_tensor_a
 !_______________________________________________________________________________
 ! assemble rhs and solve for ice velocity
 ! New implementation based on Bouillion et al. Ocean Modelling 2013
-! and Kimmritz et al., Ocean Modelling  2016
+! and Kimmritz et al., Ocean Modelling  2016 
 ! SD 14.02.17
 subroutine EVPdynamics_a(ice, partit, mesh)
     USE MOD_ICE
@@ -1139,7 +999,7 @@ subroutine EVPdynamics_a(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     u_ice           => ice%uice(:)
     v_ice           => ice%vice(:)
     a_ice           => ice%data(1)%values(:)
@@ -1156,7 +1016,7 @@ subroutine EVPdynamics_a(ice, partit, mesh)
     beta_evp_array  => ice%beta_evp_array(:)
     rhoice          => ice%thermo%rhoice
     rhosno          => ice%thermo%rhosno
-
+    
     !___________________________________________________________________________
     steps=ice%evp_rheol_steps
     rdt=ice%ice_dt
@@ -1168,27 +1028,27 @@ subroutine EVPdynamics_a(ice, partit, mesh)
     rdg_conv_elem(:)  = 0.0_WP
     rdg_shear_elem(:) = 0.0_WP
 #endif
-
-    do shortstep=1, steps
+ 
+    do shortstep=1, steps 
         call stress_tensor_a(ice, partit, mesh)
         call stress2rhs_m(ice, partit, mesh)    ! _m=_a, so no _m version is the only one!
-        do i=1,myDim_nod2D
-
+        do i=1,myDim_nod2D 
+        
             !___________________________________________________________________
-            ! if element has any cavity node skip it
+            ! if element has any cavity node skip it 
             if (ulevels_nod2d(i)>1) cycle
-
+            
             thickness=(rhoice*m_ice(i)+rhosno*m_snow(i))/max(a_ice(i),0.01_WP)
             thickness=max(thickness, 9.0_WP)   ! Limit if it is too small (0.01 m)
             inv_thickness=1.0_WP/thickness
-
+            
             umod=sqrt((u_ice_aux(i)-u_w(i))**2+(v_ice_aux(i)-v_w(i))**2)
             drag=rdt*ice%cd_oce_ice*umod*density_0*inv_thickness
-
+            
             !rhs for water stress, air stress, and u_rhs_ice/v (internal stress + ssh)
             rhsu=u_ice(i)+drag*u_w(i)+rdt*(inv_thickness*stress_atmice_x(i)+u_rhs_ice(i))
             rhsv=v_ice(i)+drag*v_w(i)+rdt*(inv_thickness*stress_atmice_y(i)+v_rhs_ice(i))
-
+            
             rhsu=beta_evp_array(i)*u_ice_aux(i)+rhsu
             rhsv=beta_evp_array(i)*v_ice_aux(i)+rhsv
             !solve (Coriolis and water stress are treated implicitly)
@@ -1198,9 +1058,9 @@ subroutine EVPdynamics_a(ice, partit, mesh)
             u_ice_aux(i)=det*((1.0_WP+beta_evp_array(i)+drag)*rhsu+fc*rhsv)
             v_ice_aux(i)=det*((1.0_WP+beta_evp_array(i)+drag)*rhsv-fc*rhsu)
         end do
-
+        
         !_______________________________________________________________________
-        ! apply sea ice velocity boundary condition
+        ! apply sea ice velocity boundary condition 
         do ed=1,myDim_edge2D
             !___________________________________________________________________
             ! apply coastal sea ice velocity boundary conditions
@@ -1208,34 +1068,34 @@ subroutine EVPdynamics_a(ice, partit, mesh)
                 u_ice_aux(edges(:,ed))=0.0_WP
                 v_ice_aux(edges(:,ed))=0.0_WP
             end if
-
+            
             !___________________________________________________________________
             ! apply sea ice velocity boundary conditions at cavity-ocean edge
-            if (use_cavity) then
+            if (use_cavity) then 
                 if ( (ulevels(edge_tri(1,ed))>1) .or. &
-                    ( edge_tri(2,ed)>0 .and. ulevels(edge_tri(2,ed))>1) ) then
+                    ( edge_tri(2,ed)>0 .and. ulevels(edge_tri(2,ed))>1) ) then 
                     u_ice_aux(edges(1:2,ed))=0.0_WP
                     v_ice_aux(edges(1:2,ed))=0.0_WP
-                end if
-            end if
+                end if 
+            end if 
         end do ! --> do ed=1,myDim_edge2D
-
+        
         call exchange_nod(u_ice_aux, v_ice_aux, partit)
     end do
-
+        
     u_ice=u_ice_aux
     v_ice=v_ice_aux
-
+    
     call find_alpha_field_a(ice, partit, mesh)             ! alpha_evp_array is initialized with alpha_evp;
-                                        ! At this stage we already have non-trivial velocities.
+                                        ! At this stage we already have non-trivial velocities. 
     call find_beta_field_a(ice, partit, mesh)
 end subroutine EVPdynamics_a
 !
 !
 !_______________________________________________________________________________
-! beta_evp_array is defined at nodes, and this is the only
-! reason we need it in addition to alpha_evp_array (we work with
-! alpha=beta, and keep different names for generality; mEVP can work with
+! beta_evp_array is defined at nodes, and this is the only 
+! reason we need it in addition to alpha_evp_array (we work with 
+! alpha=beta, and keep different names for generality; mEVP can work with 
 ! alpha \ne beta, but not aEVP).
 subroutine find_beta_field_a(ice, partit, mesh)
     USE MOD_PARTIT
@@ -1243,7 +1103,7 @@ subroutine find_beta_field_a(ice, partit, mesh)
     USE MOD_MESH
     USE MOD_ICE
     use o_param
-    Implicit none
+    Implicit none    
     type(t_mesh)  , intent(in)   , target :: mesh
     type(t_partit), intent(inout), target :: partit
     type(t_ice)   , intent(inout), target :: ice
@@ -1255,16 +1115,16 @@ subroutine find_beta_field_a(ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     alpha_evp_array => ice%alpha_evp_array(:)
     beta_evp_array  => ice%beta_evp_array(:)
-
+  
     !___________________________________________________________________________
     DO n=1, myDim_nod2D
        !________________________________________________________________________
-       ! if element has any cavity node skip it
+       ! if element has any cavity node skip it 
        if (ulevels_nod2d(n)>1) cycle
-
+       
        ! ==============
        ! FESOM1.4 and stand-alone FESIM
        ! beta_evp_array(n) =  maxval(alpha_evp_array(nod_in_elem2D(n)%addresses(1:nod_in_elem2D(n)%nmb)))
@@ -1273,6 +1133,6 @@ subroutine find_beta_field_a(ice, partit, mesh)
        beta_evp_array(n) =  maxval(alpha_evp_array(nod_in_elem2D(1:nod_in_elem2D_num(n),n)))
     END DO
 end subroutine find_beta_field_a
-!
+! 
 ! ================================================================
 !
diff --git a/src/ice_oce_coupling.F90 b/src/ice_oce_coupling.F90
index 08f29f1a..2274f95f 100755
--- a/src/ice_oce_coupling.F90
+++ b/src/ice_oce_coupling.F90
@@ -259,6 +259,7 @@ subroutine oce_fluxes(ice, dynamics, tracers, partit, mesh)
     use icedrv_main,   only: icepack_to_fesom,    &
                             init_flux_atm_ocn
 #endif
+    use iceberg_params
     use cavity_interfaces
     !---fwf-code
     use g_clock
@@ -499,7 +500,7 @@ subroutine oce_fluxes(ice, dynamics, tracers, partit, mesh)
         relax_salt(n)=relax_salt(n)-net
     end do
 !$OMP END PARALLEL DO
-
+    
     !___________________________________________________________________________
     ! enforce the total freshwater/salt flux be zero
     ! 1. water flux ! if (.not. use_virt_salt) can be used!
@@ -562,9 +563,22 @@ subroutine oce_fluxes(ice, dynamics, tracers, partit, mesh)
             where (ulevels_nod2d > 1) flux = -water_flux
         end if 
     end if 
+    
+    !___________________________________________________________________________
+    if (use_icebergs .and. lbalance_fw) then
+        call icb2fesom(mesh, partit, ice)
+        if (.not.turn_off_fw) then
+            flux = flux + (ibfwb + ibfwe + ibfwl + ibfwbv) !* steps_per_ib_step
+        end if
+        
+        call integrate_nod(ibfwb + ibfwe + ibfwl + ibfwbv, net, partit, mesh)
+        if (mype==0) write(*,*) " * total iceberg fw flux: ", net
+    end if
+
     !___________________________________________________________________________
     ! compute total global net freshwater flux into the ocean 
     call integrate_nod(flux, net, partit, mesh)
+
     
     !___________________________________________________________________________
     ! here the + sign must be used because we switched up the sign of the 
@@ -591,7 +605,7 @@ subroutine oce_fluxes(ice, dynamics, tracers, partit, mesh)
         end do
 !$OMP END PARALLEL DO
     end if 
-    
+
 !---fwf-code-begin
     if(use_landice_water) then
 !$OMP PARALLEL DO
@@ -601,15 +615,15 @@ subroutine oce_fluxes(ice, dynamics, tracers, partit, mesh)
 !$OMP END PARALLEL DO
     end if
  
-!    if(lwiso .and. use_landice_water) then
-!!$OMP PARALLEL DO
-!      do n=1, myDim_nod2D+eDim_nod2D
-!         wiso_flux_oce(n,1)=wiso_flux_oce(n,1)+runoff_landice(n)*1000.0*wiso_smow(1)*(1-30.0/1000.0)*landice_season(month)
-!         wiso_flux_oce(n,2)=wiso_flux_oce(n,2)+runoff_landice(n)*1000.0*wiso_smow(2)*(1-240.0/1000.0)*landice_season(month)
-!         wiso_flux_oce(n,3)=wiso_flux_oce(n,3)+runoff_landice(n)*1000.0*landice_season(month)
-!      end do
-!!$OMP END PARALLEL DO
-!    end if
+    if(lwiso .and. use_landice_water) then
+!$OMP PARALLEL DO
+      do n=1, myDim_nod2D+eDim_nod2D
+         wiso_flux_oce(n,1)=wiso_flux_oce(n,1)+runoff_landice(n)*1000.0*wiso_smow(1)*(1-30.0/1000.0)*landice_season(month)
+         wiso_flux_oce(n,2)=wiso_flux_oce(n,2)+runoff_landice(n)*1000.0*wiso_smow(2)*(1-240.0/1000.0)*landice_season(month)
+         wiso_flux_oce(n,3)=wiso_flux_oce(n,3)+runoff_landice(n)*1000.0*landice_season(month)
+      end do
+!$OMP END PARALLEL DO
+    end if
 !---fwf-code-end
 
     !___________________________________________________________________________
@@ -757,22 +771,6 @@ subroutine oce_fluxes(ice, dynamics, tracers, partit, mesh)
 
     !---wiso-code-end
 
-    !---fwf-code-begin
-    if(use_landice_water) then
-      do n=1, myDim_nod2D+eDim_nod2D
-         water_flux(n)=water_flux(n)-runoff_landice(n)*landice_season(month)
-      end do
-    end if
-
-    if(lwiso .and. use_landice_water) then
-      do n=1, myDim_nod2D+eDim_nod2D
-      wiso_flux_oce(n,1)=wiso_flux_oce(n,1)+runoff_landice(n)*1000.0*wiso_smow(1)*(1-30.0/1000.0)*landice_season(month)
-      wiso_flux_oce(n,2)=wiso_flux_oce(n,2)+runoff_landice(n)*1000.0*wiso_smow(2)*(1-240.0/1000.0)*landice_season(month)
-      wiso_flux_oce(n,3)=wiso_flux_oce(n,3)+runoff_landice(n)*1000.0*landice_season(month)
-      end do
-    end if
-    !---fwf-code-end
-
     !---age-code-begin
     if (use_age_tracer) then
        tracers%data(index_age_tracer)%values(:,:) = tracers%data(index_age_tracer)%values(:,:) + dt/(86400.0*(365+fleapyear))
diff --git a/src/ice_setup_step.F90 b/src/ice_setup_step.F90
index 442eab36..d65c7c8a 100755
--- a/src/ice_setup_step.F90
+++ b/src/ice_setup_step.F90
@@ -58,35 +58,35 @@ subroutine ice_setup(ice, tracers, partit, mesh)
     use g_CONFIG
     use ice_initial_state_interface
     use ice_fct_interfaces
-    implicit none
+    implicit none 
     type(t_ice)   , intent(inout), target :: ice
     type(t_tracer), intent(in)   , target :: tracers
     type(t_mesh)  , intent(in)   , target :: mesh
     type(t_partit), intent(inout), target :: partit
-
+    
     !___________________________________________________________________________
-    ! initialise ice derived type
+    ! initialise ice derived type 
     if (flag_debug .and. partit%mype==0)  print *, achar(27)//'[36m'//'     --> call ice_init'//achar(27)//'[0m'
     call ice_init(ice, partit, mesh)
-
+    
     !___________________________________________________________________________
     ! DO not change
     ice%ice_dt   = real(ice%ice_ave_steps,WP)*dt
     ! ice_dt=dt
-    ice%Tevp_inv = 3.0_WP/ice%ice_dt
+    ice%Tevp_inv = 3.0_WP/ice%ice_dt 
     ! This is combination it always enters
-    ice%Clim_evp = ice%Clim_evp*(ice%evp_rheol_steps/ice%ice_dt)**2/ice%Tevp_inv
-
+    ice%Clim_evp = ice%Clim_evp*(ice%evp_rheol_steps/ice%ice_dt)**2/ice%Tevp_inv  
+    
     !___________________________________________________________________________
     if (flag_debug .and. partit%mype==0)  print *, achar(27)//'[36m'//'     --> call ice_fct_init'//achar(27)//'[0m'
     call ice_mass_matrix_fill(ice, partit, mesh)
-
+    
     !___________________________________________________________________________
-    ! Initialization routine, user input is required
+    ! Initialization routine, user input is required 
     !call ice_init_fields_test
     if (flag_debug .and. partit%mype==0)  print *, achar(27)//'[36m'//'     --> call ice_initial_state'//achar(27)//'[0m'
     call ice_initial_state(ice, tracers, partit, mesh)   ! Use it unless running test example
-
+    
     if(partit%mype==0) write(*,*) 'Ice is initialized'
 end subroutine ice_setup
 !
@@ -106,9 +106,9 @@ subroutine ice_timestep(step, ice, partit, mesh)
     use ice_thermodynamics_interfaces
     use cavity_interfaces
 #if defined (__icepack)
-    use icedrv_main,   only: step_icepack
+    use icedrv_main,   only: step_icepack 
 #endif
-    implicit none
+    implicit none 
     integer       , intent(in)            :: step
     type(t_ice)   , intent(inout), target :: ice
     type(t_partit), intent(inout), target :: partit
@@ -128,7 +128,7 @@ subroutine ice_timestep(step, ice, partit, mesh)
     real(kind=WP), dimension(:), pointer  :: ice_temp, a_ice
     !LA 2023-03-08
     real(kind=WP), dimension(:), pointer  :: a_ice_ib
-#endif
+#endif     
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -174,57 +174,34 @@ subroutine ice_timestep(step, ice, partit, mesh)
 #if defined (__oifs) || defined (__ifsinterface)
     a_ice    => ice%data(1)%values(:)
     ice_temp => ice%data(4)%values(:)
-#endif
+#endif     
     !___________________________________________________________________________
     t0=MPI_Wtime()
 #if defined (__icepack)
-    call step_icepack(ice, mesh, time_evp, time_advec, time_therm) ! EVP, advection and thermodynamic parts
-#else
-
-    !$ACC UPDATE DEVICE (ice%work%fct_massmatrix) &
-    !$ACC DEVICE (ice%delta_min, ice%Tevp_inv, ice%cd_oce_ice) &
-    !$ACC DEVICE (ice%work%fct_tmax, ice%work%fct_tmin) &
-    !$ACC DEVICE (ice%work%fct_fluxes, ice%work%fct_plus, ice%work%fct_minus) &
-    !$ACC DEVICE (ice%work%eps11, ice%work%eps12, ice%work%eps22) &
-    !$ACC DEVICE (ice%work%sigma11, ice%work%sigma12, ice%work%sigma22) &
-    !$ACC DEVICE (ice%work%ice_strength, ice%stress_atmice_x, ice%stress_atmice_y) &
-    !$ACC DEVICE (ice%thermo%rhosno, ice%thermo%rhoice, ice%thermo%inv_rhowat) &
-    !$ACC DEVICE (ice%srfoce_ssh, ice%pstar, ice%c_pressure) &
-    !$ACC DEVICE (ice%work%inv_areamass, ice%work%inv_mass, ice%uice_rhs, ice%vice_rhs) &
-    !$ACC DEVICE (ice%uice, ice%vice, ice%srfoce_u, ice%srfoce_v, ice%uice_old, ice%vice_old) &
-    !$ACC DEVICE (ice%data(1)%values, ice%data(2)%values, ice%data(3)%values) &
-    !$ACC DEVICE (ice%data(1)%valuesl, ice%data(2)%valuesl, ice%data(3)%valuesl) &
-    !$ACC DEVICE (ice%data(1)%dvalues, ice%data(2)%dvalues, ice%data(3)%dvalues) &
-    !$ACC DEVICE (ice%data(1)%values_rhs, ice%data(2)%values_rhs, ice%data(3)%values_rhs) &
-    !$ACC DEVICE (ice%data(1)%values_div_rhs, ice%data(2)%values_div_rhs, ice%data(3)%values_div_rhs)
-#if defined (__oifs) || defined (__ifsinterface)
-    !$ACC UPDATE DEVICE (ice%data(4)%values, ice%data(4)%valuesl, ice%data(4)%dvalues, ice%data(4)%values_rhs, ice%data(4)%values_div_rhs)
-#endif
+    call step_icepack(ice, mesh, time_evp, time_advec, time_therm) ! EVP, advection and thermodynamic parts    
+#else     
+    
     !___________________________________________________________________________
     ! ===== Dynamics
-
     SELECT CASE (ice%whichEVP)
     CASE (0)
-        if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call EVPdynamics...'//achar(27)//'[0m'
-#if defined(_CRAYFTN)
-	!dir$ noinline
-#endif
+        if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call EVPdynamics...'//achar(27)//'[0m'  
         call EVPdynamics  (ice, partit, mesh)
     CASE (1)
-        if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call EVPdynamics_m...'//achar(27)//'[0m'
+        if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call EVPdynamics_m...'//achar(27)//'[0m'  
         call EVPdynamics_m(ice, partit, mesh)
     CASE (2)
-        if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call EVPdynamics_a...'//achar(27)//'[0m'
+        if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call EVPdynamics_a...'//achar(27)//'[0m'  
         call EVPdynamics_a(ice, partit, mesh)
     CASE DEFAULT
         if (mype==0) write(*,*) 'a non existing EVP scheme specified!'
         call par_ex(partit%MPI_COMM_FESOM, partit%mype)
         stop
     END SELECT
-
+    
     if (use_cavity) call cavity_ice_clean_vel(ice, partit, mesh)
-    t1=MPI_Wtime()
-
+    t1=MPI_Wtime()   
+    
     !___________________________________________________________________________
     ! ===== Advection part
     ! old FCT routines
@@ -233,49 +210,21 @@ subroutine ice_timestep(step, ice, partit, mesh)
     ! call cut_off
     ! new FCT routines from Sergey Danilov 08.05.2018
 #if defined (__oifs) || defined (__ifsinterface)
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-!$ACC parallel loop present(ice_temp, a_ice)
-#endif
     do i=1,myDim_nod2D+eDim_nod2D
         ice_temp(i) = ice_temp(i)*a_ice(i)
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-!$ACC END parallel loop
-#endif
 #endif /* (__oifs) */
     if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call ice_TG_rhs_div...'//achar(27)//'[0m'
-    call ice_TG_rhs_div    (ice, partit, mesh)
-
-    if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call ice_fct_solve...'//achar(27)//'[0m'
+    call ice_TG_rhs_div    (ice, partit, mesh)  
+    
+    if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call ice_fct_solve...'//achar(27)//'[0m' 
     call ice_fct_solve     (ice, partit, mesh)
-
+    
     if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call ice_update_for_div...'//achar(27)//'[0m'
     call ice_update_for_div(ice, partit, mesh)
-
-    !$ACC UPDATE HOST (ice%work%fct_massmatrix) &
-    !$ACC HOST (ice%delta_min, ice%Tevp_inv, ice%cd_oce_ice) &
-    !$ACC HOST (ice%work%fct_tmax, ice%work%fct_tmin) &
-    !$ACC HOST (ice%work%fct_fluxes, ice%work%fct_plus, ice%work%fct_minus) &
-    !$ACC HOST (ice%work%eps11, ice%work%eps12, ice%work%eps22) &
-    !$ACC HOST (ice%work%sigma11, ice%work%sigma12, ice%work%sigma22) &
-    !$ACC HOST (ice%work%ice_strength, ice%stress_atmice_x, ice%stress_atmice_y) &
-    !$ACC HOST (ice%thermo%rhosno, ice%thermo%rhoice, ice%thermo%inv_rhowat) &
-    !$ACC HOST (ice%srfoce_ssh, ice%pstar, ice%c_pressure) &
-    !$ACC HOST (ice%work%inv_areamass, ice%work%inv_mass, ice%uice_rhs, ice%vice_rhs) &
-    !$ACC HOST (ice%uice, ice%vice, ice%srfoce_u, ice%srfoce_v, ice%uice_old, ice%vice_old) &
-    !$ACC HOST (ice%data(1)%values, ice%data(2)%values, ice%data(3)%values) &
-    !$ACC HOST (ice%data(1)%valuesl, ice%data(2)%valuesl, ice%data(3)%valuesl) &
-    !$ACC HOST (ice%data(1)%dvalues, ice%data(2)%dvalues, ice%data(3)%dvalues) &
-    !$ACC HOST (ice%data(1)%values_rhs, ice%data(2)%values_rhs, ice%data(3)%values_rhs) &
-    !$ACC HOST (ice%data(1)%values_div_rhs, ice%data(2)%values_div_rhs, ice%data(3)%values_div_rhs)
-#if defined (__oifs) || defined (__ifsinterface)
-    !$ACC UPDATE HOST (ice%data(4)%values, ice%data(4)%valuesl, ice%data(4)%dvalues, ice%data(4)%values_rhs, ice%data(4)%values_div_rhs)
-#endif
-
+    
 #if defined (__oifs) || defined (__ifsinterface)
 !$OMP PARALLEL DO
     do i=1,myDim_nod2D+eDim_nod2D
@@ -286,10 +235,10 @@ subroutine ice_timestep(step, ice, partit, mesh)
 
     if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call cut_off...'//achar(27)//'[0m'
     call cut_off(ice, partit, mesh)
-
+    
     if (use_cavity) call cavity_ice_clean_ma(ice, partit, mesh)
     t2=MPI_Wtime()
-
+    
     !___________________________________________________________________________
     ! ===== Thermodynamic part
     if (flag_debug .and. mype==0)  print *, achar(27)//'[36m'//'     --> call thermodynamics...'//achar(27)//'[0m'
@@ -305,13 +254,13 @@ subroutine ice_timestep(step, ice, partit, mesh)
             write(*,*) " U_ice(n) = ", U_ice(i)
             write(*,*) " V_ice(n) = ", V_ice(i)
             write(*,*)
-        end if
+        end if 
     end do
 !$OMP END PARALLEL DO
     t3=MPI_Wtime()
     rtime_ice = rtime_ice + (t3-t0)
     rtime_tot = rtime_tot + (t3-t0)
-    if(mod(step,logfile_outfreq)==0 .and. mype==0) then
+    if(mod(step,logfile_outfreq)==0 .and. mype==0) then 
         write(*,*) '___ICE STEP EXECUTION TIMES____________________________'
 #if defined (__icepack)
         write(*,"(A, ES10.3)") '	Ice Dyn.        :', time_evp
@@ -337,8 +286,8 @@ subroutine ice_initial_state(ice, tracers, partit, mesh)
     USE MOD_PARTIT
     USE MOD_PARSUP
     USE MOD_MESH
-    use o_PARAM
-    use o_arrays
+    use o_PARAM   
+    use o_arrays        
     use g_CONFIG
     USE g_read_other_NetCDF, only: read_other_NetCDF
     implicit none
@@ -352,7 +301,7 @@ subroutine ice_initial_state(ice, tracers, partit, mesh)
     real(kind=WP), external               :: TFrez  ! Sea water freeze temperature.
 !============== namelistatmdata variables ================
    integer, save                                :: nm_ic_unit     = 107 ! unit to open namelist file
-   integer                                      :: iost                 !I/O status
+   integer                                      :: iost                 !I/O status 
    integer, parameter                           :: ic_max=10
    logical                                      :: ic_cyclic=.true.
    integer,             save                    :: n_ic2d
@@ -384,7 +333,6 @@ if (.not.use_icebergs) then
     v_ice        => ice%vice(:)
     a_ice        => ice%data(1)%values(:)
     m_ice        => ice%data(2)%values(:)
-    m_snow       => ice%data(3)%values(:)
     !___________________________________________________________________________
     m_ice =0._WP
     a_ice =0._WP
@@ -469,22 +417,22 @@ end if
     if (.not. ini_ice_from_file) then
     if(mype==0) write(*,*) 'initialize the sea ice: cold start'
     !___________________________________________________________________________
-    do i=1,myDim_nod2D+eDim_nod2D
+    do i=1,myDim_nod2D+eDim_nod2D        
         !_______________________________________________________________________
         if (ulevels_nod2d(i)>1) then
             !!PS m_ice(i)  = 1.0e15_WP
             !!PS m_snow(i) = 0.1e15_WP
             cycle ! --> if cavity, no sea ice, no initial state
-        endif
-
+        endif    
+        
         !_______________________________________________________________________
         if (tracers%data(1)%values(1,i)< 0.0_WP) then
             if (geo_coord_nod2D(2,i)>0._WP) then
                 m_ice(i) = 1.0_WP
-                m_snow(i)= 0.1_WP
+                m_snow(i)= 0.1_WP 
             else
                 m_ice(i) = 2.0_WP
-                m_snow(i)= 0.5_WP
+                m_snow(i)= 0.5_WP 
             end if
             a_ice(i) = 0.9_WP
             u_ice(i) = 0.0_WP
@@ -493,7 +441,7 @@ end if
     enddo
     else
     if (mype==0) write(*,*) 'initialize the sea ice: from file'
-       DO i=1, n_ic2d
+       DO i=1, n_ic2d 
           DO current_tracer=1, ice%num_itracers
              IF (ice%data(current_tracer)%ID==idlist(i)) then
                 IF (mype==0) then
diff --git a/src/ice_thermo_cpl.F90 b/src/ice_thermo_cpl.F90
index d00c4c50..a55e8d05 100644
--- a/src/ice_thermo_cpl.F90
+++ b/src/ice_thermo_cpl.F90
@@ -484,6 +484,7 @@ contains
     !runo = runo *rhofwt/rhowat
     !subli= subli*rhofwt/rhowat
     !resid= resid*rhofwt/rhowat
+    
     return
   end subroutine ice_growth
 
diff --git a/src/io_blowup.F90 b/src/io_blowup.F90
index 87e570c3..60832dd3 100644
--- a/src/io_blowup.F90
+++ b/src/io_blowup.F90
@@ -115,8 +115,8 @@ MODULE io_BLOWUP
 		call def_variable(bid, 'v'			, (/nl-1, elem2D/)	, 'meridional velocity', 'm/s', dynamics%uv(2,:,:));
 		call def_variable(bid, 'u_rhs'			, (/nl-1, elem2D/)	, 'zonal velocity', 'm/s', dynamics%uv_rhs(1,:,:));
 		call def_variable(bid, 'v_rhs'			, (/nl-1, elem2D/)	, 'meridional velocity', 'm/s', dynamics%uv_rhs(2,:,:));
-		call def_variable(bid, 'urhs_AB'	, (/nl-1, elem2D/)	, 'Adams-Bashforth for u', 'm/s', dynamics%uv_rhsAB(1,1,:,:));
-		call def_variable(bid, 'vrhs_AB'	, (/nl-1, elem2D/)	, 'Adams-Bashforth for v', 'm/s', dynamics%uv_rhsAB(1,2,:,:));
+		call def_variable(bid, 'urhs_AB'	, (/nl-1, elem2D/)	, 'Adams–Bashforth for u', 'm/s', dynamics%uv_rhsAB(1,:,:));
+		call def_variable(bid, 'vrhs_AB'	, (/nl-1, elem2D/)	, 'Adams–Bashforth for v', 'm/s', dynamics%uv_rhsAB(2,:,:));
 		call def_variable(bid, 'zbar_n_bot' , (/nod2D/)			, 'node bottom depth', 'm', zbar_n_bot);
 		call def_variable(bid, 'zbar_e_bot' , (/elem2d/)		, 'elem bottom depth', 'm', zbar_e_bot);
 		call def_variable(bid, 'bottom_node_thickness' , (/nod2D/)			, 'node bottom thickness', 'm', bottom_node_thickness);
diff --git a/src/io_meandata.F90 b/src/io_meandata.F90
index f61c2077..4b1aaec3 100644
--- a/src/io_meandata.F90
+++ b/src/io_meandata.F90
@@ -1,7 +1,6 @@
 module io_MEANDATA
   USE MOD_PARTIT
   USE MOD_PARSUP
-  USE g_clock
   use o_PARAM, only : WP
   use, intrinsic :: iso_fortran_env, only: real64, real32
   use io_data_strategy_module
@@ -21,15 +20,12 @@ module io_MEANDATA
     type(t_partit), pointer                            :: p_partit
     integer                                            :: ndim
     integer                                            :: glsize(2)
-    integer                                            :: shrinked_size
-    integer, allocatable, dimension(:)                 :: shrinked_indx
     integer                                            :: accuracy
     real(real64), allocatable, dimension(:,:) :: local_values_r8
     real(real32), allocatable, dimension(:,:) :: local_values_r4
     real(real64), allocatable :: aux_r8(:)
     real(real32), allocatable :: aux_r4(:)
-    integer                                            :: addcounter =0
-    integer                                            :: lastcounter=0 ! before addcounter is set to 0
+    integer                                            :: addcounter=0
     real(kind=WP), pointer                             :: ptr3(:,:) ! todo: use netcdf types, not WP
     character(500)                                     :: filename
     character(100)                                     :: name
@@ -54,13 +50,9 @@ module io_MEANDATA
     real(real32), allocatable, dimension(:,:) :: local_values_r4_copy
     real(kind=WP) :: ctime_copy
     integer :: mype_workaround
-    ! to be passed to MULTIO (time window for the accumulations)
-    integer :: currentDate,  currentTime
-    integer :: previousDate, previousTime
-    integer :: startDate, startTime
   contains
     final destructor
-  end type
+  end type  
 !
 !--------------------------------------------------------------------------------------------
 !
@@ -74,7 +66,6 @@ module io_MEANDATA
   logical, save                  :: vec_autorotate=.FALSE.
   logical, save                  :: lnextGEMS=.FALSE.
   integer, save                  :: nlev_upper=1
-  character(len=1), save         :: filesplit_freq='y'
   type io_entry
         CHARACTER(len=15)        :: id        ='unknown   '
         INTEGER                  :: freq      =0
@@ -92,8 +83,7 @@ module io_MEANDATA
   END INTERFACE
 !
 !--------------------------------------------------------------------------------------------
-  REAL(real64), DIMENSION(:), ALLOCATABLE, TARGET :: multio_temporary_array
-
+!
   contains
 !
 !--------------------------------------------------------------------------------------------
@@ -137,7 +127,6 @@ subroutine ini_mean_io(ice, dynamics, tracers, partit, mesh)
     use g_config,        only: use_cavity
     use g_forcing_param, only: use_virt_salt, use_landice_water, use_age_tracer !---fwf-code, age-code
     use g_config, only : lwiso !---wiso-code
-    use mod_transit, only : index_transit 
     implicit none
     integer                   :: i, j
     integer, save             :: nm_io_unit  = 103       ! unit to open namelist file, skip 100-102 for cray
@@ -150,7 +139,7 @@ subroutine ini_mean_io(ice, dynamics, tracers, partit, mesh)
     type(t_tracer), intent(in)   , target :: tracers
     type(t_dyn)   , intent(in)   , target :: dynamics
     type(t_ice)   , intent(in)   , target :: ice
-    namelist /nml_general / io_listsize, vec_autorotate, lnextGEMS, nlev_upper, filesplit_freq
+    namelist /nml_general / io_listsize, vec_autorotate, lnextGEMS, nlev_upper
     namelist /nml_list    / io_list
 
 #include "associate_part_def.h"
@@ -440,14 +429,13 @@ CASE ('kpp_sbuoyflx')
     end if
 CASE ('tx_sur    ')
     sel_forcvar(11) = 1
-    call def_stream(elem2D, myDim_elem2D,  'tx_sur',    'zonal wind str. to ocean',       'N/m2',   stress_surf(1, :),         io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
+    call def_stream(elem2D, myDim_elem2D,  'tx_sur',    'zonal wind str. to ocean',       'm/s2',   stress_surf(1, :),         io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
 CASE ('ty_sur    ')
     sel_forcvar(12) = 1
-    call def_stream(elem2D, myDim_elem2D,  'ty_sur',    'meridional wind str. to ocean',  'N/m2',   stress_surf(2, :),         io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
+    call def_stream(elem2D, myDim_elem2D,  'ty_sur',    'meridional wind str. to ocean',  'm/s2',   stress_surf(2, :),         io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
 CASE ('curl_surf ')
     if (lcurt_stress_surf) then
-    call def_stream(nod2D, myDim_nod2D,    'curl_surf', 'vorticity of the surface stress', 'none',   curl_stress_surf(:),       io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-    
+    call def_stream(nod2D, myDim_nod2D,    'curl_surf', 'vorticity of the surface stress','none',   curl_stress_surf(:),       io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
     end if
     
 !___________________________________________________________________________________________________________________________________    
@@ -457,9 +445,6 @@ CASE ('temp      ')
     call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'temp',      'temperature', 'C',      tracers%data(1)%values(:,:),             io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
 CASE ('salt      ')
     call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'salt',      'salinity',    'psu',    tracers%data(2)%values(:,:),             io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-CASE ('sigma0      ')
-    call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'sigma0',      'potential density',    'kg/m3',    density_m_rho0(:,:),             io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-
 !---wiso-code
 !___________________________________________________________________________________________________________________________________
 ! output water isotopes in ocean water
@@ -476,17 +461,11 @@ CASE ('h2o16     ')
     call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'h2o16', 'h2o16 concentration',    'kmol/m**3',    tracers%data(index_wiso_tracers(3))%values(:,:), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
     end if
 !---wiso-code-end
-
-! Transient tracers
-CASE('SF6        ')
-    if (use_transit)  call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'sf6', 'sulfur hexafluoride', 'mol / m**3', tracers%data(index_transit(1))%values(:,:),  io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-CASE('CFC-12     ')
-    if (use_transit)  call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'cfc12', 'chlorofluorocarbon CFC-12', 'mol / m**3', tracers%data(index_transit(2))%values(:,:),  io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-CASE('R14C       ')
-    if (use_transit)  call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'r14c', '14C / C ratio of DIC', 'none', tracers%data(index_transit(3))%values(:,:),  io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-CASE('R39Ar       ')
-    if (use_transit)  call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'r39ar', '39Ar / Ar ratio', 'none', tracers%data(index_transit(4))%values(:,:),  io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-! Transient tracers end
+CASE ('otracers  ')
+    do j=3, tracers%num_tracers
+    write (id_string, "(I3.3)") tracers%data(j)%ID
+    call def_stream((/nl-1, nod2D/),  (/nl-1, myDim_nod2D/),  'tra_'//id_string, 'pasive tracer ID='//id_string, 'n/a', tracers%data(j)%values(:,:), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
+    end do
 CASE ('slope_x   ')
     call def_stream((/nl-1,  nod2D/), (/nl-1, myDim_nod2D/),  'slope_x',   'neutral slope X',    'none', slope_tapered(1,:,:), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
 CASE ('slope_y   ')
@@ -572,7 +551,6 @@ CASE ('dMOC      ')
        call def_stream((/std_dens_N, elem2D/),  (/std_dens_N, myDim_elem2D/), 'std_frwt_flux',  'FW bouyancy flux      ', 'kg*m/s' ,std_dens_flux(3,:,:),   1, 'y', i_real4, partit, mesh)
        call def_stream((/std_dens_N, elem2D/),  (/std_dens_N, myDim_elem2D/), 'std_dens_dVdT',  'dV/dT',                  'm3/s'   ,std_dens_dVdT(:,:),     1, 'y', i_real4, partit, mesh)
        call def_stream((/std_dens_N, nod2D /),  (/std_dens_N,  myDim_nod2D/), 'std_dens_DIV',   'm3/s',                   'm3/s'   ,std_dens_DIV(:,:),      1, 'y', i_real4, partit, mesh)
-       if (Fer_GM) call def_stream((/std_dens_N, nod2D /),  (/std_dens_N,  myDim_nod2D/), 'std_dens_DIVbolus',   'm3/s',                   'm3/s'   ,std_dens_DIV_fer(:,:),  1, 'y', i_real4, partit, mesh)
        call def_stream((/std_dens_N, elem2D/),  (/std_dens_N, myDim_elem2D/), 'std_dens_Z',     'm',                      'm'      ,std_dens_Z(:,:),        1, 'y', i_real4, partit, mesh)
        call def_stream((/std_dens_N, elem2D/),  (/std_dens_N, myDim_elem2D/), 'std_dens_H'    , 'density thickness'     , 'm'     , std_dens_H(:,:),        1, 'y', i_real4, partit, mesh)
        call def_stream((/nl-1,       nod2D /),  (/nl-1,       myDim_nod2D /), 'density_dMOC',   'density'               , 'm',      density_dmoc(:,:),      1, 'y', i_real4, partit, mesh)
@@ -618,9 +596,11 @@ CASE ('icb       ')
     call def_stream(nod2D, myDim_nod2D, 'ibfwbv',  'basal iceberg melting',            'm/s',    ibfwbv(:),        1, 'm', i_real4, partit, mesh)
     call def_stream(nod2D, myDim_nod2D, 'ibfwl',   'lateral iceberg melting',          'm/s',    ibfwl(:),         1, 'm', i_real4, partit, mesh)
     call def_stream(nod2D, myDim_nod2D, 'ibfwe',   'iceberg erosion',                  'm/s',    ibfwe(:),         1, 'm', i_real4, partit, mesh)
-    call def_stream(nod2D, myDim_nod2D, 'ibhf',    'heat flux from iceberg melting',   'm/s',    ibhf(:),          1, 'm', i_real4, partit, mesh)
+    !call def_stream(nod2D, myDim_nod2D, 'ibhf',    'heat flux from iceberg melting',   'm/s',    ibhf(:),          1, 'm', i_real4, partit, mesh)
+    call def_stream((/nl,nod2D/), (/nl,myDim_nod2D/), 'ibhf',    'heat flux from iceberg melting',   'm/s',    ibhf_n(:,:),      1, 'm', i_real4, partit, mesh)
   end if
 !------------------------------------------
+
 !_______________________________________________________________________________
 ! TKE mixing diagnostic 
 CASE ('TKE       ')
@@ -682,8 +662,8 @@ CASE ('FORC      ')
         if (sel_forcvar( 8)==0) call def_stream(nod2D , myDim_nod2D , 'swr'   , 'short wave radiation'           , 'W/m^2', shortwave(:)     , io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
         if (sel_forcvar( 9)==0) call def_stream(nod2D , myDim_nod2D , 'lwr'   , 'long wave radiation'            , 'W/m^2', longwave(:)      , io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
         if (sel_forcvar(10)==0) call def_stream(nod2D , myDim_nod2D , 'runoff', 'river runoff'                   , 'none' , runoff(:)        , io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-        if (sel_forcvar(11)==0) call def_stream(elem2D, myDim_elem2D, 'tx_sur', 'zonal wind str. to ocean'       , 'N/m^2', stress_surf(1, :), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
-        if (sel_forcvar(12)==0) call def_stream(elem2D, myDim_elem2D, 'ty_sur', 'meridional wind str. to ocean'  , 'N/m^2', stress_surf(2, :), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
+        if (sel_forcvar(11)==0) call def_stream(elem2D, myDim_elem2D, 'tx_sur', 'zonal wind str. to ocean'       , 'm/s^2', stress_surf(1, :), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
+        if (sel_forcvar(12)==0) call def_stream(elem2D, myDim_elem2D, 'ty_sur', 'meridional wind str. to ocean'  , 'm/s^2', stress_surf(2, :), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
         
         call def_stream(nod2D , myDim_nod2D , 'cd',    'wind drag coef. '             , '',     cd_atm_oce_arr(:), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
         call def_stream(nod2D , myDim_nod2D , 'ch',    'transfer coeff. sensible heat', '',     ch_atm_oce_arr(:), io_list(i)%freq, io_list(i)%unit, io_list(i)%precision, partit, mesh)
@@ -712,11 +692,6 @@ END DO ! --> DO i=1, io_listsize
         call def_stream((/nlev_upper,   elem2D/), (/nlev_upper,   myDim_elem2D/), 'v_upper',     'meridional velocity','m/s', dynamics%uv(2,:nlev_upper,:),           3, 'h', 4, partit, mesh)
         call def_stream((/nlev_upper+1, nod2D/),  (/nlev_upper+1, myDim_nod2D/),  'w_upper',     'vertical velocity',  'm/s', dynamics%w(:nlev_upper+1,:),            3, 'h', 4, partit, mesh)
     end if
-    if (ldiag_ice) then
-        call def_stream(nod2D,  myDim_nod2D,  'vol_ice',   'ice volume',   'm',  vol_ice(:),   1, 'd', 8, partit, mesh)
-        call def_stream(nod2D,  myDim_nod2D,  'vol_snow',  'snow volume',  'm',  vol_snow(:),  1, 'd', 8, partit, mesh)
-    end if
-
     !___________________________________________________________________________
     ! Richardson number diagnostics
     if (ldiag_Ri) then
@@ -730,26 +705,16 @@ END DO ! --> DO i=1, io_listsize
         call def_stream((/nl,  nod2D/), (/nl, myDim_nod2D/), 'KvdSdz',   'KvdSdz',           'PSU m/s',   KvdSdz(:,:), 1, 'm', i_real8, partit, mesh)
     end if
     !___________________________________________________________________________
-    ! Tracers flux diagnostics
-    if (ldiag_trflx) then
-        call def_stream((/nl-1,  elem2D/), (/nl-1, myDim_elem2D/), 'utemp',   'u*temp',           'm/s*°C',     tuv(1,:,:), 1, 'm', i_real8, partit, mesh)
-        call def_stream((/nl-1,  elem2D/), (/nl-1, myDim_elem2D/), 'vtemp',   'v*temp',           'm/s*°C',     tuv(2,:,:), 1, 'm', i_real8, partit, mesh)
-        call def_stream((/nl-1,  elem2D/), (/nl-1, myDim_elem2D/), 'usalt',   'u*salt',           'm/s*psu',   suv(1,:,:), 1, 'm', i_real8, partit, mesh)
-        call def_stream((/nl-1,  elem2D/), (/nl-1, myDim_elem2D/), 'vsalt',   'v*salt',           'm/s*psu',   suv(2,:,:), 1, 'm', i_real8, partit, mesh)
-    end if
-    !___________________________________________________________________________
     ! output Redi parameterisation
-#if !defined(__MULTIO)
     if (Redi) then
         call def_stream((/nl-1  , nod2D /), (/nl-1,   myDim_nod2D /), 'Redi_K',   'Redi diffusion coefficient', 'm2/s', Ki(:,:),    1, 'y', i_real4, partit, mesh)
     end if
-#endif
 
     !___________________________________________________________________________
     ! output Monin-Obukov (TB04) mixing length
-    !if (use_momix) then
-    !    call def_stream(nod2D, myDim_nod2D, 'momix_length',   'Monin-Obukov mixing length', 'm', mixlength(:),    1, 'm', i_real4, partit, mesh)
-    !end if
+    if (use_momix) then
+        call def_stream(nod2D, myDim_nod2D, 'momix_length',   'Monin-Obukov mixing length', 'm', mixlength(:),    1, 'm', i_real4, partit, mesh)
+    end if
   
     !___________________________________________________________________________
     if (ldiag_curl_vel3) then
@@ -784,8 +749,8 @@ END DO ! --> DO i=1, io_listsize
         if (sel_forcvar( 8)==0) call def_stream(nod2D , myDim_nod2D , 'swr'   , 'short wave radiation'           , 'W/m^2', shortwave(:)     , 1, 'm', i_real4, partit, mesh)
         if (sel_forcvar( 9)==0) call def_stream(nod2D , myDim_nod2D , 'lwr'   , 'long wave radiation'            , 'W/m^2', longwave(:)      , 1, 'm', i_real4, partit, mesh)
         if (sel_forcvar(10)==0) call def_stream(nod2D , myDim_nod2D , 'runoff', 'river runoff'                   , 'none' , runoff(:)        , 1, 'm', i_real4, partit, mesh)
-        if (sel_forcvar(11)==0) call def_stream(elem2D, myDim_elem2D, 'tx_sur', 'zonal wind str. to ocean'       , 'N/m^2', stress_surf(1, :), 1, 'm', i_real4, partit, mesh)
-        if (sel_forcvar(12)==0) call def_stream(elem2D, myDim_elem2D, 'ty_sur', 'meridional wind str. to ocean'  , 'N/m^2', stress_surf(2, :), 1, 'm', i_real4, partit, mesh)
+        if (sel_forcvar(11)==0) call def_stream(elem2D, myDim_elem2D, 'tx_sur', 'zonal wind str. to ocean'       , 'm/s^2', stress_surf(1, :), 1, 'm', i_real4, partit, mesh)
+        if (sel_forcvar(12)==0) call def_stream(elem2D, myDim_elem2D, 'ty_sur', 'meridional wind str. to ocean'  , 'm/s^2', stress_surf(2, :), 1, 'm', i_real4, partit, mesh)
         call def_stream(nod2D , myDim_nod2D , 'cd',    'wind drag coef. '             , '',     cd_atm_oce_arr(:), 1, 'm', i_real4, partit, mesh)
         call def_stream(nod2D , myDim_nod2D , 'ch',    'transfer coeff. sensible heat', '',     ch_atm_oce_arr(:), 1, 'm', i_real4, partit, mesh)
         call def_stream(nod2D , myDim_nod2D , 'ce',    'transfer coeff. evaporation ' , '',     ce_atm_oce_arr(:), 1, 'm', i_real4, partit, mesh)
@@ -1074,8 +1039,6 @@ subroutine write_mean(entry, entry_index)
         ! allocate global 2d array in which local data are gathered
         if(entry%p_partit%mype==entry%root_rank) then
             if(.not. allocated(entry%aux_r8)) allocate(entry%aux_r8(size2))
-        else
-            if(.not. allocated(entry%aux_r8)) allocate(entry%aux_r8(1))
         end if
         
         !_______________________________________________________________________
@@ -1116,8 +1079,6 @@ subroutine write_mean(entry, entry_index)
         ! allocate global 2d array in which local data are gathered
         if(entry%p_partit%mype==entry%root_rank) then
             if(.not. allocated(entry%aux_r4)) allocate(entry%aux_r4(size2))
-        else
-            if(.not. allocated(entry%aux_r4)) allocate(entry%aux_r4(1))
         end if
         
         !_______________________________________________________________________
@@ -1227,9 +1188,6 @@ subroutine output(istep, ice, dynamics, tracers, partit, mesh)
     use MOD_ICE
     use mod_tracer
     use io_gather_module
-#if defined(__MULTIO)
-    use iom
-#endif
 #if defined (__icepack)
     use icedrv_main,    only: init_io_icepack
 #endif
@@ -1245,10 +1203,11 @@ subroutine output(istep, ice, dynamics, tracers, partit, mesh)
     type(t_tracer), intent(in)   , target :: tracers
     type(t_dyn)   , intent(in)   , target :: dynamics
     type(t_ice)   , intent(inout), target :: ice
-    character(:), allocatable             :: filepath
-    real(real64)                          :: rtime !timestamp of the record
+    
+    character(:), allocatable :: filepath
+    real(real64)                  :: rtime !timestamp of the record
 
-ctime=timeold+(dayold-1.)*86400
+    ctime=timeold+(dayold-1.)*86400
     
     !___________________________________________________________________________
     if (lfirst) then
@@ -1267,18 +1226,14 @@ ctime=timeold+(dayold-1.)*86400
     !___________________________________________________________________________
     !PS if (partit%flag_debug .and. partit%mype==0)  print *, achar(27)//'[33m'//' -I/O-> call update_means'//achar(27)//'[0m'  
     call update_means
+    
     !___________________________________________________________________________
     ! loop over defined streams
     do n=1, io_NSTREAMS
         !_______________________________________________________________________
         ! make pointer for entry onto io_stream object
         entry=>io_stream(n)
-!#if defined(__MULTIO)
-!        call mio_write_nod(mio, entry)
-!        lfirst=.false.
-!        return
-!#endif
-
+        
         !_______________________________________________________________________
         !check whether output will be written based on event frequency
         do_output=.false.
@@ -1303,17 +1258,12 @@ ctime=timeold+(dayold-1.)*86400
         ! if its time for output --> do_output==.true.
         if (do_output) then
             if (vec_autorotate) call io_r2g(n, partit, mesh) ! automatically detect if a vector field and rotate if makes sense!
-#if !defined(__MULTIO)
             if(entry%thread_running) call entry%thread%join()
             entry%thread_running = .false.
             
             ! define filepath
-            if (filesplit_freq=='m') then
-                filepath = trim(ResultPath)//trim(entry%name)//'.'//trim(runid)//'.'//cyearnew//'_'//cmonth//'.nc'
-            else
-                filepath = trim(ResultPath)//trim(entry%name)//'.'//trim(runid)//'.'//cyearnew//'.nc'
-            endif
-
+            filepath = trim(ResultPath)//trim(entry%name)//'.'//trim(runid)//'.'//cyearnew//'.nc'
+            
             !___________________________________________________________________
             ! only root rank task does output
             if(partit%mype == entry%root_rank) then
@@ -1360,7 +1310,7 @@ ctime=timeold+(dayold-1.)*86400
                 entry%rec_count=max(entry%rec_count, 1)
                 write(*,*) trim(entry%name)//': current mean I/O counter = ', entry%rec_count
             end if ! --> if(partit%mype == entry%root_rank) then
-#endif
+            
             !___________________________________________________________________
             ! write double precision output
             if (entry%accuracy == i_real8) then
@@ -1385,23 +1335,17 @@ ctime=timeold+(dayold-1.)*86400
                 END DO ! --> DO J=1, size(entry%local_values_r4,dim=2)
 !$OMP END PARALLEL DO
             end if ! --> if (entry%accuracy == i_real8) then
+            
             !___________________________________________________________________
-            entry%lastcounter=entry%addcounter
             entry%addcounter   = 0  ! clean_meanarrays
             entry%ctime_copy = ctime
-
-#if defined(__MULTIO)
-!            if (n==1) then
-            entry%rec_count = istep
-            call send_data_to_multio(entry)
-!            end if            
-#else
+            
             !___________________________________________________________________
             ! this is where the magic happens --> here do_output_callback is
             ! triggered as a method of the io_stream object --> call write_mean(...)
             call entry%thread%run()
             entry%thread_running = .true.
-#endif
+            
         endif ! --> if (do_output) then
     end do ! --> do n=1, io_NSTREAMS
     lfirst=.false.
@@ -1501,12 +1445,7 @@ subroutine def_stream3D(glsize, lcsize, name, description, units, data, freq, fr
     !___________________________________________________________________________
     ! initialise meandata streaming object
     call associate_new_stream(name, entry)
-    entry%previousDate=-1
-    entry%previousTime=-1
-    entry%currentDate=yearold * 10000 + month * 100 + day_in_month
-    entry%currentTime=INT(INT(timeold / 3600) * 10000 + (INT(timeold / 60) - INT(timeold / 3600) * 60) * 100 + (timeold-INT(timeold / 60) * 60))
-    entry%startDate=entry%currentDate
-    entry%startTime=entry%currentTime
+
     !___________________________________________________________________________
     ! fill up 3d meandata streaming object
     ! 3d specific
@@ -1581,12 +1520,7 @@ subroutine def_stream2D(glsize, lcsize, name, description, units, data, freq, fr
     !___________________________________________________________________________
     ! initialise meandata streaming object
     call associate_new_stream(name, entry)
-    entry%previousDate=-1
-    entry%previousTime=-1
-    entry%currentDate=yearold * 10000 + month * 100 + day_in_month
-    entry%currentTime=INT(INT(timeold / 3600) * 10000 + (INT(timeold / 60) - INT(timeold / 3600) * 60) * 100 + (timeold-INT(timeold / 60) * 60))
-    entry%startDate=entry%currentDate
-    entry%startTime=entry%currentTime
+    
     !___________________________________________________________________________
     ! fill up 3d meandata streaming object
     ! 2d specific
@@ -1689,16 +1623,8 @@ subroutine def_stream_after_dimension_specific(entry, name, description, units,
     !___________________________________________________________________________
     if(entry%glsize(1)==mesh%nod2D  .or. entry%glsize(2)==mesh%nod2D) then
       entry%is_elem_based = .false.
-      entry%shrinked_size=partit%myDim_nod2D
     else if(entry%glsize(1)==mesh%elem2D .or. entry%glsize(2)==mesh%elem2D) then
       entry%is_elem_based = .true.
-      entry%shrinked_size=partit%myDim_elem2D_shrinked
-      allocate(entry%shrinked_indx(entry%shrinked_size))
-      entry%shrinked_indx=partit%myInd_elem2D_shrinked
-!      write(*,*) partit%mype, partit%myDim_elem2D, partit%myDim_elem2D_shrinked, partit%myDim_elem2D-partit%myDim_elem2D_shrinked
-!      entry_index=0
-!      call MPI_AllREDUCE(partit%myDim_elem2D_shrinked, entry_index, 1, MPI_INTEGER, MPI_SUM, partit%MPI_COMM_FESOM, err)
-!      write(*,*) 'total elem=', mesh%elem2D, entry_index
     else
       if(partit%mype == 0) print *,"can not determine if ",trim(name)," is node or elem based"
       stop
@@ -1788,8 +1714,6 @@ subroutine io_r2g(n, partit, mesh)
     IF ((trim(entry_x%name)=='atmice_x') .AND. ((trim(entry_y%name)=='atmice_y'))) do_rotation=.TRUE.
     IF ((trim(entry_x%name)=='atmoce_x') .AND. ((trim(entry_y%name)=='atmoce_y'))) do_rotation=.TRUE.    
     IF ((trim(entry_x%name)=='iceoce_x') .AND. ((trim(entry_y%name)=='iceoce_y'))) do_rotation=.TRUE.    
-    IF ((trim(entry_x%name)=='utemp'   ) .AND. ((trim(entry_y%name)=='vtemp'   ))) do_rotation=.TRUE.
-    IF ((trim(entry_x%name)=='usalt'   ) .AND. ((trim(entry_y%name)=='vsalt'   ))) do_rotation=.TRUE.
 
     IF (.NOT. (do_rotation)) RETURN
    
@@ -1837,86 +1761,4 @@ subroutine io_r2g(n, partit, mesh)
 !$OMP END PARALLEL DO
     END IF
 end subroutine
-
-#if defined(__MULTIO)
-SUBROUTINE send_data_to_multio(entry)
-    USE iom
-    IMPLICIT NONE
-
-    TYPE(Meandata), TARGET, INTENT(INOUT)           :: entry
-    TYPE(iom_field_request)                         :: request
-    INTEGER                                         :: numLevels, globalSize, lev, i, n
-
-    numLevels = entry%glsize(1)
-    globalSize = entry%glsize(2)
-
-    request%name = trim(entry%name)
-    entry%previousDate=entry%currentDate
-    entry%previousTime=entry%currentTime
-    entry%currentDate=yearnew * 10000 + month * 100 + day_in_month
-    entry%currentTime=INT(INT(timenew / 3600) * 10000 + (INT(timenew / 60) - INT(timenew / 3600) * 60) * 100 + (timenew-INT(timenew / 60) * 60))
-
-    request%previousDate=entry%previousDate
-    request%previousTime=entry%previousTime
-    request%currentDate =entry%currentDate
-    request%currentTime =entry%currentTime
-    request%startDate   =entry%startDate
-    request%startTime   =entry%startTime
-    request%lastcounter =entry%lastcounter
-    request%sampleInterval=INT(dt)
-
-    IF (.NOT. entry%is_elem_based) THEN
-        request%gridType = "N grid"
-    ELSE
-        request%gridType = "C grid"
-    END IF
-    request%globalSize = globalSize
-    request%step = entry%rec_count
-    if (numLevels==1) then
-        request%category="ocean-2d"
-    else
-        request%category="ocean-3d"
-    end if
-    ! loop over vertical layers --> do gather 3d variables layerwise in 2d slices
-    DO lev=1, numLevels
-        request%level = lev
-
-        IF (entry%is_elem_based) THEN
-            n = SIZE(entry%shrinked_indx)
-        ELSE
-            n = entry%shrinked_size
-        END IF
-
-        IF (ALLOCATED(multio_temporary_array) .AND. (SIZE(multio_temporary_array) .LT. n)) THEN
-            DEALLOCATE(multio_temporary_array)
-        ENDIF
-
-        IF (.NOT. ALLOCATED(multio_temporary_array)) THEN
-            ALLOCATE(multio_temporary_array(n))
-        ENDIF
-
-!$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(i)
-        DO i = 1, n
-            IF (entry%is_elem_based) THEN
-                IF (entry%accuracy == i_real8) THEN
-                    multio_temporary_array(i) = entry%local_values_r8_copy(lev, entry%shrinked_indx(i))
-                ELSE
-                    multio_temporary_array(i) = entry%local_values_r4_copy(lev, entry%shrinked_indx(i))
-                ENDIF
-            ELSE
-                IF (entry%accuracy == i_real8) THEN
-                    multio_temporary_array(i) = entry%local_values_r8_copy(lev, i)
-                ELSE
-                    multio_temporary_array(i) = entry%local_values_r4_copy(lev, i)
-                ENDIF
-            ENDIF
-        ENDDO
-!$OMP END PARALLEL DO
-
-        request%values(1:n) => multio_temporary_array(1:n)
-
-        CALL iom_send_fesom_data(request)
-    END DO
-END SUBROUTINE
-#endif
 end module
diff --git a/src/io_netcdf_file_module.F90 b/src/io_netcdf_file_module.F90
index 82a211b0..c786ff5d 100644
--- a/src/io_netcdf_file_module.F90
+++ b/src/io_netcdf_file_module.F90
@@ -304,7 +304,7 @@ contains
     class(netcdf_file_type), intent(in) :: this
     integer, intent(in) :: varindex
     integer, dimension(:) :: starts, sizes
-    real(8), intent(inout), target :: values(:) ! must be inout or the allocation might be screwed
+    real(8), intent(inout), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO parameters
     include "netcdf.inc"
     real(8), pointer :: values_ptr(:)
@@ -324,7 +324,7 @@ contains
     class(netcdf_file_type), intent(in) :: this
     integer, intent(in) :: varindex
     integer, dimension(:) :: starts, sizes
-    real(4), intent(inout), target :: values(:) ! must be inout or the allocation might be screwed
+    real(4), intent(inout), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO parameters
     include "netcdf.inc"
     real(4), pointer :: values_ptr(:)
@@ -344,7 +344,7 @@ contains
     class(netcdf_file_type), intent(in) :: this
     integer, intent(in) :: varindex
     integer, dimension(:) :: starts, sizes
-    integer, intent(inout), target :: values(:) ! must be inout or the allocation might be screwed
+    integer, intent(inout), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO parameters
     include "netcdf.inc"
     integer, pointer :: values_ptr(:)
@@ -474,7 +474,7 @@ contains
     class(netcdf_file_type), intent(in) :: this
     integer, intent(in) :: varindex
     integer, dimension(:) :: starts, sizes
-    real(8), intent(in), target :: values(:) ! must be inout or the allocation might be screwed
+    real(8), intent(in), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO parameters
     include "netcdf.inc"
     real(8), pointer :: values_ptr(:)
@@ -493,7 +493,7 @@ contains
     class(netcdf_file_type), intent(in) :: this
     integer, intent(in) :: varindex
     integer, dimension(:) :: starts, sizes
-    real(4), intent(in), target :: values(:) ! must be inout or the allocation might be screwed
+    real(4), intent(in), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO parameters
     include "netcdf.inc"
     real(4), pointer :: values_ptr(:)
@@ -512,7 +512,7 @@ contains
     class(netcdf_file_type), intent(in) :: this
     integer, intent(in) :: varindex
     integer, dimension(:) :: starts, sizes
-    integer, intent(in), target :: values(:) ! must be inout or the allocation might be screwed
+    integer, intent(in), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO parameters
     include "netcdf.inc"
     integer, pointer :: values_ptr(:)
diff --git a/src/io_netcdf_module.F90 b/src/io_netcdf_module.F90
index 134b3ed8..1fe6f7cd 100644
--- a/src/io_netcdf_module.F90
+++ b/src/io_netcdf_module.F90
@@ -106,7 +106,7 @@ module io_netcdf_module
     use, intrinsic :: ISO_C_BINDING
     class(netcdf_variable_handle), intent(in) :: this
     integer, intent(in) :: timeindex
-    real(8), intent(inout), target :: values(:) ! must be inout or the allocation might be screwed
+    real(8), intent(inout), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO args
     real(8), pointer :: values_ptr(:)
     integer, allocatable, dimension(:) :: starts, sizes
@@ -125,7 +125,7 @@ module io_netcdf_module
     use, intrinsic :: ISO_C_BINDING
     class(netcdf_variable_handle), intent(in) :: this
     integer, intent(in) :: timeindex
-    real(4), intent(inout), target :: values(:) ! must be inout or the allocation might be screwed
+    real(4), intent(inout), target :: values(..) ! must be inout or the allocation might be screwed
     ! EO args
     real(4), pointer :: values_ptr(:)
     integer, allocatable, dimension(:) :: starts, sizes
diff --git a/src/io_restart.F90 b/src/io_restart.F90
index 18451d89..980ff156 100644
--- a/src/io_restart.F90
+++ b/src/io_restart.F90
@@ -76,25 +76,21 @@ subroutine ini_ocean_io(year, dynamics, tracers, partit, mesh)
 #endif
   call oce_files%def_elem_var('u', 'zonal velocity',        'm/s', dynamics%uv(1,:,:), mesh, partit)
   call oce_files%def_elem_var('v', 'meridional velocity',   'm/s', dynamics%uv(2,:,:), mesh, partit)
-  call oce_files%def_elem_var('urhs_AB', 'Adams-Bashforth for u (n-1 for AB2 and n-2 for AB3)', 'm/s', dynamics%uv_rhsAB(1,1,:,:), mesh, partit)
-  call oce_files%def_elem_var('vrhs_AB', 'Adams-Bashforth for v (n-1 for AB2 and n-2 for AB3)', 'm/s', dynamics%uv_rhsAB(1,2,:,:), mesh, partit)
-  if (dynamics%AB_order==3) then
-       call oce_files%def_elem_var_optional('urhs_AB3', 'Adams-Bashforth for u (n-1) for AB3', 'm/s', dynamics%uv_rhsAB(2,1,:,:), mesh, partit)
-       call oce_files%def_elem_var_optional('vrhs_AB3', 'Adams-Bashforth for v (n-1) for AB3', 'm/s', dynamics%uv_rhsAB(2,2,:,:), mesh, partit)
-  end if
+  call oce_files%def_elem_var('urhs_AB', 'Adams–Bashforth for u', 'm/s', dynamics%uv_rhsAB(1,:,:), mesh, partit)
+  call oce_files%def_elem_var('vrhs_AB', 'Adams–Bashforth for v', 'm/s', dynamics%uv_rhsAB(2,:,:), mesh, partit)
   
-!___Save restart variables for TKE and IDEMIX_________________________________
+  !___Save restart variables for TKE and IDEMIX_________________________________
 !   if (trim(mix_scheme)=='cvmix_TKE' .or. trim(mix_scheme)=='cvmix_TKE+IDEMIX') then
   if (mix_scheme_nmb==5 .or. mix_scheme_nmb==56) then
-        call oce_files%def_node_var_optional('tke', 'Turbulent Kinetic Energy', 'm2/s2', tke(:,:), mesh, partit)
+        call oce_files%def_node_var('tke', 'Turbulent Kinetic Energy', 'm2/s2', tke(:,:), mesh, partit)
   endif
 !   if (trim(mix_scheme)=='cvmix_IDEMIX' .or. trim(mix_scheme)=='cvmix_TKE+IDEMIX') then
   if (mix_scheme_nmb==6 .or. mix_scheme_nmb==56) then
-        call oce_files%def_elem_var_optional('iwe', 'Internal Wave Energy'    , 'm2/s2', iwe(:,:), mesh, partit)
+        call oce_files%def_elem_var('iwe', 'Internal Wave Energy'    , 'm2/s2', iwe(:,:), mesh, partit)
   endif 
   if (dynamics%opt_visc==8) then
-        call oce_files%def_elem_var_optional('uke', 'unresolved kinetic energy', 'm2/s2', uke(:,:), mesh, partit)
-        call oce_files%def_elem_var_optional('uke_rhs', 'unresolved kinetic energy rhs', 'm2/s2', uke_rhs(:,:), mesh, partit)
+        call oce_files%def_elem_var('uke', 'unresolved kinetic energy', 'm2/s2', uke(:,:), mesh, partit)
+        call oce_files%def_elem_var('uke_rhs', 'unresolved kinetic energy rhs', 'm2/s2', uke_rhs(:,:), mesh, partit)
   endif
   
   do j=1,tracers%num_tracers
@@ -107,34 +103,14 @@ subroutine ini_ocean_io(year, dynamics, tracers, partit, mesh)
          trname='salt'
          longname='salinity'
          units='psu'
-       CASE(6)
-         trname='sf6'
-         longname='sulfur hexafluoride'
-         units='mol / m**3'
-       CASE(12)
-         trname='cfc12'
-         longname='chlorofluorocarbon CFC-12'
-         units='mol / m**3'
-       CASE(14)
-         trname='r14c'
-         longname='14C / C ratio of DIC'
-         units='none'
-       CASE(39)
-         trname='r39ar'
-         longname='39Ar / Ar ratio'
-         units='none'
        CASE DEFAULT
-!        other passive tracers
          write(trname,'(A3,i1)') 'tra_', j
          write(longname,'(A15,i1)') 'passive tracer ', j
          units='none'
      END SELECT
      call oce_files%def_node_var(trim(trname), trim(longname), trim(units), tracers%data(j)%values(:,:), mesh, partit)
-     longname=trim(longname)//', Adams-Bashforth'
-     call oce_files%def_node_var(trim(trname)//'_AB', trim(longname), trim(units), tracers%data(j)%valuesAB(:,:),    mesh, partit)
-     call oce_files%def_node_var_optional(trim(trname)//'_M1', trim(longname), trim(units), tracers%data(j)%valuesold(1,:,:), mesh, partit)
-     if (tracers%data(j)%AB_order==3) &
-     call oce_files%def_node_var_optional(trim(trname)//'_M2', trim(longname), trim(units), tracers%data(j)%valuesold(2,:,:), mesh, partit)
+     longname=trim(longname)//', Adams–Bashforth'
+     call oce_files%def_node_var(trim(trname)//'_AB', trim(longname), trim(units), tracers%data(j)%valuesAB(:,:), mesh, partit)
   end do
   call oce_files%def_node_var('w', 'vertical velocity', 'm/s', dynamics%w, mesh, partit)
   call oce_files%def_node_var('w_expl', 'vertical velocity', 'm/s', dynamics%w_e, mesh, partit)
@@ -185,7 +161,7 @@ end subroutine ini_ice_io
 !
 !--------------------------------------------------------------------------------------------
 !
-subroutine restart(istep, nstart, ntotal, l_read, which_readr, ice, dynamics, tracers, partit, mesh)
+subroutine restart(istep, l_read, which_readr, ice, dynamics, tracers, partit, mesh)
 
 #if defined(__icepack)
   icepack restart not merged here ! produce a compiler error if USE_ICEPACK=ON; todo: merge icepack restart from 68d8b8b
@@ -196,7 +172,7 @@ subroutine restart(istep, nstart, ntotal, l_read, which_readr, ice, dynamics, tr
   ! this is the main restart subroutine
   ! if l_read   is TRUE the restart file will be read
 
-  integer :: istep, nstart, ntotal
+  integer :: istep
   logical :: l_read
   logical :: is_portable_restart_write, is_raw_restart_write, is_bin_restart_write
   type(t_mesh)  , intent(inout), target :: mesh
@@ -220,12 +196,12 @@ subroutine restart(istep, nstart, ntotal, l_read, which_readr, ice, dynamics, tr
   ! initialize directory for core dump restart 
   if(.not. initialized_raw) then
     initialized_raw = .true.
-    raw_restart_dirpath  = trim(ResultPath)//"fesom_raw_restart/np"//int_to_txt(partit%npes)
-    raw_restart_infopath = trim(ResultPath)//"fesom_raw_restart/np"//int_to_txt(partit%npes)//".info"
+    raw_restart_dirpath  = trim(ResultPath)//"/fesom_raw_restart/np"//int_to_txt(partit%npes)
+    raw_restart_infopath = trim(ResultPath)//"/fesom_raw_restart/np"//int_to_txt(partit%npes)//".info"
     if(raw_restart_length_unit /= "off") then
       if(partit%mype == RAW_RESTART_METADATA_RANK) then
         ! execute_command_line with mkdir sometimes fails, use a custom implementation around mkdir from C instead
-        call mkdir(trim(ResultPath)//"fesom_raw_restart") ! we have no mkdir -p, create the intermediate dirs separately
+        call mkdir(trim(ResultPath)//"/fesom_raw_restart") ! we have no mkdir -p, create the intermediate dirs separately
         call mkdir(raw_restart_dirpath)
       end if
       call MPI_Barrier(partit%MPI_COMM_FESOM, mpierr) ! make sure the dir has been created before we continue...
@@ -236,12 +212,12 @@ subroutine restart(istep, nstart, ntotal, l_read, which_readr, ice, dynamics, tr
   ! initialize directory for derived type binary restart
   if(.not. initialized_bin) then
     initialized_bin = .true.
-    bin_restart_dirpath  = trim(ResultPath)//"fesom_bin_restart/np"//int_to_txt(partit%npes)
-    bin_restart_infopath = trim(ResultPath)//"fesom_bin_restart/np"//int_to_txt(partit%npes)//".info"
+    bin_restart_dirpath  = trim(ResultPath)//"/fesom_bin_restart/np"//int_to_txt(partit%npes)
+    bin_restart_infopath = trim(ResultPath)//"/fesom_bin_restart/np"//int_to_txt(partit%npes)//".info"
     if(bin_restart_length_unit /= "off") then
         if(partit%mype == RAW_RESTART_METADATA_RANK) then
             ! execute_command_line with mkdir sometimes fails, use a custom implementation around mkdir from C instead
-            call mkdir(trim(ResultPath)//"fesom_bin_restart") ! we have no mkdir -p, create the intermediate dirs separately
+            call mkdir(trim(ResultPath)//"/fesom_bin_restart") ! we have no mkdir -p, create the intermediate dirs separately
             call mkdir(bin_restart_dirpath)
         end if
         call MPI_Barrier(partit%MPI_COMM_FESOM, mpierr) ! make sure the dir has been created before we continue...
@@ -346,11 +322,7 @@ subroutine restart(istep, nstart, ntotal, l_read, which_readr, ice, dynamics, tr
   if(is_portable_restart_write .and. (raw_restart_length_unit /= "off")) then
     is_raw_restart_write = .true. ! always write a raw restart together with the portable restart (unless raw restarts are off)
   else
-#if !defined __ifsinterface
     is_raw_restart_write = is_due(trim(raw_restart_length_unit), raw_restart_length, istep)
-#else
-    is_raw_restart_write = is_due(trim(raw_restart_length_unit), raw_restart_length, istep) .OR. (istep==ntotal)
-#endif
   end if
   
   ! --> should write derived type binary restart: True/False
diff --git a/src/io_restart_derivedtype.F90 b/src/io_restart_derivedtype.F90
index 3d07a244..dd13b91e 100644
--- a/src/io_restart_derivedtype.F90
+++ b/src/io_restart_derivedtype.F90
@@ -13,9 +13,9 @@ module restart_derivedtype_module
             type(t_mesh)  ,         intent(in), target           :: mesh
             type(t_ice)   ,         intent(in), target, optional :: ice
             type(t_dyn)   ,         intent(in), target, optional :: dynamics
-            type(t_tracer),         intent(in), target, optional :: tracers
+            type(t_tracer),         intent(in), target, optional :: tracers            
         end subroutine
-
+        
         subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
             use MOD_ICE
             use MOD_DYN
@@ -27,10 +27,10 @@ module restart_derivedtype_module
             type(t_mesh)  , intent(inout), target           :: mesh
             type(t_ice)   , intent(inout), target, optional :: ice
             type(t_dyn)   , intent(inout), target, optional :: dynamics
-            type(t_tracer), intent(inout), target, optional :: tracers
+            type(t_tracer), intent(inout), target, optional :: tracers            
         end subroutine
     end interface
-end module
+end module    
 !
 !
 !_______________________________________________________________________________
@@ -41,8 +41,8 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
     use MOD_PARTIT
     use MOD_MESH
     use fortran_utils
-    implicit none
-
+    implicit none 
+    
     integer, dimension(3) , intent(in)           :: ctarr ! //cstep,ctime,cyear//
     character(len=*)      , intent(in)           :: path_in
     character(len=*)      , intent(in)           :: pathi_in
@@ -55,6 +55,13 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
     ! EO parameters
     integer fileunit, fileunit_i
 
+#if defined(__PGI)
+    if (partit%mype == 0) then
+       write(*,*) 'write_all_bin_restarts is deactivated for PGI compiler because of T_TRACER%DATA & T_ICE%DATA cause write call to crash'
+       write(*,*) '*** checked for NVHPC/22.1 ***'
+    end if
+#else
+
     !___________________________________________________________________________
     ! write info file
     if(partit%mype == 0) then
@@ -65,10 +72,10 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
         write(fileunit_i, '(g0)') ctarr(1)
         write(fileunit_i, '(g0)') ctarr(2)
         write(fileunit_i, '(2(g0))') "! year: ",ctarr(3)
-    end if
-
+    end if 
+    
     !___________________________________________________________________________
-    ! mesh derived type
+    ! mesh derived type 
     fileunit = partit%mype+300
     open(newunit = fileunit, &
         file     = trim(path_in)//'/'//'t_mesh.'//mpirank_to_txt(partit%MPI_COMM_FESOM), &
@@ -79,10 +86,10 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
     if(partit%mype == 0) then
         write(fileunit_i, '(1(g0))') "!   t_mesh"
         print *, achar(27)//'[33m'//'     > write derived type t_mesh'//achar(27)//'[0m'
-    end if
-
+    end if     
+    
     !___________________________________________________________________________
-    ! partit derived type
+    ! partit derived type 
     fileunit = partit%mype+300
     open(newunit = fileunit, &
         file     = trim(path_in)//'/'//'t_partit.'//mpirank_to_txt(partit%MPI_COMM_FESOM), &
@@ -90,10 +97,10 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
         form     = 'unformatted')
     write(fileunit) partit
     close(fileunit)
-    if(partit%mype == 0) then
+    if(partit%mype == 0) then 
         write(fileunit_i, '(1(g0))') "!   t_partit"
         print *, achar(27)//'[33m'//'     > write derived type t_partit'//achar(27)//'[0m'
-    end if
+    end if 
 
     !___________________________________________________________________________
     ! tracer derived type
@@ -112,7 +119,7 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
     end if
 
     !___________________________________________________________________________
-    ! dynamics derived type
+    ! dynamics derived type 
     if (present(dynamics)) then
         fileunit = partit%mype+300
         open(newunit = fileunit, &
@@ -121,14 +128,14 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
             form     = 'unformatted')
         write(fileunit) dynamics
         close(fileunit)
-        if(partit%mype == 0) then
+        if(partit%mype == 0) then 
             write(fileunit_i, '(1(g0))') "!   t_dynamics"
             print *, achar(27)//'[33m'//'     > write derived type t_dynamics'//achar(27)//'[0m'
-        end if
-    end if
-
+        end if     
+    end if     
+    
     !___________________________________________________________________________
-    ! ice derived type
+    ! ice derived type 
     if (present(ice)) then
         fileunit = partit%mype+300
         open(newunit = fileunit, &
@@ -137,18 +144,19 @@ subroutine write_all_bin_restarts(ctarr, path_in, pathi_in, partit, mesh, ice, d
             form    = 'unformatted')
         write(fileunit) ice
         close(fileunit)
-        if(partit%mype == 0) then
+        if(partit%mype == 0) then 
             write(fileunit_i, '(1(g0))') "!   t_ice"
             print *, achar(27)//'[33m'//'     > write derived type t_ice'//achar(27)//'[0m'
-        end if
-    end if
+        end if     
+    end if 
     !___________________________________________________________________________
     if(partit%mype == 0) close(fileunit_i)
+#endif !defined(__PGI)
 end subroutine
 !
 !
 !_______________________________________________________________________________
-! read derived type binary restart files, depending on input (see optional) not
+! read derived type binary restart files, depending on input (see optional) not 
 ! all derived type binaries are read --> functionalitiy for dwarfs !
 subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
     use MOD_ICE
@@ -157,22 +165,29 @@ subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
     use MOD_PARTIT
     use MOD_MESH
     use fortran_utils
-    implicit none
-
-    ! do optional here for the usage with dwarfs, since there only specific derived
+    implicit none 
+    
+    ! do optional here for the usage with dwarfs, since there only specific derived  
     ! types will be needed
     character(len=*), intent(in)                    :: path_in
     type(t_partit), intent(inout), target           :: partit
-    type(t_mesh)  , intent(inout), target           :: mesh
+    type(t_mesh)  , intent(inout), target           :: mesh    
     type(t_ice)   , intent(inout), target, optional :: ice
     type(t_dyn)   , intent(inout), target, optional :: dynamics
     type(t_tracer), intent(inout), target, optional :: tracers
     integer fileunit
 
+#if defined(__PGI)
+    if (partit%mype == 0) then
+       write(*,*) 'read_all_bin_restarts is deactivated for PGI compiler because of T_TRACER%DATA & T_ICE%DATA cause write call to crash'
+       write(*,*) '*** checked for NVHPC/22.1 ***'
+    end if
+#else
+
     !___________________________________________________________________________
     if (partit%mype==0) print *, achar(27)//'[1;33m'//' --> read restarts from derived type binary'//achar(27)//'[0m'
     !___________________________________________________________________________
-    ! mesh derived type
+    ! mesh derived type 
     fileunit = partit%mype+300
     open( newunit = fileunit, &
           file     = trim(path_in)//'/'//'t_mesh.'//mpirank_to_txt(partit%MPI_COMM_FESOM), &
@@ -181,9 +196,9 @@ subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
     read(fileunit) mesh
     close(fileunit)
     if (partit%mype==0) print *, achar(27)//'[33m'//'     > read derived type t_mesh'//achar(27)//'[0m'
-
+    
     !___________________________________________________________________________
-    ! partit derived type
+    ! partit derived type 
     fileunit = partit%mype+300
     open(newunit = fileunit, &
          file     = trim(path_in)//'/'//'t_partit.'//mpirank_to_txt(partit%MPI_COMM_FESOM), &
@@ -192,22 +207,22 @@ subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
     read(fileunit) partit
     close(fileunit)
     if (partit%mype==0) print *, achar(27)//'[33m'//'     > read derived type t_partit'//achar(27)//'[0m'
-
+    
     !___________________________________________________________________________
-    ! tracer derived type
+    ! tracer derived type     
     if (present(tracers)) then
         fileunit = partit%mype+300
         open(newunit = fileunit, &
             file     = trim(path_in)//'/'//'t_tracer.'//mpirank_to_txt(partit%MPI_COMM_FESOM), &
             status   = 'old', &
             form     = 'unformatted')
-        read(fileunit) tracers
+        read(fileunit) tracers  
         close(fileunit)
         if (partit%mype==0) print *, achar(27)//'[33m'//'     > read derived type t_tracer'//achar(27)//'[0m'
-    end if
-
+    end if 
+    
     !___________________________________________________________________________
-    ! dynamics derived type
+    ! dynamics derived type 
     if (present(dynamics)) then
         fileunit = partit%mype+300
         open(newunit = fileunit, &
@@ -217,10 +232,10 @@ subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
         read(fileunit) dynamics
         close(fileunit)
         if (partit%mype==0) print *, achar(27)//'[33m'//'     > read derived type t_dynamics'//achar(27)//'[0m'
-    end if
-
+    end if 
+    
     !___________________________________________________________________________
-    ! ice derived type
+    ! ice derived type 
     if (present(ice)) then
         fileunit = partit%mype+300
         open(newunit = fileunit, &
@@ -231,4 +246,6 @@ subroutine read_all_bin_restarts(path_in, partit, mesh, ice, dynamics, tracers)
         close(fileunit)
         if (partit%mype==0) print *, achar(27)//'[33m'//'     > read derived type t_ice'//achar(27)//'[0m'
     end if
+#endif !defined(__PGI)
 end subroutine
+  
diff --git a/src/io_restart_file_group.F90 b/src/io_restart_file_group.F90
index f9e6d2b6..0d3bb54f 100644
--- a/src/io_restart_file_group.F90
+++ b/src/io_restart_file_group.F90
@@ -17,7 +17,7 @@ module restart_file_group_module
 
   type restart_file_group
     private
-    type(restart_file_type), public :: files(32)
+    type(restart_file_type), public :: files(30)
     integer, public :: nfiles = 0 ! todo: allow dynamically allocated size without messing with shallow copied pointers
   contains
     generic, public :: def_node_var => def_node_var_2d, def_node_var_3d
diff --git a/src/oce_adv_tra_driver.F90 b/src/oce_adv_tra_driver.F90
index 52125f40..c51be2e1 100644
--- a/src/oce_adv_tra_driver.F90
+++ b/src/oce_adv_tra_driver.F90
@@ -103,44 +103,24 @@ subroutine do_oce_adv_tra(dt, vel, w, wi, we, tr_num, dynamics, tracers, partit,
     fct_minus       => tracers%work%fct_minus
     dttf_h          => tracers%work%del_ttf_advhoriz
     dttf_v          => tracers%work%del_ttf_advvert
-
     !___________________________________________________________________________
-    ! compute FCT horzontal and vertical low order solution as well as lw order
+    ! compute FCT horzontal and vertical low order solution as well as lw order 
     ! part of antidiffusive flux
-    if (trim(tracers%data(tr_num)%tra_adv_lim)=='FCT') then
+    if (trim(tracers%data(tr_num)%tra_adv_lim)=='FCT') then 
         ! compute the low order upwind horizontal flux
         ! o_init_zero=.true.  : zero the horizontal flux before computation
         ! o_init_zero=.false. : input flux will be substracted
         call adv_tra_hor_upw1(vel, ttf, partit, mesh, adv_flux_hor, o_init_zero=.true.)
         ! update the LO solution for horizontal contribution
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR COLLAPSE(2) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
         do n=1, myDim_nod2D+eDim_nod2D
-           do nz=1, mesh%nl - 1
-              fct_LO(nz,n) = 0.0_WP
-           end do
+           fct_LO(:,n) = 0.0_WP
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(e, enodes, el, nl1, nu1, nl2, nu2, nu12, nl12, nz)
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC PARALLEL LOOP GANG PRIVATE(enodes, el) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#else
-        !$ACC UPDATE SELF(fct_lo, adv_flux_hor)
-#endif
-#endif
         do e=1, myDim_edge2D
             enodes=edges(:,e)
-            el=edge_tri(:,e)
+            el=edge_tri(:,e)        
             nl1=nlevels(el(1))-1
             nu1=ulevels(el(1))
             nl2=0
@@ -148,110 +128,66 @@ subroutine do_oce_adv_tra(dt, vel, w, wi, we, tr_num, dynamics, tracers, partit,
             if(el(2)>0) then
                 nl2=nlevels(el(2))-1
                 nu2=ulevels(el(2))
-            end if
-
+            end if     
+            
             nl12 = max(nl1,nl2)
             nu12 = nu1
             if (nu2>0) nu12 = min(nu1,nu2)
-
+            
             !!PS do  nz=1, max(nl1, nl2)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
             call omp_set_lock(partit%plock(enodes(1)))
 #else
 !$OMP ORDERED
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC LOOP VECTOR
-#endif
 #endif
             do nz=nu12, nl12
-#if !defined(DISABLE_OPENACC_ATOMICS)
-               !$ACC ATOMIC UPDATE
-#endif
-
                fct_LO(nz, enodes(1))=fct_LO(nz, enodes(1))+adv_flux_hor(nz, e)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
             end do
             call omp_unset_lock(partit%plock(enodes(1)))
             call omp_set_lock  (partit%plock(enodes(2)))
             do nz=nu12, nl12
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-               !$ACC ATOMIC UPDATE
-#endif
 #endif
                fct_LO(nz, enodes(2))=fct_LO(nz, enodes(2))-adv_flux_hor(nz, e)
             end do
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
             call omp_unset_lock(partit%plock(enodes(2)))
 #else
 !$OMP END ORDERED
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC END LOOP
-#endif
 #endif
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END PARALLEL LOOP
-#else
-        !$ACC UPDATE DEVICE(fct_lo)
-#endif
-#endif
-
         ! compute the low order upwind vertical flux (explicit part only)
         ! zero the input/output flux before computation
-        call adv_tra_ver_upw1(we, ttf, partit, mesh, adv_flux_ver, o_init_zero=.true.)
+        call adv_tra_ver_upw1(we, ttf, partit, mesh, adv_flux_ver, o_init_zero=.true.)        
         ! update the LO solution for vertical contribution
-
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(n, nu1, nl1, nz)
-#else
-        !$ACC PARALLEL LOOP GANG PRESENT(fct_LO) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
         do n=1, myDim_nod2D
             nu1 = ulevels_nod2D(n)
             nl1 = nlevels_nod2D(n)
             !!PS do  nz=1, nlevels_nod2D(n)-1
-            !$ACC LOOP VECTOR
             do  nz= nu1, nl1-1
                 fct_LO(nz,n)=(ttf(nz,n)*hnode(nz,n)+(fct_LO(nz,n)+(adv_flux_ver(nz, n)-adv_flux_ver(nz+1, n)))*dt/areasvol(nz,n))/hnode_new(nz,n)
             end do
-            !$ACC END LOOP
         end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
+
         if (dynamics%use_wsplit) then !wvel/=wvel_e
             ! update for implicit contribution (w_split option)
-!when adv_tra_vert_impl is ported to ACC the UPDATEs below wont be needed!
-!$ACC UPDATE HOST(fct_LO)
             call adv_tra_vert_impl(dt, wi, fct_LO, partit, mesh)
-!$ACC UPDATE DEVICE(fct_LO)
             ! compute the low order upwind vertical flux (full vertical velocity)
             ! zero the input/output flux before computation
-            ! --> compute here low order part of vertical anti diffusive fluxes,
+            ! --> compute here low order part of vertical anti diffusive fluxes, 
             !     has to be done on the full vertical velocity w
             call adv_tra_ver_upw1(w, ttf, partit, mesh, adv_flux_ver, o_init_zero=.true.)
         end if
-        call exchange_nod(fct_LO, partit, luse_g2g = .true.)
+        call exchange_nod(fct_LO, partit)
 !$OMP BARRIER
     end if
     do_zero_flux=.true.
     if (trim(tracers%data(tr_num)%tra_adv_lim)=='FCT') do_zero_flux=.false.
     !___________________________________________________________________________
-    ! do horizontal tracer advection, in case of FCT high order solution
+    ! do horizontal tracer advection, in case of FCT high order solution 
     SELECT CASE(trim(tracers%data(tr_num)%tra_adv_hor))
         CASE('MUSCL')
             ! compute the untidiffusive horizontal flux (o_init_zero=.false.: input is the LO horizontal flux computed above)
@@ -269,9 +205,8 @@ subroutine do_oce_adv_tra(dt, vel, w, wi, we, tr_num, dynamics, tracers, partit,
     else
        pwvel=>we
     end if
-
     !___________________________________________________________________________
-    ! do vertical tracer advection, in case of FCT high order solution
+    ! do vertical tracer advection, in case of FCT high order solution 
     SELECT CASE(trim(tracers%data(tr_num)%tra_adv_ver))
         CASE('QR4C')
             ! compute the untidiffusive vertical flux   (o_init_zero=.false.:input is the LO vertical flux computed above)
@@ -285,7 +220,7 @@ subroutine do_oce_adv_tra(dt, vel, w, wi, we, tr_num, dynamics, tracers, partit,
         CASE DEFAULT !unknown
             if (mype==0) write(*,*) 'Unknown vertical advection type ',  trim(tracers%data(tr_num)%tra_adv_ver), '! Check your namelists!'
             call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
-        ! --> be aware the vertical implicite part in case without FCT is done in
+        ! --> be aware the vertical implicite part in case without FCT is done in 
         !     oce_ale_tracer.F90 --> subroutine diff_ver_part_impl_ale(tr_num, partit, mesh)
         !     for do_wimpl=.true.
     END SELECT
@@ -298,7 +233,6 @@ subroutine do_oce_adv_tra(dt, vel, w, wi, we, tr_num, dynamics, tracers, partit,
     else
        call oce_tra_adv_flux2dtracer(dt, dttf_h, dttf_v, adv_flux_hor, adv_flux_ver, partit, mesh)
     end if
-
 end subroutine do_oce_adv_tra
 !
 !
@@ -328,131 +262,70 @@ subroutine oce_tra_adv_flux2dtracer(dt, dttf_h, dttf_v, flux_h, flux_v, partit,
     !___________________________________________________________________________
     ! c. Update the solution
     ! Vertical
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, nz, k, elem, enodes, num, el, nu12, nl12, nu1, nu2, nl1, nl2, edge)
-#endif
     if (present(use_lo)) then
        if (use_lo) then
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-        !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
           do n=1, myDim_nod2d
              nu1 = ulevels_nod2D(n)
              nl1 = nlevels_nod2D(n)
              !!PS do nz=1,nlevels_nod2D(n)-1
-             !$ACC LOOP VECTOR
-             do nz=nu1, nl1-1
+             do nz=nu1, nl1-1  
                 dttf_v(nz,n)=dttf_v(nz,n)-ttf(nz,n)*hnode(nz,n)+LO(nz,n)*hnode_new(nz,n)
              end do
-             !$ACC END LOOP
-          end do
-#ifndef ENABLE_OPENACC
+           end do
 !$OMP END DO
-#else
-         !$ACC END PARALLEL LOOP
-#endif
        end if
     end if
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1, myDim_nod2d
         nu1 = ulevels_nod2D(n)
         nl1 = nlevels_nod2D(n)
-        !$ACC LOOP VECTOR
-        do nz=nu1,nl1-1
+        do nz=nu1,nl1-1  
             dttf_v(nz,n)=dttf_v(nz,n) + (flux_v(nz,n)-flux_v(nz+1,n))*dt/areasvol(nz,n)
         end do
-        !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
-!$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
+!$OMP END DO  
     ! Horizontal
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC PARALLEL LOOP GANG PRIVATE(enodes, el) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#else
-    !$ACC UPDATE SELF(dttf_h, flux_h)
-#endif
-#endif
     do edge=1, myDim_edge2D
         enodes(1:2)=edges(:,edge)
         el=edge_tri(:,edge)
         nl1=nlevels(el(1))-1
         nu1=ulevels(el(1))
-
+        
         nl2=0
         nu2=0
         if(el(2)>0) then
             nl2=nlevels(el(2))-1
             nu2=ulevels(el(2))
-        end if
-
+        end if 
+        
         nl12 = max(nl1,nl2)
         nu12 = nu1
         if (nu2>0) nu12 = min(nu1,nu2)
-
-#ifndef ENABLE_OPENACC
+            
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
         call omp_set_lock(partit%plock(enodes(1)))
 #else
 !$OMP ORDERED
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC LOOP VECTOR
-#endif
 #endif
         do nz=nu12, nl12
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
             dttf_h(nz,enodes(1))=dttf_h(nz,enodes(1))+flux_h(nz,edge)*dt/areasvol(nz,enodes(1))
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
         end do
         call omp_unset_lock(partit%plock(enodes(1)))
         call omp_set_lock  (partit%plock(enodes(2)))
         do nz=nu12, nl12
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-            !$ACC ATOMIC UPDATE
-#endif
 #endif
             dttf_h(nz,enodes(2))=dttf_h(nz,enodes(2))-flux_h(nz,edge)*dt/areasvol(nz,enodes(2))
         end do
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
         call omp_unset_lock(partit%plock(enodes(2)))
 #else
 !$OMP END ORDERED
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-        !$ACC END LOOP
-#endif
 #endif
     end do
-
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-!$OMP END PARALLEL
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC END PARALLEL LOOP
-#else
-    !$ACC UPDATE DEVICE(dttf_h)
-#endif
-#endif
-
+!$OMP END PARALLEL 
 end subroutine oce_tra_adv_flux2dtracer
diff --git a/src/oce_adv_tra_fct.F90 b/src/oce_adv_tra_fct.F90
index f9a38278..acd77293 100644
--- a/src/oce_adv_tra_fct.F90
+++ b/src/oce_adv_tra_fct.F90
@@ -48,13 +48,13 @@ subroutine oce_adv_tra_fct_init(twork, partit, mesh)
 #include "associate_mesh_ass.h"
 
     my_size=myDim_nod2D+eDim_nod2D
-    allocate(twork%fct_LO(nl-1, my_size))        ! Low-order solution
+    allocate(twork%fct_LO(nl-1, my_size))        ! Low-order solution 
     allocate(twork%adv_flux_hor(nl-1,partit%myDim_edge2D)) ! antidiffusive hor. contributions / from edges
     allocate(twork%adv_flux_ver(nl, partit%myDim_nod2D))   ! antidiffusive ver. fluxes / from nodes
 
     allocate(twork%fct_ttf_max(nl-1, my_size),twork%fct_ttf_min(nl-1, my_size))
     allocate(twork%fct_plus(nl-1, my_size),   twork%fct_minus(nl-1, my_size))
-    ! Initialize with zeros:
+    ! Initialize with zeros: 
     twork%fct_LO=0.0_WP
     twork%adv_flux_hor=0.0_WP
     twork%adv_flux_ver=0.0_WP
@@ -62,7 +62,7 @@ subroutine oce_adv_tra_fct_init(twork, partit, mesh)
     twork%fct_ttf_min=0.0_WP
     twork%fct_plus=0.0_WP
     twork%fct_minus=0.0_WP
-
+    
     if (mype==0) write(*,*) 'FCT is initialized'
 end subroutine oce_adv_tra_fct_init
 
@@ -75,7 +75,7 @@ subroutine oce_tra_adv_fct(dt, ttf, lo, adf_h, adf_v, fct_ttf_min, fct_ttf_max,
     ! Limits antidiffusive fluxes==the difference in flux HO-LO
     ! LO ==Low-order  (first-order upwind)
     ! HO ==High-order (3rd/4th order gradient reconstruction method)
-    ! Adds limited fluxes to the LO solution
+    ! Adds limited fluxes to the LO solution   
     use MOD_MESH
     use MOD_TRACER
     USE MOD_PARTIT
@@ -95,7 +95,7 @@ subroutine oce_tra_adv_fct(dt, ttf, lo, adf_h, adf_v, fct_ttf_min, fct_ttf_max,
     real(kind=WP), intent(inout)      :: fct_minus(mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
     real(kind=WP), intent(inout)      :: AUX(:,:,:) !a large auxuary array, let us use twork%edge_up_dn_grad(1:4, 1:NL-2, 1:partit%myDim_edge2D) to save space
     integer                           :: n, nz, k, elem, enodes(3), num, el(2), nl1, nl2, nu1, nu2, nl12, nu12, edge
-    real(kind=WP)                     :: flux, ae, tvert_max(mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D), tvert_min(mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
+    real(kind=WP)                     :: flux, ae,tvert_max(mesh%nl-1),tvert_min(mesh%nl-1) 
     real(kind=WP)                     :: flux_eps=1e-16
     real(kind=WP)                     :: bignumber=1e3
 #include "associate_part_def.h"
@@ -103,202 +103,115 @@ subroutine oce_tra_adv_fct(dt, ttf, lo, adf_h, adf_v, fct_ttf_min, fct_ttf_max,
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
 
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, nz, k, elem, enodes, num, el, nl1, nl2, nu1, nu2, nl12, nu12, edge, &
-!$OMP                          flux, ae)
+!$OMP                          flux, ae,tvert_max, tvert_min)
     ! --------------------------------------------------------------------------
     ! ttf is the tracer field on step n
-    ! del_ttf is the increment
+    ! del_ttf is the increment 
     ! vlimit sets the version of limiting, see below
     ! --------------------------------------------------------------------------
     !___________________________________________________________________________
     ! a1. max, min between old solution and updated low-order solution per node
-#else
-    !$ACC DATA CREATE(tvert_max, tvert_min)
-#endif
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1,myDim_nod2D + edim_nod2d
         nu1 = ulevels_nod2D(n)
         nl1 = nlevels_nod2D(n)
-        !$ACC LOOP VECTOR
-        do nz=nu1, nl1-1
+        do nz=nu1, nl1-1 
             fct_ttf_max(nz,n)=max(LO(nz,n), ttf(nz,n))
             fct_ttf_min(nz,n)=min(LO(nz,n), ttf(nz,n))
         end do
-        !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
-!$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
+!$OMP END DO        
     !___________________________________________________________________________
     ! a2. Admissible increments on elements
     !     (only layers below the first and above the last layer)
     !     look for max, min bounds for each element --> AUX here auxilary array
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG PRIVATE(enodes) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do elem=1, myDim_elem2D
         enodes=elem2D_nodes(:,elem)
         nu1 = ulevels(elem)
         nl1 = nlevels(elem)
-        !$ACC LOOP VECTOR
         do nz=nu1, nl1-1
-            AUX(1,nz,elem)=max(fct_ttf_max(nz,enodes(1)), fct_ttf_max(nz,enodes(2)), fct_ttf_max(nz,enodes(3)))
-            AUX(2,nz,elem)=min(fct_ttf_min(nz,enodes(1)), fct_ttf_min(nz,enodes(2)), fct_ttf_min(nz,enodes(3)))
+            AUX(1,nz,elem)=maxval(fct_ttf_max(nz,enodes))
+            AUX(2,nz,elem)=minval(fct_ttf_min(nz,enodes))
         end do
-        !$ACC END LOOP
         if (nl1<=nl-1) then
-            !$ACC LOOP VECTOR
             do nz=nl1,nl-1
                 AUX(1,nz,elem)=-bignumber
                 AUX(2,nz,elem)= bignumber
             end do
-            !$ACC END LOOP
         endif
     end do ! --> do elem=1, myDim_elem2D
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     !___________________________________________________________________________
     ! a3. Bounds on clusters and admissible increments
     ! Vertical1: In this version we look at the bounds on the clusters
-    !            above and below, which leaves wide bounds because typically
-    !            vertical gradients are larger.
+    !            above and below, which leaves wide bounds because typically 
+    !            vertical gradients are larger.  
         !Horizontal
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1, myDim_nod2D
        nu1 = ulevels_nod2D(n)
-       nl1 = nlevels_nod2D(n)
+       nl1 = nlevels_nod2D(n)      
        !___________________________________________________________________
-       !$ACC LOOP VECTOR
        do nz=nu1,nl1-1
-          ! max,min horizontal bound in cluster around node n in every
+          ! max,min horizontal bound in cluster around node n in every 
           ! vertical layer
           ! nod_in_elem2D     --> elem indices of which node n is surrounded
-          ! nod_in_elem2D_num --> max number of surrounded elem
-          tvert_max(nz, n) = AUX(1,nz, nod_in_elem2D(1, n))
-          tvert_min(nz, n) = AUX(2,nz, nod_in_elem2D(1, n))
-          !$ACC LOOP SEQ
-          do elem=2,nod_in_elem2D_num(n)
-              tvert_max(nz, n) = dmax1(tvert_max(nz, n), AUX(1,nz, nod_in_elem2D(elem,n)))
-              tvert_min(nz, n) = dmin1(tvert_min(nz, n), AUX(2,nz, nod_in_elem2D(elem,n)))
-          end do
-          !$ACC END LOOP
-       end do
-       !$ACC END LOOP
-    end do
-#ifndef ENABLE_OPENACC
-!$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
-!$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
-    do n=1, myDim_nod2D
-       nu1 = ulevels_nod2D(n)
-       nl1 = nlevels_nod2D(n)
+          ! nod_in_elem2D_num --> max number of surrounded elem 
+          tvert_max(nz)= maxval(AUX(1,nz,nod_in_elem2D(1:nod_in_elem2D_num(n),n)))
+          tvert_min(nz)= minval(AUX(2,nz,nod_in_elem2D(1:nod_in_elem2D_num(n),n)))
+       end do            
        !___________________________________________________________________
-       ! calc max,min increment of surface layer with respect to low order
-       ! solution
-       fct_ttf_max(nu1,n)=tvert_max(nu1, n)-LO(nu1,n)
-       fct_ttf_min(nu1,n)=tvert_min(nu1, n)-LO(nu1,n)
-       ! calc max,min increment from nz-1:nz+1 with respect to low order
+       ! calc max,min increment of surface layer with respect to low order 
+       ! solution 
+       fct_ttf_max(nu1,n)=tvert_max(nu1)-LO(nu1,n)
+       fct_ttf_min(nu1,n)=tvert_min(nu1)-LO(nu1,n)     
+       ! calc max,min increment from nz-1:nz+1 with respect to low order 
        ! solution at layer nz
-       !$ACC LOOP VECTOR
-       do nz=nu1+1,nl1-2
-          fct_ttf_max(nz,n)=dmax1(tvert_max(nz-1, n), tvert_max(nz, n), tvert_max(nz+1, n))-LO(nz,n)
-          fct_ttf_min(nz,n)=dmin1(tvert_min(nz-1, n), tvert_min(nz, n), tvert_min(nz+1, n))-LO(nz,n)
+       do nz=nu1+1,nl1-2  
+          fct_ttf_max(nz,n)=maxval(tvert_max(nz-1:nz+1))-LO(nz,n)
+          fct_ttf_min(nz,n)=minval(tvert_min(nz-1:nz+1))-LO(nz,n)
        end do
-       !$ACC END LOOP
-       ! calc max,min increment of bottom layer -1 with respect to low order
-       ! solution
+       ! calc max,min increment of bottom layer -1 with respect to low order 
+       ! solution 
        nz=nl1-1
-       fct_ttf_max(nz,n)=tvert_max(nz, n)-LO(nz,n)
-       fct_ttf_min(nz,n)=tvert_min(nz, n)-LO(nz,n)
+       fct_ttf_max(nz,n)=tvert_max(nz)-LO(nz,n)
+       fct_ttf_min(nz,n)=tvert_min(nz)-LO(nz,n)  
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-   !$ACC END PARALLEL LOOP
-#endif
     !___________________________________________________________________________
     ! b1. Split positive and negative antidiffusive contributions
-    ! --> sum all positive (fct_plus), negative (fct_minus) antidiffusive
+    ! --> sum all positive (fct_plus), negative (fct_minus) antidiffusive 
     !     horizontal element and vertical node contribution to node n and layer nz
     !     see. R. Löhner et al. "finite element flux corrected transport (FEM-FCT)
     !     for the euler and navier stoke equation
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1, myDim_nod2D
         nu1 = ulevels_nod2D(n)
         nl1 = nlevels_nod2D(n)
-       !$ACC LOOP VECTOR
         do nz=nu1,nl1-1
             fct_plus(nz,n)=0._WP
             fct_minus(nz,n)=0._WP
         end do
-       !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     !Vertical
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1, myDim_nod2D
        nu1 = ulevels_nod2D(n)
        nl1 = nlevels_nod2D(n)
-       !$ACC LOOP VECTOR
        do nz=nu1,nl1-1
           fct_plus(nz,n) =fct_plus(nz,n) +(max(0.0_WP,adf_v(nz,n))+max(0.0_WP,-adf_v(nz+1,n)))
           fct_minus(nz,n)=fct_minus(nz,n)+(min(0.0_WP,adf_v(nz,n))+min(0.0_WP,-adf_v(nz+1,n)))
        end do
-       !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
 
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
     !Horizontal
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC PARALLEL LOOP GANG PRIVATE(enodes, el) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#else
-    !$ACC UPDATE SELF(fct_plus, fct_minus, adf_h)
-#endif
-#endif 
     do edge=1, myDim_edge2D
-       enodes(1:2)=edges(:,edge)
+       enodes(1:2)=edges(:,edge)   
        el=edge_tri(:,edge)
        nl1=nlevels(el(1))-1
        nu1=ulevels(el(1))
@@ -307,152 +220,91 @@ subroutine oce_tra_adv_fct(dt, ttf, lo, adf_h, adf_v, fct_ttf_min, fct_ttf_max,
        if (el(2)>0) then
           nl2=nlevels(el(2))-1
           nu2=ulevels(el(2))
-       end if
-
+       end if  
+        
        nl12 = max(nl1,nl2)
        nu12 = nu1
        if (nu2>0) nu12 = min(nu1,nu2)
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
        call omp_set_lock(partit%plock(enodes(1)))
 #else
 !$OMP ORDERED
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-       !$ACC LOOP VECTOR
-#endif
 #endif
        do nz=nu12, nl12
-#if !defined(DISABLE_OPENACC_ATOMICS)
-          !$ACC ATOMIC UPDATE
-#endif
           fct_plus (nz,enodes(1))=fct_plus (nz,enodes(1)) + max(0.0_WP, adf_h(nz,edge))
-#if !defined(DISABLE_OPENACC_ATOMICS)
-          !$ACC ATOMIC UPDATE
-#endif
           fct_minus(nz,enodes(1))=fct_minus(nz,enodes(1)) + min(0.0_WP, adf_h(nz,edge))
-
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
        end do
        call omp_unset_lock(partit%plock(enodes(1)))
        call omp_set_lock  (partit%plock(enodes(2)))
-       do nz=nu12, nl12
-#endif
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-          !$ACC ATOMIC UPDATE
-#endif
+       do nz=nu12, nl12  
 #endif
           fct_plus (nz,enodes(2))=fct_plus (nz,enodes(2)) + max(0.0_WP,-adf_h(nz,edge))
-#if !defined(DISABLE_OPENACC_ATOMICS)
-          !$ACC ATOMIC UPDATE
-#endif
-          fct_minus(nz,enodes(2))=fct_minus(nz,enodes(2)) + min(0.0_WP,-adf_h(nz,edge))
+          fct_minus(nz,enodes(2))=fct_minus(nz,enodes(2)) + min(0.0_WP,-adf_h(nz,edge)) 
        end do
-#if !defined(DISABLE_OPENACC_ATOMICS)
-       !$ACC END LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
        call omp_unset_lock(partit%plock(enodes(2)))
 #else
 !$OMP END ORDERED
-#endif
 #endif
     end do
-#ifndef ENABLE_OPENACC
-!$OMP END DO
-#else
-#if !defined(DISABLE_OPENACC_ATOMICS)
-    !$ACC END PARALLEL LOOP
-#else
-    !$ACC UPDATE DEVICE(fct_plus, fct_minus)
-#endif
-#endif
+!$OMP END DO 
     !___________________________________________________________________________
     ! b2. Limiting factors
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1,myDim_nod2D
         nu1=ulevels_nod2D(n)
         nl1=nlevels_nod2D(n)
-        !$ACC LOOP VECTOR
         do nz=nu1,nl1-1
             flux=fct_plus(nz,n)*dt/areasvol(nz,n)+flux_eps
             fct_plus(nz,n)=min(1.0_WP,fct_ttf_max(nz,n)/flux)
             flux=fct_minus(nz,n)*dt/areasvol(nz,n)-flux_eps
             fct_minus(nz,n)=min(1.0_WP,fct_ttf_min(nz,n)/flux)
         end do
-        !$ACC END LOOP
-    end do
-#ifndef ENABLE_OPENACC
+    end do 
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     ! fct_minus and fct_plus must be known to neighbouring PE
 !$OMP MASTER
-    call exchange_nod(fct_plus, fct_minus, partit, luse_g2g = .true.)
+    call exchange_nod(fct_plus, fct_minus, partit)
 !$OMP END MASTER
 !$OMP BARRIER
-
     !___________________________________________________________________________
-    ! b3. Limiting
+    ! b3. Limiting   
     !Vertical
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do n=1, myDim_nod2D
         nu1=ulevels_nod2D(n)
         nl1=nlevels_nod2D(n)
-
+        
         !_______________________________________________________________________
         nz=nu1
         ae=1.0_WP
         flux=adf_v(nz,n)
-        if(flux>=0.0_WP) then
+        if(flux>=0.0_WP) then 
             ae=min(ae,fct_plus(nz,n))
         else
             ae=min(ae,fct_minus(nz,n))
         end if
-        adf_v(nz,n)=ae*adf_v(nz,n)
-
+        adf_v(nz,n)=ae*adf_v(nz,n) 
+        
         !_______________________________________________________________________
-        !$ACC LOOP VECTOR
         do nz=nu1+1,nl1-1
             ae=1.0_WP
             flux=adf_v(nz,n)
-            if(flux>=0._WP) then
+            if(flux>=0._WP) then 
                 ae=min(ae,fct_minus(nz-1,n))
                 ae=min(ae,fct_plus(nz,n))
             else
                 ae=min(ae,fct_plus(nz-1,n))
                 ae=min(ae,fct_minus(nz,n))
-            end if
+            end if            
             adf_v(nz,n)=ae*adf_v(nz,n)
         end do
-        !$ACC END LOOP
-    ! the bottom flux is always zero
+    ! the bottom flux is always zero 
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
     !Horizontal
-#ifndef ENABLE_OPENACC
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG PRIVATE(enodes, el) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do edge=1, myDim_edge2D
         enodes(1:2)=edges(:,edge)
         el=edge_tri(:,edge)
@@ -463,17 +315,16 @@ subroutine oce_tra_adv_fct(dt, ttf, lo, adf_h, adf_v, fct_ttf_min, fct_ttf_max,
         if(el(2)>0) then
             nu2=ulevels(el(2))
             nl2=nlevels(el(2))-1
-        end if
-
+        end if  
+        
         nl12 = max(nl1,nl2)
         nu12 = nu1
         if (nu2>0) nu12 = min(nu1,nu2)
-
-        !$ACC LOOP VECTOR
+        
         do nz=nu12, nl12
             ae=1.0_WP
             flux=adf_h(nz,edge)
-
+            
             if(flux>=0._WP) then
                 ae=min(ae,fct_plus(nz,enodes(1)))
                 ae=min(ae,fct_minus(nz,enodes(2)))
@@ -481,20 +332,10 @@ subroutine oce_tra_adv_fct(dt, ttf, lo, adf_h, adf_v, fct_ttf_min, fct_ttf_max,
                 ae=min(ae,fct_minus(nz,enodes(1)))
                 ae=min(ae,fct_plus(nz,enodes(2)))
             endif
-
+            
             adf_h(nz,edge)=ae*adf_h(nz,edge)
         end do
-        !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
-#else
-    !$ACC END PARALLEL LOOP
-#endif
-
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL
-#else
-!$ACC END DATA
-#endif
 end subroutine oce_tra_adv_fct
diff --git a/src/oce_adv_tra_hor.F90 b/src/oce_adv_tra_hor.F90
index 91564059..c7e209df 100644
--- a/src/oce_adv_tra_hor.F90
+++ b/src/oce_adv_tra_hor.F90
@@ -3,7 +3,7 @@
 module oce_adv_tra_hor_interfaces
   interface
 ! (low order upwind)
-! returns flux given at edges which contributes with
+! returns flux given at edges which contributes with 
 ! plus sign into 1st. node and with the minus sign into the 2nd node
 ! IF init_zero=.TRUE.  : flux will be set to zero before computation
 ! IF init_zero=.FALSE. : flux=flux-input flux
@@ -22,7 +22,7 @@ module oce_adv_tra_hor_interfaces
     end subroutine
 !===============================================================================
 ! MUSCL
-! returns flux given at edges which contributes with
+! returns flux given at edges which contributes with 
 ! plus sign into 1st. node and with the minus sign into the 2nd node
 ! IF init_zero=.TRUE.  : flux will be set to zero before computation
 ! IF init_zero=.FALSE. : flux=flux-input flux
@@ -32,7 +32,7 @@ module oce_adv_tra_hor_interfaces
       USE MOD_PARTIT
       USE MOD_PARSUP
       type(t_partit),intent(in), target :: partit
-      type(t_mesh),  intent(in), target :: mesh
+      type(t_mesh),  intent(in), target :: mesh    
       real(kind=WP), intent(in)         :: num_ord    ! num_ord is the fraction of fourth-order contribution in the solution
       real(kind=WP), intent(in)         :: ttf(   mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
       real(kind=WP), intent(in)         :: vel(2, mesh%nl-1, partit%myDim_elem2D+partit%eDim_elem2D)
@@ -48,7 +48,7 @@ module oce_adv_tra_hor_interfaces
       USE MOD_PARTIT
       USE MOD_PARSUP
       type(t_partit),intent(inout), target :: partit
-      type(t_mesh),  intent(in), target :: mesh
+      type(t_mesh),  intent(in), target :: mesh    
       real(kind=WP), intent(in)         :: num_ord    ! num_ord is the fraction of fourth-order contribution in the solution
       real(kind=WP), intent(in)         :: ttf(   mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
       real(kind=WP), intent(in)         :: vel(2, mesh%nl-1, partit%myDim_elem2D+partit%eDim_elem2D)
@@ -68,7 +68,7 @@ subroutine adv_tra_hor_upw1(vel, ttf, partit, mesh, flux, o_init_zero)
     use g_comm_auto
     implicit none
     type(t_partit),intent(in), target :: partit
-    type(t_mesh),  intent(in), target :: mesh
+    type(t_mesh),  intent(in), target :: mesh    
     real(kind=WP), intent(in)         :: ttf(   mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
     real(kind=WP), intent(in)         :: vel(2, mesh%nl-1, partit%myDim_elem2D+partit%eDim_elem2D)
     real(kind=WP), intent(inout)      :: flux(  mesh%nl-1, partit%myDim_edge2D)
@@ -89,52 +89,38 @@ subroutine adv_tra_hor_upw1(vel, ttf, partit, mesh, flux, o_init_zero)
        l_init_zero=o_init_zero
     end if
     if (l_init_zero) then
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-       !$ACC PARALLEL LOOP GANG VECTOR COLLAPSE(2) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
        do edge=1, myDim_edge2D
-          do nz=1, mesh%nl-1
-             flux(nz,edge)=0.0_WP
-          end do
+          flux(:,edge)=0.0_WP
        end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-       !$ACC END PARALLEL LOOP
-#endif
     end if
 
     ! The result is the low-order solution horizontal fluxes
     ! They are put into flux
     !___________________________________________________________________________
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(edge, deltaX1, deltaY1, deltaX2, deltaY2, &
 !$OMP                       a, vflux, el, enodes, nz, nu12, nl12, nl1, nl2, nu1, nu2)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG PRIVATE(enodes, el) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do edge=1, myDim_edge2D
         ! local indice of nodes that span up edge ed
-        enodes=edges(:,edge)
-
+        enodes=edges(:,edge)      
+        
         ! local index of element that contribute to edge
-        el=edge_tri(:,edge)
-
+        el=edge_tri(:,edge)        
+        
         ! number of layers -1 at elem el(1)
         nl1=nlevels(el(1))-1
-
+        
         ! index off surface layer in case of cavity !=1
         nu1=ulevels(el(1))
-
-        ! edge_cross_dxdy(1:2,ed)... dx,dy distance from element centroid el(1) to
+        
+        ! edge_cross_dxdy(1:2,ed)... dx,dy distance from element centroid el(1) to 
         ! center of edge --> needed to calc flux perpedicular to edge from elem el(1)
         deltaX1=edge_cross_dxdy(1,edge)
         deltaY1=edge_cross_dxdy(2,edge)
-        a=r_earth*elem_cos(el(1))
-
+        a=r_earth*elem_cos(el(1)) 
+        
         !_______________________________________________________________________
         ! same parameter but for other element el(2) that contributes to edge ed
         ! if el(2)==0 than edge is boundary edge
@@ -147,28 +133,27 @@ subroutine adv_tra_hor_upw1(vel, ttf, partit, mesh, flux, o_init_zero)
             nl2=nlevels(el(2))-1
             nu2=ulevels(el(2))
             a=0.5_WP*(a+r_earth*elem_cos(el(2)))
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
-        ! nl12 ... minimum number of layers -1 between element el(1) & el(2) that
+        ! nl12 ... minimum number of layers -1 between element el(1) & el(2) that 
         ! contribute to edge ed
-        ! nu12 ... upper index of layers between element el(1) & el(2) that
+        ! nu12 ... upper index of layers between element el(1) & el(2) that 
         ! contribute to edge ed
         ! be carefull !!! --> if ed is a boundary edge than el(1)~=0 and el(2)==0
         !                     that means nl1>0, nl2==0, n2=min(nl1,nl2)=0 !!!
         nl12=min(nl1,nl2)
-        nu12=max(nu1,nu2)
-
+        nu12=max(nu1,nu2)        
+        
         !_______________________________________________________________________
         ! (A) goes only into this loop when the edge has only facing element
-        ! el(1) --> so the edge is a boundary edge --> this is for ocean
+        ! el(1) --> so the edge is a boundary edge --> this is for ocean 
         ! surface in case of cavity
-        !$ACC LOOP VECTOR
-        do nz=nu1, nu12-1
+        do nz=nu1, nu12-1              
            !____________________________________________________________________
            ! volume flux across the segments
-           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))
-
+           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) 
+           
            !____________________________________________________________________
            ! 1st. low order upwind solution
            flux(nz, edge)=-0.5_WP*(                                                     &
@@ -176,54 +161,48 @@ subroutine adv_tra_hor_upw1(vel, ttf, partit, mesh, flux, o_init_zero)
                          ttf(nz, enodes(2))*(vflux-abs(vflux))  &
                          )-flux(nz, edge)
         end do
-        !$ACC END LOOP
-
+        
         !_______________________________________________________________________
         ! (B) goes only into this loop when the edge has only facing elemenmt
-        ! el(2) --> so the edge is a boundary edge --> this is for ocean
+        ! el(2) --> so the edge is a boundary edge --> this is for ocean 
         ! surface in case of cavity
-        if (nu2 > 0) then
-            !$ACC LOOP VECTOR
+        if (nu2 > 0) then 
             do nz=nu2, nu12-1
                 !___________________________________________________________
                 ! volume flux across the segments
                 vflux=(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
-
+                
                 !___________________________________________________________
                 ! 1st. low order upwind solution
                 flux(nz, edge)=-0.5_WP*(                                           &
                             ttf(nz, enodes(1))*(vflux+abs(vflux))+ &
                             ttf(nz, enodes(2))*(vflux-abs(vflux)))-flux(nz, edge)
             end do
-            !$ACC END LOOP
-        end if
-
+        end if     
+        
         !_______________________________________________________________________
         ! (C) Both segments
         ! loop over depth layers from top (nu12) to nl12
-        ! be carefull !!! --> if ed is a boundary edge, el(2)==0 than nl12=0 so
+        ! be carefull !!! --> if ed is a boundary edge, el(2)==0 than nl12=0 so 
         !                     you wont enter in this loop
-        !$ACC LOOP VECTOR
         do nz=nu12, nl12
             !___________________________________________________________________
             ! 1st. low order upwind solution
             ! here already assumed that ed is NOT! a boundary edge so el(2) should exist
             vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) &
                   +(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
-
+                  
             flux(nz, edge)=-0.5_WP*(                                                     &
                          ttf(nz, enodes(1))*(vflux+abs(vflux))+ &
                          ttf(nz, enodes(2))*(vflux-abs(vflux)))-flux(nz, edge)
         end do
-        !$ACC END LOOP
-
+        
         !_______________________________________________________________________
         ! (D) remaining segments on the left or on the right
-        !$ACC LOOP VECTOR
-        do nz=nl12+1, nl1
+        do nz=nl12+1, nl1              
            !____________________________________________________________________
            ! volume flux across the segments
-           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))
+           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))                 
            !____________________________________________________________________
            ! 1st. low order upwind solution
            flux(nz, edge)=-0.5_WP*(                                                     &
@@ -231,11 +210,9 @@ subroutine adv_tra_hor_upw1(vel, ttf, partit, mesh, flux, o_init_zero)
                          ttf(nz, enodes(2))*(vflux-abs(vflux))  &
                          )-flux(nz, edge)
         end do
-        !$ACC END LOOP
-
+        
         !_______________________________________________________________________
         ! (E) remaining segments on the left or on the right
-        !$ACC LOOP VECTOR
         do nz=nl12+1, nl2
                 !_______________________________________________________________
                 ! volume flux across the segments
@@ -246,14 +223,9 @@ subroutine adv_tra_hor_upw1(vel, ttf, partit, mesh, flux, o_init_zero)
                              ttf(nz, enodes(1))*(vflux+abs(vflux))+ &
                              ttf(nz, enodes(2))*(vflux-abs(vflux)))-flux(nz, edge)
         end do
-        !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-    !$ACC END PARALLEL LOOP
-#endif
 end subroutine adv_tra_hor_upw1
 !
 !
@@ -266,7 +238,7 @@ subroutine adv_tra_hor_muscl(vel, ttf, partit, mesh, num_ord, flux, edge_up_dn_g
     use g_comm_auto
     implicit none
     type(t_partit),intent(in), target :: partit
-    type(t_mesh),  intent(in), target :: mesh
+    type(t_mesh),  intent(in), target :: mesh    
     real(kind=WP), intent(in)         :: num_ord    ! num_ord is the fraction of fourth-order contribution in the solution
     real(kind=WP), intent(in)         :: ttf(   mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
     real(kind=WP), intent(in)         :: vel(2, mesh%nl-1, partit%myDim_elem2D+partit%eDim_elem2D)
@@ -307,23 +279,23 @@ subroutine adv_tra_hor_muscl(vel, ttf, partit, mesh, num_ord, flux, edge_up_dn_g
 !$OMP DO
     do edge=1, myDim_edge2D
         ! local indice of nodes that span up edge ed
-        enodes=edges(:,edge)
-
+        enodes=edges(:,edge)          
+        
         ! local index of element that contribute to edge
-        el=edge_tri(:,edge)
-
+        el=edge_tri(:,edge)        
+        
         ! number of layers -1 at elem el(1)
-        nl1=nlevels(el(1))-1
-
+        nl1=nlevels(el(1))-1   
+        
         ! index off surface layer in case of cavity !=1
         nu1=ulevels(el(1))
-
-        ! edge_cross_dxdy(1:2,ed)... dx,dy distance from element centroid el(1) to
+        
+        ! edge_cross_dxdy(1:2,ed)... dx,dy distance from element centroid el(1) to 
         ! center of edge --> needed to calc flux perpedicular to edge from elem el(1)
         deltaX1=edge_cross_dxdy(1,edge)
         deltaY1=edge_cross_dxdy(2,edge)
-        a=r_earth*elem_cos(el(1))
-
+        a=r_earth*elem_cos(el(1))        
+        
         !_______________________________________________________________________
         ! same parameter but for other element el(2) that contributes to edge ed
         ! if el(2)==0 than edge is boundary edge
@@ -336,88 +308,88 @@ subroutine adv_tra_hor_muscl(vel, ttf, partit, mesh, num_ord, flux, edge_up_dn_g
             nl2=nlevels(el(2))-1
             nu2=ulevels(el(2))
             a=0.5_WP*(a+r_earth*elem_cos(el(2)))
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
-        ! n2 ... minimum number of layers -1 between element el(1) & el(2) that
+        ! n2 ... minimum number of layers -1 between element el(1) & el(2) that 
         ! contribute to edge ed
-        ! nu12 ... upper index of layers between element el(1) & el(2) that
+        ! nu12 ... upper index of layers between element el(1) & el(2) that 
         ! contribute to edge ed
         ! be carefull !!! --> if ed is a boundary edge than el(1)~=0 and el(2)==0
         !                     that means nl1>0, nl2==0, n2=min(nl1,nl2)=0 !!!
-        nl12=min(nl1,nl2)
+        nl12=min(nl1,nl2)        
         nu12=max(nu1,nu2)
-
+        
         !_______________________________________________________________________
         ! (A) goes only into this loop when the edge has only facing element
-        ! el(1) --> so the edge is a boundary edge --> this is for ocean
+        ! el(1) --> so the edge is a boundary edge --> this is for ocean 
         ! surface in case of cavity
         do nz=nu1, nu12-1
            c_lo(1)=real(max(sign(1, nboundary_lay(enodes(1))-nz), 0),WP)
            c_lo(2)=real(max(sign(1, nboundary_lay(enodes(2))-nz), 0),WP)
-
+           
            !____________________________________________________________________
            Tmean2=ttf(nz, enodes(2))- &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP*c_lo(2)
-
+                
            Tmean1=ttf(nz, enodes(1))+ &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)
-
+                  
            !____________________________________________________________________
            ! volume flux across the segments
-           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))
+           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) 
            cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
            flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-
+        
         !_______________________________________________________________________
         ! (B) goes only into this loop when the edge has only facing elemenmt
-        ! el(2) --> so the edge is a boundary edge --> this is for ocean
+        ! el(2) --> so the edge is a boundary edge --> this is for ocean 
         ! surface in case of cavity
-        if (nu2 > 0) then
+        if (nu2 > 0) then 
             do nz=nu2, nu12-1
                 c_lo(1)=real(max(sign(1, nboundary_lay(enodes(1))-nz), 0),WP)
                 c_lo(2)=real(max(sign(1, nboundary_lay(enodes(2))-nz), 0),WP)
-
+                
                 !_______________________________________________________________
                 Tmean2=ttf(nz, enodes(2))- &
                         (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                         edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                         edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP*c_lo(2)
-
+                        
                 Tmean1=ttf(nz, enodes(1))+ &
                         (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                         edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
-                        edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)
-
+                        edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)  
+                        
                 !_______________________________________________________________
                 ! volume flux across the segments
                 vflux=(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
                 cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
                 flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
             end do
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
         ! (C) Both segments
         ! loop over depth layers from top to n2
-        ! be carefull !!! --> if ed is a boundary edge, el(2)==0 than n2=0 so
+        ! be carefull !!! --> if ed is a boundary edge, el(2)==0 than n2=0 so 
         !                     you wont enter in this loop
         do nz=nu12, nl12
             c_lo(1)=real(max(sign(1, nboundary_lay(enodes(1))-nz), 0),WP)
             c_lo(2)=real(max(sign(1, nboundary_lay(enodes(2))-nz), 0),WP)
-
+            
            !___________________________________________________________________
            ! MUSCL-type reconstruction
            ! check if upwind or downwind triagle is necessary
            !
            ! cross product between velocity vector and cross vector edge-elem-center
            ! cross product > 0 --> angle vec_v and (dx,dy) --> [0   180] --> upwind triangle
-           ! cross product < 0 --> angle vec_v and (dx,dy) --> [180 360] --> downwind triangle
+           ! cross product < 0 --> angle vec_v and (dx,dy) --> [180 360] --> downwind triangle 
            !
            !       o                  o      !     o                  o
            !      / \                / \     !    / \                / \
@@ -426,20 +398,20 @@ subroutine adv_tra_hor_muscl(vel, ttf, partit, mesh, num_ord, flux, edge_up_dn_g
            !   o-------o----+---->o-------o  ! o-------o----+---->o-------o
            !           1   /      2          !         1     \vec_v
            !              /vec_v             !                \
-           !   --> downwind triangle         ! --> upwind triangle
-           !
+           !   --> downwind triangle         ! --> upwind triangle 
+           !  
            !  edge_up_dn_grad(1,nz,edge) ... gradTR_x upwind
            !  edge_up_dn_grad(2,nz,edge) ... gradTR_x downwind
            !  edge_up_dn_grad(3,nz,edge) ... gradTR_y upwind
            !  edge_up_dn_grad(4,nz,edge) ... gradTR_y downwind
-
+           
            !___________________________________________________________________
-           ! use downwind triangle to interpolate Tracer to edge center with
+           ! use downwind triangle to interpolate Tracer to edge center with 
            ! fancy scheme --> Linear upwind reconstruction
            ! T_n+0.5 = T_n+1 - 1/2*deltax*GRADIENT
            ! --> GRADIENT = 2/3 GRAD_edgecenter + 1/3 GRAD_downwindtri
            ! T_n+0.5 = T_n+1 - 2/6*(T_n+1-T_n) + 1/6*gradT_down
-           ! --> edge_up_dn_grad ... contains already elemental tracer gradient
+           ! --> edge_up_dn_grad ... contains already elemental tracer gradient 
            !     of up and dn wind triangle
            ! --> Tmean2 ... edge center interpolated Tracer using tracer
            !     gradient info from upwind triangle
@@ -447,8 +419,8 @@ subroutine adv_tra_hor_muscl(vel, ttf, partit, mesh, num_ord, flux, edge_up_dn_g
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP*c_lo(2)
-
-            ! use upwind triangle to interpolate Tracer to edge center with
+            
+            ! use upwind triangle to interpolate Tracer to edge center with 
             ! fancy scheme --> Linear upwind reconstruction
             ! T_n+0.5 = T_n + 1/2*deltax*GRADIENT
             ! --> GRADIENT = 2/3 GRAD_edgecenter + 1/3 GRAD_downwindtri
@@ -458,78 +430,78 @@ subroutine adv_tra_hor_muscl(vel, ttf, partit, mesh, num_ord, flux, edge_up_dn_g
             Tmean1=ttf(nz, enodes(1))+ &
                    (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                    edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
-                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)
-
+                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)   
+                   
             !___________________________________________________________________
             ! volume flux along the edge segment ed
             ! netto volume flux along segment that comes from edge node 1 and 2
             !
-            !
+            !                         
             !                         C1 (centroid el(1)) --> (u1,v1)
             !                         x
-            !                         ^
-            !               (dx1,dy1) |
+            !                         ^ 
+            !               (dx1,dy1) |       
             !                         |---> vec_n1 (dy1,-dx1)--> project vec_u1 onto vec_n1 --> -v1*dx1+u1*dy1 -->
             !                         |                                                                          |
             !    enodes(1) o----------O---------o enodes(2)                                                      |-> calculate volume flux out of/in
             !          vflux_________/|                                                                          |   the volume of enode1(enode2) through
             !                         |---> vec_n2 (dy2,-dx2)--> project vec_u2 onto vec_n2 --> -v2*dx2+u2*dy2 -->   sections of dx1,dy1 and dx2,dy2
-            !               (dx2,dy2) |                                                                              --> vflux
+            !               (dx2,dy2) |                                                                              --> vflux 
             !                         v
             !                         x
-            !                         C2 (centroid el(2)) --> (u2,v2)
-
+            !                         C2 (centroid el(2)) --> (u2,v2)   
+            
             ! here already assumed that ed is NOT! a boundary edge so el(2) should exist
             vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) &
                   +(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
-
+                        
             !___________________________________________________________________
             ! (1-num_ord) is done with 3rd order upwind
             cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
             flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-
+        
         !_______________________________________________________________________
         ! (D) remaining segments on the left or on the right
         do nz=nl12+1, nl1
            c_lo(1)=real(max(sign(1, nboundary_lay(enodes(1))-nz), 0),WP)
            c_lo(2)=real(max(sign(1, nboundary_lay(enodes(2))-nz), 0),WP)
-
+           
            !____________________________________________________________________
            Tmean2=ttf(nz, enodes(2))- &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP*c_lo(2)
-
+                
            Tmean1=ttf(nz, enodes(1))+ &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)
-
+                  
            !____________________________________________________________________
            ! volume flux across the segments
-           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))
+           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) 
            cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
            flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-
+        
         !_______________________________________________________________________
         ! (E) remaining segments on the left or on the right
         do nz=nl12+1, nl2
            c_lo(1)=real(max(sign(1, nboundary_lay(enodes(1))-nz), 0),WP)
            c_lo(2)=real(max(sign(1, nboundary_lay(enodes(2))-nz), 0),WP)
-
+           
            !____________________________________________________________________
            Tmean2=ttf(nz, enodes(2))- &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP*c_lo(2)
-
+                
            Tmean1=ttf(nz, enodes(1))+ &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
-                  edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)
-
+                  edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP*c_lo(1)  
+                  
            !____________________________________________________________________
            ! volume flux across the segments
            vflux=(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
@@ -551,7 +523,7 @@ end subroutine adv_tra_hor_muscl
     use g_comm_auto
     implicit none
     type(t_partit),intent(inout), target :: partit
-    type(t_mesh),  intent(in), target :: mesh
+    type(t_mesh),  intent(in), target :: mesh    
     real(kind=WP), intent(in)         :: num_ord    ! num_ord is the fraction of fourth-order contribution in the solution
     real(kind=WP), intent(in)         :: ttf(   mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
     real(kind=WP), intent(in)         :: vel(2, mesh%nl-1, partit%myDim_elem2D+partit%eDim_elem2D)
@@ -575,50 +547,38 @@ end subroutine adv_tra_hor_muscl
        l_init_zero=o_init_zero
     end if
     if (l_init_zero) then
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-       !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
        do edge=1, myDim_edge2D
           flux(:,edge)=0.0_WP
        end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-       !$ACC END PARALLEL LOOP
-#endif
     end if
 
     ! The result is the low-order solution horizontal fluxes
     ! They are put into flux
     !___________________________________________________________________________
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(edge, deltaX1, deltaY1, deltaX2, deltaY2, Tmean1, Tmean2, cHO, &
 !$OMP                                     a, vflux, el, enodes, nz, nu12, nl12, nl1, nl2, nu1, nu2)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG PRIVATE(enodes, el) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
     do edge=1, myDim_edge2D
         ! local indice of nodes that span up edge ed
-        enodes=edges(:,edge)
-
+        enodes=edges(:,edge)  
+        
         ! local index of element that contribute to edge
-        el=edge_tri(:,edge)
-
+        el=edge_tri(:,edge)    
+        
         ! number of layers -1 at elem el(1)
-        nl1=nlevels(el(1))-1
-
+        nl1=nlevels(el(1))-1      
+        
         ! index off surface layer in case of cavity !=1
         nu1=ulevels(el(1))
-
-        ! edge_cross_dxdy(1:2,ed)... dx,dy distance from element centroid el(1) to
+        
+        ! edge_cross_dxdy(1:2,ed)... dx,dy distance from element centroid el(1) to 
         ! center of edge --> needed to calc flux perpedicular to edge from elem el(1)
         deltaX1=edge_cross_dxdy(1,edge)
         deltaY1=edge_cross_dxdy(2,edge)
-        a=r_earth*elem_cos(el(1))
-
+        a=r_earth*elem_cos(el(1))        
+        
         !_______________________________________________________________________
         ! same parameter but for other element el(2) that contributes to edge ed
         ! if el(2)==0 than edge is boundary edge
@@ -631,56 +591,53 @@ end subroutine adv_tra_hor_muscl
             nl2=nlevels(el(2))-1
             nu2=ulevels(el(2))
             a=0.5_WP*(a+r_earth*elem_cos(el(2)))
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
-        ! n2 ... minimum number of layers -1 between element el(1) & el(2) that
+        ! n2 ... minimum number of layers -1 between element el(1) & el(2) that 
         ! contribute to edge ed
-        ! nu12 ... upper index of layers between element el(1) & el(2) that
+        ! nu12 ... upper index of layers between element el(1) & el(2) that 
         ! contribute to edge ed
         ! be carefull !!! --> if ed is a boundary edge than el(1)~=0 and el(2)==0
         !                     that means nl1>0, nl2==0, n2=min(nl1,nl2)=0 !!!
-        nl12=min(nl1,nl2)
-        nu12=max(nu1,nu2)
-
+        nl12=min(nl1,nl2) 
+        nu12=max(nu1,nu2) 
+        
         !_______________________________________________________________________
         ! (A) goes only into this loop when the edge has only facing element
-        ! el(1) --> so the edge is a boundary edge --> this is for ocean
+        ! el(1) --> so the edge is a boundary edge --> this is for ocean 
         ! surface in case of cavity
-        !$ACC LOOP VECTOR
         do nz=nu1, nu12-1
            !____________________________________________________________________
            Tmean2=ttf(nz, enodes(2))- &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP
-
+                
            Tmean1=ttf(nz, enodes(1))+ &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP
-
+                  
            !____________________________________________________________________
            ! volume flux across the segments
-           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))
+           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) 
            cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
            flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-        !$ACC END LOOP
-
+        
         !_______________________________________________________________________
         ! (B) goes only into this loop when the edge has only facing elemenmt
-        ! el(2) --> so the edge is a boundary edge --> this is for ocean
+        ! el(2) --> so the edge is a boundary edge --> this is for ocean 
         ! surface in case of cavity
-        if (nu2 > 0) then
-            !$ACC LOOP VECTOR
+        if (nu2 > 0) then 
             do nz=nu2,nu12-1
             !___________________________________________________________________
             Tmean2=ttf(nz, enodes(2))- &
                     (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                     edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                     edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP
-
+                    
             Tmean1=ttf(nz, enodes(1))+ &
                     (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                     edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
@@ -691,15 +648,13 @@ end subroutine adv_tra_hor_muscl
             cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
             flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
             end do
-            !$ACC END LOOP
         end if
-
+        
         !_______________________________________________________________________
         ! (C) Both segments
         ! loop over depth layers from top to n2
-        ! be carefull !!! --> if ed is a boundary edge, el(2)==0 than n2=0 so
+        ! be carefull !!! --> if ed is a boundary edge, el(2)==0 than n2=0 so 
         !                     you wont enter in this loop
-        !$ACC LOOP VECTOR
         do nz=nu12, nl12
            !___________________________________________________________________
            ! MUSCL-type reconstruction
@@ -707,7 +662,7 @@ end subroutine adv_tra_hor_muscl
            !
            ! cross product between velocity vector and cross vector edge-elem-center
            ! cross product > 0 --> angle vec_v and (dx,dy) --> [0   180] --> upwind triangle
-           ! cross product < 0 --> angle vec_v and (dx,dy) --> [180 360] --> downwind triangle
+           ! cross product < 0 --> angle vec_v and (dx,dy) --> [180 360] --> downwind triangle 
            !
            !       o                  o      !     o                  o
            !      / \                / \     !    / \                / \
@@ -716,20 +671,20 @@ end subroutine adv_tra_hor_muscl
            !   o-------o----+---->o-------o  ! o-------o----+---->o-------o
            !           1   /      2          !         1     \vec_v
            !              /vec_v             !                \
-           !   --> downwind triangle         ! --> upwind triangle
-           !
+           !   --> downwind triangle         ! --> upwind triangle 
+           !  
            !  edge_up_dn_grad(1,nz,edge) ... gradTR_x upwind
            !  edge_up_dn_grad(2,nz,edge) ... gradTR_x downwind
            !  edge_up_dn_grad(3,nz,edge) ... gradTR_y upwind
            !  edge_up_dn_grad(4,nz,edge) ... gradTR_y downwind
-
+           
            !___________________________________________________________________
-           ! use downwind triangle to interpolate Tracer to edge center with
+           ! use downwind triangle to interpolate Tracer to edge center with 
            ! fancy scheme --> Linear upwind reconstruction
            ! T_n+0.5 = T_n+1 - 1/2*deltax*GRADIENT
            ! --> GRADIENT = 2/3 GRAD_edgecenter + 1/3 GRAD_downwindtri
            ! T_n+0.5 = T_n+1 - 2/6*(T_n+1-T_n) + 1/6*gradT_down
-           ! --> edge_up_dn_grad ... contains already elemental tracer gradient
+           ! --> edge_up_dn_grad ... contains already elemental tracer gradient 
            !     of up and dn wind triangle
            ! --> Tmean2 ... edge center interpolated Tracer using tracer
            !     gradient info from upwind triangle
@@ -737,8 +692,8 @@ end subroutine adv_tra_hor_muscl
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP
-
-            ! use upwind triangle to interpolate Tracer to edge center with
+            
+            ! use upwind triangle to interpolate Tracer to edge center with 
             ! fancy scheme --> Linear upwind reconstruction
             ! T_n+0.5 = T_n + 1/2*deltax*GRADIENT
             ! --> GRADIENT = 2/3 GRAD_edgecenter + 1/3 GRAD_downwindtri
@@ -753,82 +708,74 @@ end subroutine adv_tra_hor_muscl
             ! volume flux along the edge segment ed
             ! netto volume flux along segment that comes from edge node 1 and 2
             !
-            !
+            !                         
             !                         C1 (centroid el(1)) --> (u1,v1)
             !                         x
-            !                         ^
-            !               (dx1,dy1) |
+            !                         ^ 
+            !               (dx1,dy1) |       
             !                         |---> vec_n1 (dy1,-dx1)--> project vec_u1 onto vec_n1 --> -v1*dx1+u1*dy1 -->
             !                         |                                                                          |
             !    enodes(1) o----------O---------o enodes(2)                                                      |-> calculate volume flux out of/in
             !          vflux_________/|                                                                          |   the volume of enode1(enode2) through
             !                         |---> vec_n2 (dy2,-dx2)--> project vec_u2 onto vec_n2 --> -v2*dx2+u2*dy2 -->   sections of dx1,dy1 and dx2,dy2
-            !               (dx2,dy2) |                                                                              --> vflux
+            !               (dx2,dy2) |                                                                              --> vflux 
             !                         v
             !                         x
-            !                         C2 (centroid el(2)) --> (u2,v2)
-
+            !                         C2 (centroid el(2)) --> (u2,v2)   
+            
             ! here already assumed that ed is NOT! a boundary edge so el(2) should exist
             vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) &
                   +(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
-
+                        
             !___________________________________________________________________
             ! (1-num_ord) is done with 3rd order upwind
             cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
             flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-        !$ACC END LOOP
-
+        
         !_______________________________________________________________________
         ! (D) remaining segments on the left or on the right
-        !$ACC LOOP VECTOR
         do nz=nl12+1, nl1
            !____________________________________________________________________
            Tmean2=ttf(nz, enodes(2))- &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP
-
+                
            Tmean1=ttf(nz, enodes(1))+ &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP
-
+                  
            !____________________________________________________________________
            ! volume flux across the segments
-           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1))
+           vflux=(-VEL(2,nz,el(1))*deltaX1 + VEL(1,nz,el(1))*deltaY1)*helem(nz,el(1)) 
            cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
            flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-        !$ACC END LOOP
-
+        
         !_______________________________________________________________________
         ! (E) remaining segments on the left or on the right
-        !$ACC LOOP VECTOR
         do nz=nl12+1, nl2
            !____________________________________________________________________
            Tmean2=ttf(nz, enodes(2))- &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(2,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(4,nz,edge))/6.0_WP
-
+                
            Tmean1=ttf(nz, enodes(1))+ &
                   (2.0_WP*(ttf(nz, enodes(2))-ttf(nz,enodes(1)))+ &
                   edge_dxdy(1,edge)*a*edge_up_dn_grad(1,nz,edge)+ &
                   edge_dxdy(2,edge)*r_earth*edge_up_dn_grad(3,nz,edge))/6.0_WP
-
+                  
            !____________________________________________________________________
            ! volume flux across the segments
            vflux=(VEL(2,nz,el(2))*deltaX2 - VEL(1,nz,el(2))*deltaY2)*helem(nz,el(2))
            cHO=(vflux+abs(vflux))*Tmean1 + (vflux-abs(vflux))*Tmean2
            flux(nz,edge)=-0.5_WP*(1.0_WP-num_ord)*cHO - vflux*num_ord*0.5_WP*(Tmean1+Tmean2)-flux(nz,edge)
         end do
-        !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-    !$ACC END PARALLEL LOOP
-#endif 
 end subroutine adv_tra_hor_mfct
+
diff --git a/src/oce_adv_tra_ver.F90 b/src/oce_adv_tra_ver.F90
index 6429447f..1041a160 100644
--- a/src/oce_adv_tra_ver.F90
+++ b/src/oce_adv_tra_ver.F90
@@ -114,11 +114,11 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
 
     dt_inv=1.0_WP/dt
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(a, b, c, tr, cp, tp, n, nz, nzmax, nzmin, m, zinv, dz, c1, v_adv)
-!$OMP DO
+!$OMP DO    
     !___________________________________________________________________________
     ! loop over local nodes
-    do n=1,myDim_nod2D
-
+    do n=1,myDim_nod2D  
+        
         ! initialise
         a  = 0.0_WP
         b  = 0.0_WP
@@ -126,17 +126,17 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
         tr = 0.0_WP
         tp = 0.0_WP
         cp = 0.0_WP
-
+        
         ! max. number of levels at node n
         nzmax=nlevels_nod2D(n)
-
+        
         ! upper surface index, in case of cavity !=1
         nzmin=ulevels_nod2D(n)
-
+        
         !___________________________________________________________________________
-        ! Here can not exchange zbar_n & Z_n with zbar_3d_n & Z_3d_n because
+        ! Here can not exchange zbar_n & Z_n with zbar_3d_n & Z_3d_n because  
         ! they be calculate from the actualized mesh with hnode_new
-        ! calculate new zbar (depth of layers) and Z (mid depths of layers)
+        ! calculate new zbar (depth of layers) and Z (mid depths of layers) 
         ! depending on layer thinkness over depth at node n
         ! Be carefull here vertical operation have to be done on NEW vertical mesh !!!
         zbar_n=0.0_WP
@@ -148,27 +148,27 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
             Z_n(nz-1)  = zbar_n(nz)   + hnode_new(nz-1,n)/2.0_WP
         end do
         zbar_n(nzmin) = zbar_n(nzmin+1) + hnode_new(nzmin,n)
-
+        
         !_______________________________________________________________________
-        ! Regular part of coefficients: --> surface layer
+        ! Regular part of coefficients: --> surface layer 
         nz=nzmin
-
+        
         ! 1/dz(nz)
         zinv=1.0_WP*dt    ! no .../(zbar(1)-zbar(2)) because of  ALE
-
+        
         !!PS a(nz)=0.0_WP
         !!PS v_adv=zinv*areasvol(nz+1,n)/areasvol(nz,n)
         !!PS b(nz)= hnode_new(nz,n)+W(nz, n)*zinv-min(0._WP, W(nz+1, n))*v_adv
         !!PS c(nz)=-max(0._WP, W(nz+1, n))*v_adv
-
+        
         a(nz)=0.0_WP
         v_adv=zinv*area(nz  ,n)/areasvol(nz,n)
         b(nz)= hnode_new(nz,n)+W(nz, n)*v_adv
-
+        
         v_adv=zinv*area(nz+1,n)/areasvol(nz,n)
         b(nz)= b(nz)-min(0._WP, W(nz+1, n))*v_adv
         c(nz)=-max(0._WP, W(nz+1, n))*v_adv
-
+        
         !_______________________________________________________________________
         ! Regular part of coefficients: --> 2nd...nl-2 layer
         do nz=nzmin+1, nzmax-2
@@ -176,12 +176,12 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
             v_adv=zinv*area(nz  ,n)/areasvol(nz,n)
             a(nz)=min(0._WP, W(nz, n))*v_adv
             b(nz)=hnode_new(nz,n)+max(0._WP, W(nz, n))*v_adv
-
+            
             v_adv=zinv*area(nz+1,n)/areasvol(nz,n)
             b(nz)=b(nz)-min(0._WP, W(nz+1, n))*v_adv
             c(nz)=     -max(0._WP, W(nz+1, n))*v_adv
         end do ! --> do nz=2, nzmax-2
-
+        
         !_______________________________________________________________________
         ! Regular part of coefficients: --> nl-1 layer
         nz=nzmax-1
@@ -193,12 +193,12 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
         a(nz)=                min(0._WP, W(nz, n))*v_adv
         b(nz)=hnode_new(nz,n)+max(0._WP, W(nz, n))*v_adv
         c(nz)=0.0_WP
-
+        
         !_______________________________________________________________________
         nz=nzmin
         dz=hnode_new(nz,n) ! It would be (zbar(nz)-zbar(nz+1)) if not ALE
         tr(nz)=-(b(nz)-dz)*ttf(nz,n)-c(nz)*ttf(nz+1,n)
-
+        
         do nz=nzmin+1,nzmax-2
             dz=hnode_new(nz,n)
             tr(nz)=-a(nz)*ttf(nz-1,n)-(b(nz)-dz)*ttf(nz,n)-c(nz)*ttf(nz+1,n)
@@ -206,28 +206,28 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
         nz=nzmax-1
         dz=hnode_new(nz,n)
         tr(nz)=-a(nz)*ttf(nz-1,n)-(b(nz)-dz)*ttf(nz,n)
-
+        
         !_______________________________________________________________________
         nz = nzmin
         cp(nz) = c(nz)/b(nz)
         tp(nz) = tr(nz)/b(nz)
-
+        
         ! solve for vectors c-prime and t, s-prime
         do nz = nzmin+1,nzmax-1
             m = b(nz)-cp(nz-1)*a(nz)
             cp(nz) = c(nz)/m
             tp(nz) = (tr(nz)-tp(nz-1)*a(nz))/m
         end do
-
+        
         !_______________________________________________________________________
-        ! start with back substitution
+        ! start with back substitution 
         tr(nzmax-1) = tp(nzmax-1)
-
+        
         ! solve for x from the vectors c-prime and d-prime
         do nz = nzmax-2, nzmin, -1
             tr(nz) = tp(nz)-cp(nz)*tr(nz+1)
         end do
-
+        
         !_______________________________________________________________________
         ! update tracer
         do nz=nzmin,nzmax-1
@@ -235,7 +235,6 @@ subroutine adv_tra_vert_impl(dt, w, ttf, partit, mesh)
         end do
     end do ! --> do n=1,myDim_nod2D
 !$OMP END DO
-!$OMP BARRIER
 !$OMP END PARALLEL
 end subroutine adv_tra_vert_impl
 !
@@ -268,63 +267,44 @@ subroutine adv_tra_ver_upw1(w, ttf, partit, mesh, flux, o_init_zero)
        l_init_zero=o_init_zero
     end if
     if (l_init_zero) then
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-        !$ACC PARALLEL LOOP GANG VECTOR COLLAPSE(2) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
-        do n=1, myDim_nod2D
-          do nz=1,mesh%nl
-            flux(nz, n)=0.0_WP
-          end do
-        end do
-#ifndef ENABLE_OPENACC
+       do n=1, myDim_nod2D
+          flux(:, n)=0.0_WP
+       end do
 !$OMP END PARALLEL DO
-#else
-        !$ACC END PARALLEL LOOP
-#endif
     end if
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(tvert, n, nz, nzmax, nzmin)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
+
     do n=1, myDim_nod2D
        !_______________________________________________________________________
        nzmax=nlevels_nod2D(n)
        nzmin=ulevels_nod2D(n)
-
+       
        !_______________________________________________________________________
        ! vert. flux at surface layer
        nz=nzmin
        flux(nz,n)=-W(nz,n)*ttf(nz,n)*area(nz,n)-flux(nz,n)
-
+       
        !_______________________________________________________________________
        ! vert. flux at bottom layer --> zero bottom flux
        nz=nzmax
        flux(nz,n)= 0.0_WP-flux(nz,n)
-
+       
        !_______________________________________________________________________
        ! Be carefull have to do vertical tracer advection here on old vertical grid
-       ! also horizontal advection is done on old mesh (see helem contains old
+       ! also horizontal advection is done on old mesh (see helem contains old 
        ! mesh information)
        !_______________________________________________________________________
-       ! vert. flux at remaining levels
-       !$ACC LOOP VECTOR
+       ! vert. flux at remaining levels    
        do nz=nzmin+1,nzmax-1
           flux(nz,n)=-0.5*(                                                        &
                       ttf(nz  ,n)*(W(nz,n)+abs(W(nz,n)))+ &
                       ttf(nz-1,n)*(W(nz,n)-abs(W(nz,n))))*area(nz,n)-flux(nz,n)
        end do
-       !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-    !$ACC END PARALLEL LOOP
-#endif
 end subroutine adv_tra_ver_upw1
 !
 !
@@ -359,28 +339,15 @@ subroutine adv_tra_ver_qr4c(w, ttf, partit, mesh, num_ord, flux, o_init_zero)
        l_init_zero=o_init_zero
     end if
     if (l_init_zero) then
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DO
-#else
-       !$ACC PARALLEL LOOP GANG VECTOR COLLAPSE(2) DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
        do n=1, myDim_nod2D
-          do nz=1, mesh%nl
-             flux(nz, n)=0.0_WP
-          end do
+          flux(:, n)=0.0_WP
        end do
-#ifndef ENABLE_OPENACC
 !$OMP END PARALLEL DO
-#else
-       !$ACC END PARALLEL LOOP
-#endif
     end if
-#ifndef ENABLE_OPENACC
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(tvert,n, nz, nzmax, nzmin, Tmean, Tmean1, Tmean2, qc, qu,qd)
 !$OMP DO
-#else
-    !$ACC PARALLEL LOOP GANG DEFAULT(PRESENT) VECTOR_LENGTH(acc_vl)
-#endif
+
     do n=1, myDim_nod2D
        !_______________________________________________________________________
        nzmax=nlevels_nod2D(n)
@@ -389,48 +356,42 @@ subroutine adv_tra_ver_qr4c(w, ttf, partit, mesh, num_ord, flux, o_init_zero)
        ! vert. flux at surface layer
        nz=nzmin
        flux(nz,n)=-ttf(nz,n)*W(nz,n)*area(nz,n)-flux(nz,n)
-
+       
        !_______________________________________________________________________
        ! vert. flux 2nd layer --> centered differences
        nz=nzmin+1
        flux(nz,n)=-0.5_WP*(ttf(nz-1,n)+ttf(nz,n))*W(nz,n)*area(nz,n)-flux(nz,n)
-
+       
        !_______________________________________________________________________
        ! vert. flux at bottom - 1 layer --> centered differences
        nz=nzmax-1
        flux(nz,n)=-0.5_WP*(ttf(nz-1,n)+ttf(nz,n))*W(nz,n)*area(nz,n)-flux(nz,n)
-
+       
        !_______________________________________________________________________
        ! vert. flux at bottom layer --> zero bottom flux
        nz=nzmax
        flux(nz,n)= 0.0_WP-flux(nz,n)
-
+       
        !_______________________________________________________________________
        ! Be carefull have to do vertical tracer advection here on old vertical grid
-       ! also horizontal advection is done on old mesh (see helem contains old
+       ! also horizontal advection is done on old mesh (see helem contains old 
        ! mesh information)
        !_______________________________________________________________________
-       ! vert. flux at remaining levels
-       !$ACC LOOP VECTOR
+       ! vert. flux at remaining levels    
        do nz=nzmin+2,nzmax-2
             !centered (4th order)
             qc=(ttf(nz-1,n)-ttf(nz  ,n))/(Z_3d_n(nz-1,n)-Z_3d_n(nz  ,n))
-            qu=(ttf(nz  ,n)-ttf(nz+1,n))/(Z_3d_n(nz  ,n)-Z_3d_n(nz+1,n))
+            qu=(ttf(nz  ,n)-ttf(nz+1,n))/(Z_3d_n(nz  ,n)-Z_3d_n(nz+1,n))    
             qd=(ttf(nz-2,n)-ttf(nz-1,n))/(Z_3d_n(nz-2,n)-Z_3d_n(nz-1,n))
-
+                    
             Tmean1=ttf(nz  ,n)+(2*qc+qu)*(zbar_3d_n(nz,n)-Z_3d_n(nz  ,n))/3.0_WP
             Tmean2=ttf(nz-1,n)+(2*qc+qd)*(zbar_3d_n(nz,n)-Z_3d_n(nz-1,n))/3.0_WP
             Tmean =(W(nz,n)+abs(W(nz,n)))*Tmean1+(W(nz,n)-abs(W(nz,n)))*Tmean2
             flux(nz,n)=(-0.5_WP*(1.0_WP-num_ord)*Tmean - num_ord*(0.5_WP*(Tmean1+Tmean2))*W(nz,n))*area(nz,n)-flux(nz,n)
        end do
-       !$ACC END LOOP
     end do
-#ifndef ENABLE_OPENACC
 !$OMP END DO
 !$OMP END PARALLEL
-#else
-    !$ACC END PARALLEL LOOP
-#endif
 end subroutine adv_tra_ver_qr4c
 !
 !
@@ -476,7 +437,7 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
     ! Vertical advection
     ! --------------------------------------------------------------------------
     ! A piecewise parabolic scheme for uniformly-spaced layers.
-    ! See Colella and Woodward, JCP, 1984, 174-201. It can be coded so as to to take
+    ! See Colella and Woodward, JCP, 1984, 174-201. It can be coded so as to to take 
     ! non-uniformity into account, but this is more cumbersome. This is the version for AB
     ! time stepping
     ! --------------------------------------------------------------------------
@@ -486,14 +447,14 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
 !$OMP DO
     do n=1, myDim_nod2D
         !_______________________________________________________________________
-        !Interpolate to zbar...depth levels --> all quantities (tracer ...) are
-        ! calculated on mid depth levels
+        !Interpolate to zbar...depth levels --> all quantities (tracer ...) are 
+        ! calculated on mid depth levels 
         ! nzmax ... number of depth levels at node n
         nzmax=nlevels_nod2D(n)
         nzmin=ulevels_nod2D(n)
-
+        
         ! tracer at surface level
-        tv(nzmin)=ttf(nzmin,n)
+        tv(nzmin)=ttf(nzmin,n)        
         ! tracer at surface+1 level
 !       tv(2)=-ttf(1,n)*min(sign(1.0, W(2,n)), 0._WP)+ttf(2,n)*max(sign(1.0, W(2,n)), 0._WP)
 !       tv(3)=-ttf(2,n)*min(sign(1.0, W(3,n)), 0._WP)+ttf(3,n)*max(sign(1.0, W(3,n)), 0._WP)
@@ -503,7 +464,7 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
 !       tv(nzmax-1)=0.5_WP*(ttf(nzmax-2,n)+ttf(nzmax-1,n))
         ! tracer at bottom level
         tv(nzmax)=ttf(nzmax-1,n)
-
+        
         !_______________________________________________________________________
         ! calc tracer for surface+2 until depth-2 layer
         ! see Colella and Woodward, JCP, 1984, 174-201 --> equation (1.9)
@@ -514,20 +475,20 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
             ! for uniform spaced vertical grids --> piecewise parabolic method (ppm)
             ! equation (1.9)
             ! tv(nz)=(7.0_WP*(ttf(nz-1,n)+ttf(nz,n))-(ttf(nz-2,n)+ttf(nz+1,n)))/12.0_WP
-
+            
             !___________________________________________________________________
-            ! for non-uniformity spaced vertical grids --> piecewise parabolic
-            ! method (ppm) see see Colella and Woodward, JCP, 1984, 174-201
+            ! for non-uniformity spaced vertical grids --> piecewise parabolic 
+            ! method (ppm) see see Colella and Woodward, JCP, 1984, 174-201 
             ! --> full equation (1.6), (1.7) and (1.8)
             dzjm1    = hnode_new(nz-1,n)
             dzj      = hnode_new(nz  ,n)
             dzjp1    = hnode_new(nz+1,n)
             dzjp2    = hnode_new(nz+2,n)
             ! Be carefull here vertical operation have to be done on NEW vertical mesh !!!
-
+            
             !___________________________________________________________________
             ! equation (1.7)
-            ! --> Here deltaj is the average slope in the jth zone of the parabola
+            ! --> Here deltaj is the average slope in the jth zone of the parabola 
             !     with zone averages a_(j-1) and a_j, a_(j+1)
             ! --> a_j^n
             deltaj   = dzj/(dzjm1+dzj+dzjp1)* &
@@ -535,7 +496,7 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
                        (2._WP*dzjm1+dzj    )/(dzjp1+dzj)*(ttf(nz+1,n)-ttf(nz  ,n)) +  &
                        (dzj    +2._WP*dzjp1)/(dzjm1+dzj)*(ttf(nz  ,n)-ttf(nz-1,n)) &
                       )
-            ! --> a_(j+1)^n
+            ! --> a_(j+1)^n          
             deltajp1 = dzjp1/(dzj+dzjp1+dzjp2)* &
                       ( &
                        (2._WP*dzj+dzjp1  )/(dzjp2+dzjp1)*(ttf(nz+2,n)-ttf(nz+1,n)) +  &
@@ -543,7 +504,7 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
                       )
             !___________________________________________________________________
             ! condition (1.8)
-            ! --> This modification leads to a somewhat steeper representation of
+            ! --> This modification leads to a somewhat steeper representation of 
             !     discontinuities in the solution. It also guarantees that a_(j+0.5)
             !     lies in the range of values defined by a_j; and a_(j+1);
             if ( (ttf(nz+1,n)-ttf(nz  ,n))*(ttf(nz  ,n)-ttf(nz-1,n)) > 0._WP ) then
@@ -577,7 +538,7 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
                         )
                        !tv(nz+1)=max(min(ttf(nz, n), ttf(nz+1, n)), min(max(ttf(nz, n), ttf(nz+1, n)), tv(nz+1)))
         end do ! --> do nz=2,nzmax-3
-
+        
         tvert(1:nzmax)=0._WP
         ! loop over layers (segments)
         do nz=nzmin, nzmax-1
@@ -597,17 +558,17 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
             if ((aR-aL)*(ttf(nz, n)-0.5_WP*(aR+aL))<-(aR-aL)**2/6._WP) then
                 aR =3._WP*ttf(nz, n)-2._WP*aL
             end if
-
+            
             dzj   = hnode(nz,n)
             aj=6.0_WP*(ttf(nz, n)-0.5_WP*(aL+aR))
-
+            
             if (W(nz,n)>0._WP) then
                 x=min(W(nz,n)*dt/dzj, 1._WP)
                 tvert(nz  )=(-aL-0.5_WP*x*(aR-aL+(1._WP-2._WP/3._WP*x)*aj))
                 tvert(nz  )=tvert(nz) ! compute 2nd moment for DVD
                 tvert(nz  )=tvert(nz)*area(nz,n)*W(nz,n)
             end if
-
+            
             if (W(nz+1,n)<0._WP) then
                 x=min(-W(nz+1,n)*dt/dzj, 1._WP)
                 tvert(nz+1)=(-aR+0.5_WP*x*(aR-aL-(1._WP-2._WP/3._WP*x)*aj))
@@ -615,12 +576,12 @@ subroutine adv_tra_vert_ppm(dt, w, ttf, partit, mesh, flux, o_init_zero)
                 tvert(nz+1)=tvert(nz+1)*area(nz+1,n)*W(nz+1,n)
             end if
         end do
-
+        
         !_______________________________________________________________________
         ! Surface flux
         tvert(nzmin)= -tv(nzmin)*W(nzmin,n)*area(nzmin,n)
         ! Zero bottom flux
-        tvert(nzmax)=0.0_WP
+        tvert(nzmax)=0.0_WP        
         flux(nzmin:nzmax, n)=tvert(nzmin:nzmax)-flux(nzmin:nzmax, n)
     end do ! --> do n=1, myDim_nod2D
 !       if (mype==0) write(*,*) 'PPM overshoot statistics:', real(overshoot_counter)/real(counter)
@@ -669,22 +630,22 @@ subroutine adv_tra_ver_cdiff(w, ttf, partit, mesh, flux, o_init_zero)
         !_______________________________________________________________________
         nzmax=nlevels_nod2D(n)-1
         nzmin=ulevels_nod2D(n)
-
+        
         !_______________________________________________________________________
         ! Surface flux
         tvert(nzmin)= -W(nzmin,n)*ttf(nzmin,n)*area(nzmin,n)
-
+        
         !_______________________________________________________________________
         ! Zero bottom flux
-        tvert(nzmax+1)=0.0_WP
-
+        tvert(nzmax+1)=0.0_WP        
+        
         !_______________________________________________________________________
         ! Other levels
         do nz=nzmin+1, nzmax
             tv=0.5_WP*(ttf(nz-1,n)+ttf(nz,n))
             tvert(nz)= -tv*W(nz,n)*area(nz,n)
         end do
-
+        
         !_______________________________________________________________________
         flux(nzmin:nzmax, n)=tvert(nzmin:nzmax)-flux(nzmin:nzmax, n)
     end do ! --> do n=1, myDim_nod2D
diff --git a/src/oce_ale.F90 b/src/oce_ale.F90
index 4b3bb379..6ebacd85 100644
--- a/src/oce_ale.F90
+++ b/src/oce_ale.F90
@@ -1977,6 +1977,7 @@ subroutine compute_hbar_ale(dynamics, partit, mesh)
 
 !$OMP PARALLEL DO
     do n=1,myDim_nod2D
+        if (ulevels_nod2D(n) > 1) cycle ! --> if cavity node hbar == hbar_old
         hbar(n)=hbar_old(n)+ssh_rhs_old(n)*dt/areasvol(ulevels_nod2D(n),n)
     end do
 !$OMP END PARALLEL DO
@@ -2533,7 +2534,7 @@ subroutine vert_vel_ale(dynamics, partit, mesh)
     end do
 !$OMP END PARALLEL DO
 
-!$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(n, nz, nzmin, nzmax, c1, c2)
+!$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(n, nz, nzmin, nzmax)
     do n=1, myDim_nod2D+eDim_nod2D
         nzmin = ulevels_nod2D(n)
         nzmax = nlevels_nod2D(n)-1
@@ -2997,13 +2998,7 @@ subroutine oce_timestep_ale(n, ice, dynamics, tracers, partit, mesh)
     else  
         call pressure_force_4_zxxxx(tracers, partit, mesh)
     end if
-    
-    !___________________________________________________________________________
-    ! check validity of visc_opt=5 selection
-    ! --> need to know buoyancy frequency to do so.
-    ! --> only check on the first timestep 
-    if (n==1) call check_viscopt(dynamics, partit, mesh)
-    
+
     !___________________________________________________________________________
     ! calculate alpha and beta
     ! it will be used for KPP, Redi, GM etc. Shall we keep it on in general case?
@@ -3203,7 +3198,7 @@ subroutine oce_timestep_ale(n, ice, dynamics, tracers, partit, mesh)
     !   rigid lid.
 !$OMP PARALLEL DO
     do node=1, myDim_nod2D+eDim_nod2D
-       eta_n(node)=alpha*hbar(node)+(1.0_WP-alpha)*hbar_old(node)
+       if (ulevels_nod2D(node)==1) eta_n(node)=alpha*hbar(node)+(1.0_WP-alpha)*hbar_old(node)
     end do
 !$OMP END PARALLEL DO
     ! --> eta_(n)
diff --git a/src/oce_ale_pressure_bv.F90 b/src/oce_ale_pressure_bv.F90
index a2f759d7..5cf1774d 100644
--- a/src/oce_ale_pressure_bv.F90
+++ b/src/oce_ale_pressure_bv.F90
@@ -1,8 +1,11 @@
 module densityJM_components_interface
   interface
-    subroutine densityJM_components(t, s, bulk_0, bulk_pz, bulk_pz2, rhopot)
+    subroutine densityJM_components(t, s, bulk_0, bulk_pz, bulk_pz2, rhopot, partit, mesh)
+      USE MOD_MESH
+      USE MOD_PARTIT
       USE MOD_PARSUP
-      USE o_param
+      type(t_mesh),   intent(in) ,    target :: mesh
+      type(t_partit), intent(inout),  target :: partit
       real(kind=WP),  intent(IN)             :: t,s
       real(kind=WP),  intent(OUT)            :: bulk_0, bulk_pz, bulk_pz2, rhopot
     end subroutine
@@ -11,9 +14,12 @@ end module
 
 module density_linear_interface
   interface
-    subroutine density_linear(t, s, bulk_0, bulk_pz, bulk_pz2, rho_out)
+    subroutine density_linear(t, s, bulk_0, bulk_pz, bulk_pz2, rho_out, partit, mesh)
+      USE MOD_MESH
+      USE MOD_PARTIT
       USE MOD_PARSUP
-      USE o_param
+      type(t_mesh),   intent(in) ,    target :: mesh
+      type(t_partit), intent(inout),  target :: partit
       real(kind=WP),  intent(IN)             :: t,s
       real(kind=WP),  intent(OUT)            :: bulk_0, bulk_pz, bulk_pz2, rho_out
     end subroutine
@@ -192,7 +198,7 @@ end module
 !
 !===============================================================================
 subroutine pressure_bv(tracers, partit, mesh)
-! fill in the hydrostatic pressure and the Brunt-Vaisala frequency
+! fill in the hydrostatic pressure and the Brunt-Vaisala frequency 
 ! in a single pass the using split form of the equation of state
 ! as proposed by NR
     use g_config
@@ -257,7 +263,7 @@ subroutine pressure_bv(tracers, partit, mesh)
             end do
         end do
     endif
-
+    
     !___________________________________________________________________________
 
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(zmean, dz_inv, bv,  a, rho_up, rho_dn, t, s, node, nz, nl1, nzmax, nzmin, &
@@ -267,10 +273,10 @@ subroutine pressure_bv(tracers, partit, mesh)
     do node=1, myDim_nod2D+eDim_nod2D
         nzmin = ulevels_nod2D(node)
         nzmax = nlevels_nod2D(node)
-
+            
         !!PS nl1= nlevels_nod2d(node)-1
         rho      = 0.0_WP
-        bulk_0   = 0.0_WP
+        bulk_0   = 0.0_WP 
         bulk_pz  = 0.0_WP
         bulk_pz2 = 0.0_WP
         rhopot   = 0.0_WP
@@ -278,7 +284,7 @@ subroutine pressure_bv(tracers, partit, mesh)
         ! it will be used for computing MLD according to FESOM 1.4 implementation (after Large et al. 1997)
         db_max   = 0.0_WP
         dbsfc1   = 0.0_WP
-
+        
         !_______________________________________________________________________
         ! apply equation of state
         do nz=nzmin, nzmax-1
@@ -286,17 +292,17 @@ subroutine pressure_bv(tracers, partit, mesh)
             s=salt(nz, node)
             select case(state_equation)
                 case(0)
-                    call density_linear(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz))
+                    call density_linear(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz), partit, mesh)
                 case(1)
-                    call densityJM_components(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz))
+                    call densityJM_components(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz), partit, mesh)
                 case default !unknown
                     if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                     call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
             end select
         end do
-
-        !NR split the loop here. The Intel compiler could not resolve that there is no dependency
-        !NR and did not vectorize the full loop.
+            
+        !NR split the loop here. The Intel compiler could not resolve that there is no dependency 
+        !NR and did not vectorize the full loop. 
         !_______________________________________________________________________
         ! calculate density for MOC
         if (ldiag_dMOC) then
@@ -306,10 +312,10 @@ subroutine pressure_bv(tracers, partit, mesh)
                 density_dmoc(nz,node)= rho(nz)*rhopot(nz)/(rho(nz)-200._WP)
                         !           density_dmoc(nz,node)   = rhopot(nz)
             end do
-        end if
-
-        !_______________________________________________________________________
-        ! compute density for PGF
+        end if 
+            
+        !_______________________________________________________________________    
+        ! compute density for PGF 
         !!PS do nz=1, nl1
         do nz=nzmin, nzmax-1
             !___________________________________________________________________
@@ -317,42 +323,42 @@ subroutine pressure_bv(tracers, partit, mesh)
             !!PS rho(nz)=rho(nz)*rhopot(nz)/(rho(nz)+0.1_WP*Z_3d_n(nz,node))-density_0
             rho(nz) = rho(nz)*rhopot(nz)/(rho(nz)+0.1_WP*Z_3d_n(nz,node)*real(state_equation))-density_ref(nz,node)
             density_m_rho0(nz,node) = rho(nz)
-
+            
             !___________________________________________________________________
             ! buoyancy difference between the surface and the grid points blow (adopted from FESOM 1.4)
-            ! --> bring density of surface point adiabatically to the same
-            !     depth level as the deep point --> than calculate bouyancy
+            ! --> bring density of surface point adiabatically to the same 
+            !     depth level as the deep point --> than calculate bouyancy 
             !     difference
             rho_surf=bulk_0(nzmin)   + Z_3d_n(nz,node)*(bulk_pz(nzmin)   + Z_3d_n(nz,node)*bulk_pz2(nzmin))
             !!PS rho_surf=rho_surf*rhopot(nzmin)/(rho_surf+0.1_WP*Z_3d_n(nz,node))-density_0
             rho_surf=rho_surf*rhopot(nzmin)/(rho_surf+0.1_WP*Z_3d_n(nz,node)*real(state_equation)) !-density_ref(nzmin,node)
-
+            
             !!PS dbsfc1(nz) = -g * ( rho_surf - rho(nz) ) / (rho(nz)+density_0)      ! this is also required when KPP is ON
             !!PS dbsfc1(nz) = -g * density_0_r * ( rho_surf - rho(nz) )
             dbsfc1(nz) = -g * ( rho_surf - (rho(nz)+density_ref(nz,node)) ) / (rho(nz)+density_ref(nz,node))      ! this is also required when KPP is ON
-
+            
             db_max = max(dbsfc1(nz)/abs(Z_3d_n(nzmin,node)-Z_3d_n(max(nz, nzmin+1),node)), db_max)
         end do
-
+        
         dbsfc1(nzmax)=dbsfc1(nzmax-1)
         if (mixing_kpp) then ! in case KPP is ON store the buoyancy difference with respect to the surface (m/s2)
             dbsfc(nzmin:nzmax, node )=dbsfc1(nzmin:nzmax)
         end if
-
+        
         !_______________________________________________________________________
-        ! fill density levels that are occupied by the cavity with the density
-        ! that corresponds to a water mass that has the same temperature and salinity
+        ! fill density levels that are occupied by the cavity with the density 
+        ! that corresponds to a water mass that has the same temperature and salinity 
         ! like at the cavity-ocean interface --> compute water mass density that
         ! is replaced by the cavity
-        if (nzmin>1) then
+        if (nzmin>1) then 
             t=temp(nzmin, node)
             s=salt(nzmin, node)
             do nz=1, nzmin-1
                 select case(state_equation)
                     case(0)
-                        call density_linear(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz))
+                        call density_linear(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz), partit, mesh)
                     case(1)
-                        call densityJM_components(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz))
+                        call densityJM_components(t, s, bulk_0(nz), bulk_pz(nz), bulk_pz2(nz), rhopot(nz), partit, mesh)
                     case default !unknown
                         if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                         call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
@@ -363,93 +369,93 @@ subroutine pressure_bv(tracers, partit, mesh)
                 density_m_rho0(nz,node) = rho(nz)
             end do
         end if
-
+            
         !_______________________________________________________________________
-        ! calculate pressure
+        ! calculate pressure 
         if (trim(which_ale)=='linfs' .or. use_cavity .eqv. .true.) then
             !!PS hpressure(1, node)=-Z_3d_n(1,node)*rho(1)*g
             !!PS hpressure(1, node)=0.5_WP*hnode(1,node)*rho(1)*g
             !___________________________________________________________________
             ! compute pressure boundary condition at cavity-ocean interface
             ! hpressure(nzmin, node)
-            if (nzmin>1) then ! cavity case
-
-                ! --> this as a upper pressure boundary condition incase of a
+            if (nzmin>1) then ! cavity case 
+                
+                ! --> this as a upper pressure boundary condition incase of a 
                 ! homogenous ocean, with no fluxes and boundary condition creates
-                ! no pressure gradient errors
+                ! no pressure gradient errors 
                 hpressure(nzmin, node)=0.5_WP*(zbar_3d_n(1,node)-zbar_3d_n(2,node))*rho(1)*g
                 do nz=2,nzmin
                     a=0.5_WP*g*(rho(nz-1)*(zbar_3d_n(nz-1,node)-zbar_3d_n(nz,node))+rho(nz)*(zbar_3d_n(nz,node)-zbar_3d_n(nz+1,node)))
-                    hpressure(nzmin, node)=hpressure(nzmin, node)+a
+                    hpressure(nzmin, node)=hpressure(nzmin, node)+a                
                 end do
-
-                !  --> this as a upper pressure boundary condition incase of a
+                
+                !  --> this as a upper pressure boundary condition incase of a 
                 ! homogenous ocean, with no fluxes and boundary condition creates
-                ! pgf errors
+                ! pgf errors 
                 !!PS hpressure(nzmin, node)=-Z_3d_n(nzmin,node)*rho(nzmin)*g
             else
                 hpressure(nzmin, node)=-Z_3d_n(nzmin,node)*rho(nzmin)*g
-            end if
-
+            end if 
+            
             !___________________________________________________________________
             ! compute pressure below surface boundary
             do nz=nzmin+1,nzmax-1
-                ! why 0.5 ... integrate g*rho*dz vertically, integrate half layer
-                ! thickness of previouse layer and half layer thickness of actual
+                ! why 0.5 ... integrate g*rho*dz vertically, integrate half layer 
+                ! thickness of previouse layer and half layer thickness of actual 
                 ! layer to integrate pressure on mid depth level of actual layer
                 a=0.5_WP*g*(rho(nz-1)*hnode(nz-1,node)+rho(nz)*hnode(nz,node))
                 hpressure(nz, node)=hpressure(nz-1, node)+a
             end do
-        end if
-
+        end if    
+            
         !___________________________________________________________________
-        ! calculate mixed layer depth after Monterey and Levitus, (1997) who
-        ! compute MLD as the depth at which the density over depth differs
-        ! by 0.125 sigma units from the surface density (Griffies et al., 2009).
-        ! This MLD definition was also supported in FESOM1.4 (-->MLD2) and after
-        ! the definition of Large et al. (1997), who suggest to compute MLD
-        ! as the shallowest depth where the vertical derivative of buoyancy
-        ! is equal to a local critical buoyancy gradient (Griffies et al.,
-        ! 2009) (-->MLD1).
-        ! To be CMOR compliant we add MLD3, which is identical to MLD2, except
+        ! calculate mixed layer depth after Monterey and Levitus, (1997) who 
+        ! compute MLD as the depth at which the density over depth differs 
+        ! by 0.125 sigma units from the surface density (Griffies et al., 2009). 
+        ! This MLD definition was also supported in FESOM1.4 (-->MLD2) and after 
+        ! the definition of Large et al. (1997), who suggest to compute MLD 
+        ! as the shallowest depth where the vertical derivative of buoyancy 
+        ! is equal to a local critical buoyancy gradient (Griffies et al., 
+        ! 2009) (-->MLD1). 
+        ! To be CMOR compliant we add MLD3, which is identical to MLD2, except 
         ! for the critical density threshold at 0.03, rather than 0.125 kg/m³
-        ! BV frequency:  bvfreq(nl,:), squared value is stored
+        ! BV frequency:  bvfreq(nl,:), squared value is stored   
         MLD1(node)=Z_3d_n(nzmin+1,node)
         MLD2(node)=Z_3d_n(nzmin+1,node)
         MLD3(node)=Z_3d_n(nzmin+1,node)
         MLD1_ind(node)=nzmin+1
         MLD2_ind(node)=nzmin+1
         MLD3_ind(node)=nzmin+1
-
+        
         flag1=.true.
         flag2=.true.
         flag3=.true.
         do nz=nzmin+1,nzmax-1
             zmean   = 0.5_WP*sum(Z_3d_n(nz-1:nz, node))
-            bulk_up = bulk_0(nz-1) + zmean*(bulk_pz(nz-1) + zmean*bulk_pz2(nz-1))
+            bulk_up = bulk_0(nz-1) + zmean*(bulk_pz(nz-1) + zmean*bulk_pz2(nz-1)) 
             bulk_dn = bulk_0(nz)   + zmean*(bulk_pz(nz)   + zmean*bulk_pz2(nz))
-            rho_up  = bulk_up*rhopot(nz-1) / (bulk_up + 0.1_WP*zmean*real(state_equation))
+            rho_up  = bulk_up*rhopot(nz-1) / (bulk_up + 0.1_WP*zmean*real(state_equation))  
             rho_dn  = bulk_dn*rhopot(nz)   / (bulk_dn + 0.1_WP*zmean*real(state_equation))
-            dz_inv  = 1.0_WP/(Z_3d_n(nz-1,node)-Z_3d_n(nz,node))
+            dz_inv  = 1.0_WP/(Z_3d_n(nz-1,node)-Z_3d_n(nz,node))  
             !_______________________________________________________________
-            ! squared brunt väisälä frequence N^2 --> N^2>0 stratification is
-            ! stable, vertical elongated parcel is accelaratedtowards
-            ! initial point --> does oscillation with frequency N.
-            ! N^2<0 stratification is unstable vertical elongated parcel is
-            ! accelerated away from initial point
-            bvfreq(nz,node)  = -g*dz_inv*(rho_up-rho_dn)/density_0
+            ! squared brunt väisälä frequence N^2 --> N^2>0 stratification is 
+            ! stable, vertical elongated parcel is accelaratedtowards 
+            ! initial point --> does oscillation with frequency N. 
+            ! N^2<0 stratification is unstable vertical elongated parcel is 
+            ! accelerated away from initial point 
+            bvfreq(nz,node)  = -g*dz_inv*(rho_up-rho_dn)/density_0          
             !!PS !--> Why not like this ?
-            !!PS bvfreq(nz,node)  = -g*dz_inv*(rho_up-rho_dn)/(rho_dn)
+            !!PS bvfreq(nz,node)  = -g*dz_inv*(rho_up-rho_dn)/(rho_dn)            
             !_______________________________________________________________
             ! define MLD following Large et al. 1997
-            ! MLD is the shallowest depth where the local buoyancy gradient matches the maximum buoyancy gradient
+            ! MLD is the shallowest depth where the local buoyancy gradient matches the maximum buoyancy gradient 
             ! between the surface and any discrete depth within the water column.
             if (bvfreq(nz, node) > db_max .and. flag1) then
                 MLD1(node)    =Z_3d_n(nz, node)
                 MLD1_ind(node)=nz
                 flag1=.false.
             end if
-
+            
             ! another definition of MLD after Levitus
             if ((rhopot(nz)-rhopot(nzmin) > sigma_theta_crit) .and. flag2) then
                 MLD2(node)=MLD2(node)+(Z_3d_n(nz,node)-MLD2(node))/(rhopot(nz)-rhopot(nz-1)+1.e-20)*(rhopot(1)+sigma_theta_crit-rhopot(nz-1))
@@ -467,15 +473,15 @@ subroutine pressure_bv(tracers, partit, mesh)
                 MLD3(node)=Z_3d_n(nz,node)
             end if
         end do
-
+        
         if (flag2) MLD2_ind(node)=nzmax-1
         if (flag3) MLD3_ind(node)=nzmax-1
-
+                
         bvfreq(nzmin,node)=bvfreq(nzmin+1,node)
-        bvfreq(nzmax,node)=bvfreq(nzmax-1,node)
+        bvfreq(nzmax,node)=bvfreq(nzmax-1,node) 
         !___________________________________________________________________
-        ! The mixed layer depth
-        ! mixlay_depth
+        ! The mixed layer depth 
+        ! mixlay_depth    
         ! bv_ref
         !_______________________________________________________________________
         ! BV is defined on full levels except for the first and the last ones.
@@ -523,14 +529,14 @@ subroutine pressure_force_4_linfs(tracers, partit, mesh)
     ! calculate pressure gradient force (PGF) for linfs with full cells
     if ( .not. use_partial_cell .and. .not. use_cavity_partial_cell) then
         call pressure_force_4_linfs_fullcell(partit, mesh)
-
+        
     elseif (use_cavity .and. use_cavity_partial_cell ) then
         if     (trim(which_pgf)=='sergey') then
-            call pressure_force_4_linfs_cavity(partit, mesh)
-        elseif (trim(which_pgf)=='shchepetkin') then
+            call pressure_force_4_linfs_cavity(partit, mesh)  
+        elseif (trim(which_pgf)=='shchepetkin') then    
             call pressure_force_4_linfs_shchepetkin(partit, mesh)
         elseif (trim(which_pgf)=='easypgf') then
-            call pressure_force_4_linfs_easypgf(tracers, partit, mesh)
+            call pressure_force_4_linfs_easypgf(tracers, partit, mesh)    
         else
             write(*,*) '________________________________________________________'
             write(*,*) ' --> ERROR: the choosen form of pressure gradient       '
@@ -539,8 +545,8 @@ subroutine pressure_force_4_linfs(tracers, partit, mesh)
             write(*,*) '            shchepetkin, easypgf '
             write(*,*) '________________________________________________________'
             call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
-        end if
-
+        end if     
+        
     !___________________________________________________________________________
     ! calculate pressure gradient force (PGF) for linfs with partiall cells
     else ! --> (trim(which_ale)=='linfs' .and. use_partial_cell )
@@ -551,7 +557,7 @@ subroutine pressure_force_4_linfs(tracers, partit, mesh)
         elseif (trim(which_pgf)=='cubicspline') then
             call pressure_force_4_linfs_cubicspline(partit, mesh)
         elseif (trim(which_pgf)=='easypgf') then
-            call pressure_force_4_linfs_easypgf(tracers, partit, mesh)
+            call pressure_force_4_linfs_easypgf(tracers, partit, mesh)    
         else
             write(*,*) '________________________________________________________'
             write(*,*) ' --> ERROR: the choosen form of pressure gradient       '
@@ -560,8 +566,8 @@ subroutine pressure_force_4_linfs(tracers, partit, mesh)
             write(*,*) '            shchepetkin, cubicspline '
             write(*,*) '________________________________________________________'
             call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
-        end if
-    end if
+        end if 
+    end if 
 end subroutine pressure_force_4_linfs
 !
 !
@@ -577,7 +583,7 @@ subroutine pressure_force_4_linfs_fullcell(partit, mesh)
     use g_config
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
-    type(t_partit), intent(inout),  target :: partit
+    type(t_partit), intent(inout),  target :: partit    
     integer                                :: elem, elnodes(3), nle, ule,  nlz
 
 #include "associate_part_def.h"
@@ -592,30 +598,30 @@ subroutine pressure_force_4_linfs_fullcell(partit, mesh)
         ! number of levels at elem
         nle=nlevels(elem)-1
         ule=ulevels(elem)
-
+            
         !_______________________________________________________________________
-        ! node indices of elem
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+            
         !_______________________________________________________________________
-        ! loop over mid-depth levels to calculate the pressure gradient
+        ! loop over mid-depth levels to calculate the pressure gradient 
         ! force (pgf) --> from top to bottom
         !!PS do nlz=1,nle
         do nlz=ule,nle
             pgf_x(nlz,elem) = sum(gradient_sca(1:3,elem)*hpressure(nlz,elnodes)/density_0)
             pgf_y(nlz,elem) = sum(gradient_sca(4:6,elem)*hpressure(nlz,elnodes)/density_0)
-        end do
+        end do 
     end do !-->do elem=1, myDim_elem2D
 !$OMP END PARALLEL DO
-end subroutine pressure_force_4_linfs_fullcell
+end subroutine pressure_force_4_linfs_fullcell   
 !
 !
 !
 !===============================================================================
-! Calculate pressure gradient force (PGF) like in NEMO based on NEMO ocean engine
-! Gurvan Madec, and the NEMO team gurvan.madec@locean-ipsl.umpc.fr, nemo
-! st@locean-ipsl.umpc.fr calculate vertical center index for linear
-! interpolation, densities are interpolated to shallowest nodal mid depth level
+! Calculate pressure gradient force (PGF) like in NEMO based on NEMO ocean engine 
+! Gurvan Madec, and the NEMO team gurvan.madec@locean-ipsl.umpc.fr, nemo 
+! st@locean-ipsl.umpc.fr calculate vertical center index for linear 
+! interpolation, densities are interpolated to shallowest nodal mid depth level 
 ! that contribute to element --> advantage no extrapolation neccessary
 ! Calculate pressure gradient force (PGF) like in NEMO based on NEMO ocean engine
 ! Gurvan Madec, and the NEMO team gurvan.madec@locean-ipsl.umpc.fr, nemo st@locean-ipsl.umpc.fr
@@ -633,7 +639,7 @@ subroutine pressure_force_4_linfs_nemo(tracers, partit, mesh)
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
     type(t_partit), intent(inout),  target :: partit
-    type(t_tracer), intent(in),     target :: tracers
+    type(t_tracer), intent(in),     target :: tracers    
     logical                                :: do_interpTS=.true.
     integer                                :: elem, elnodes(3), nle, ule, nlz, nln(3), uln(3), ni, nlc, nlce
     real(kind=WP)                          :: hpress_n_bottom(3)
@@ -660,22 +666,22 @@ subroutine pressure_force_4_linfs_nemo(tracers, partit, mesh)
         ! nle...number of mid-depth levels at elem
         nle          = nlevels(elem)-1
         ule          = ulevels(elem)
-
-        ! node indices of elem
+        
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+        
         !_______________________________________________________________________
-        ! 1) loop over mid-depth levels to calculate the pressure gradient
-        !    force (pgf) --> goes until one layer above the bottom
-        !
+        ! 1) loop over mid-depth levels to calculate the pressure gradient 
+        !    force (pgf) --> goes until one layer above the bottom 
+        !        
         !     :             :      --> nle-2
         !     :             :
         ! ----------   ----------
-        !
+        ! 
         !     o            o       --> nle-1
         ! T,S,rho(nle-1)
         ! ----------   ----------
-        !
+        ! 
         !     x <--- nle --o linear interpolate densities to shallowest mid depth level
         !     o
         !              ----------
@@ -685,20 +691,20 @@ subroutine pressure_force_4_linfs_nemo(tracers, partit, mesh)
         ! //////////   //////////
         ! //////////   //////////
         !!PS do nlz=1,nle-1
-        do nlz=ule,nle-1
+        do nlz=ule,nle-1 
             pgf_x(nlz,elem) = sum(gradient_sca(1:3,elem)*hpressure(nlz,elnodes)/density_0)
             pgf_y(nlz,elem) = sum(gradient_sca(4:6,elem)*hpressure(nlz,elnodes)/density_0)
         end do
-
+                
         !_______________________________________________________________________
-        ! 2) interpolate/extrapolate nodal density linearly to elemental
+        ! 2) interpolate/extrapolate nodal density linearly to elemental 
         !    bottom depth
         !_______________________________________________________________________
         ! nln...number of mid-depth levels at node
         nln     = nlevels_nod2d(elnodes)-1
         uln     = ulevels_nod2d(elnodes)
         !!PS uln     = ulevels_nod2d(elnodes)
-
+        
         !_______________________________________________________________________
         ! calculate mid depth element level --> Z_e
         zbar_n       = 0.0_WP
@@ -716,35 +722,35 @@ subroutine pressure_force_4_linfs_nemo(tracers, partit, mesh)
         ! --> Z_n       ... depth of mid-depth level at elements
         ! --> zbar_n_3d ... depth of level at nodes
         ! --> Z_n_3d    ... depth of mid-depth level at nodes
-
+        
         !_______________________________________________________________________
         ! loop over nodal indices of element
         do ni=1,3
             !___________________________________________________________________
-            ! calculate vertical center index for linear interpolation, densities
+            ! calculate vertical center index for linear interpolation, densities 
             ! are interpolated to shallowest nodal mid depth level that contribute
             ! to element --> advantage no extrapolation neccessary
-            !!PS nlc   = minloc( Z_3d_n(1:nln(ni),elnodes(ni))-maxval(Z_3d_n(nle,elnodes)),1,&
+            !!PS nlc   = minloc( Z_3d_n(1:nln(ni),elnodes(ni))-maxval(Z_3d_n(nle,elnodes)),1,& 
             !!PS                 Z_3d_n(1:nln(ni),elnodes(ni))-maxval(Z_3d_n(nle,elnodes))>0.0_WP &  ! mask for index selection
             !!PS                 )+1
-            nlc   = minloc( Z_3d_n(uln(ni):nln(ni),elnodes(ni))-maxval(Z_3d_n(nle,elnodes)),1,&
+            nlc   = minloc( Z_3d_n(uln(ni):nln(ni),elnodes(ni))-maxval(Z_3d_n(nle,elnodes)),1,& 
                             Z_3d_n(uln(ni):nln(ni),elnodes(ni))-maxval(Z_3d_n(nle,elnodes))>0.0_WP &  ! mask for index selection
                             )+1
-            nlc   = min(nlc,nln(ni))
+            nlc   = min(nlc,nln(ni)) 
             dZn   = Z_3d_n(nlc,elnodes(ni))    -Z_3d_n(nlc-1,elnodes(ni))
             dZn_i = maxval(Z_3d_n(nle,elnodes))-Z_3d_n(nlc-1,elnodes(ni))
-            dh    = minval(hnode(nle  ,elnodes))
-
+            dh    = minval(hnode(nle  ,elnodes)) 
+            
             !___________________________________________________________________
             !! Option (A): interpolate density directly ...
-            !if (.not. do_interpTS ) then
+            !if (.not. do_interpTS ) then 
             !    dval              = density_m_rho0(nlc  ,elnodes(ni))-density_m_rho0(nlc-1,elnodes(ni))
             !    interp_n_dens(ni) = density_m_rho0(nlc-1,elnodes(ni)) + (dval/dZn*dZn_i)
-            !
-            !! Option (B): NEMO ocean engine Gurvan Madec, and the NEMO team suggest
-            !! not to linearly interpolate the density towards the bottom rather to
-            !! interpolate temperature and salinity and calculate from them the
-            !! bottom density to account for the non linearities in the equation of
+            !    
+            !! Option (B): NEMO ocean engine Gurvan Madec, and the NEMO team suggest 
+            !! not to linearly interpolate the density towards the bottom rather to 
+            !! interpolate temperature and salinity and calculate from them the 
+            !! bottom density to account for the non linearities in the equation of 
             !! state ...
             !else
                 ! ... interpolate temperature and saltinity ...
@@ -752,15 +758,15 @@ subroutine pressure_force_4_linfs_nemo(tracers, partit, mesh)
                 interp_n_temp = temp(nlc-1,elnodes(ni)) + (dval/dZn*dZn_i)
                 dval          = salt(nlc  ,elnodes(ni)) - salt(nlc-1,elnodes(ni))
                 interp_n_salt = salt(nlc-1,elnodes(ni)) + (dval/dZn*dZn_i)
-
-                ! calculate density at element mid-depth bottom depth via
-                ! equation of state from linear interpolated temperature and
+                
+                ! calculate density at element mid-depth bottom depth via 
+                ! equation of state from linear interpolated temperature and 
                 ! salinity
                 select case(state_equation)
                     case(0)
-                        call density_linear(interp_n_temp, interp_n_salt, bulk_0, bulk_pz, bulk_pz2, rhopot)
+                        call density_linear(interp_n_temp, interp_n_salt, bulk_0, bulk_pz, bulk_pz2, rhopot, partit, mesh)
                     case(1)
-                        call densityJM_components(interp_n_temp, interp_n_salt, bulk_0, bulk_pz, bulk_pz2, rhopot)
+                        call densityJM_components(interp_n_temp, interp_n_salt, bulk_0, bulk_pz, bulk_pz2, rhopot, partit, mesh)
                     case default !unknown
                         if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                         call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
@@ -768,28 +774,28 @@ subroutine pressure_force_4_linfs_nemo(tracers, partit, mesh)
                 interp_n_dens(ni) = bulk_0 + Z_n(nle)*(bulk_pz + Z_n(nle)*bulk_pz2)
                 !!PS interp_n_dens(ni) = interp_n_dens(ni)*rhopot/(interp_n_dens(ni)+0.1_WP*Z_n(nle))*real(state_equation))-density_0
                 interp_n_dens(ni) = interp_n_dens(ni)*rhopot/(interp_n_dens(ni)+0.1_WP*Z_n(nle)*real(state_equation))-density_ref(nle,elnodes(ni))
-            !end if
-
-            ! calculate proper starting level for extrapolation of
+            !end if 
+                
+            ! calculate proper starting level for extrapolation of 
             ! bottom pressure
             nlce = min(nlc,nle)
-
+                
             !___________________________________________________________________
-            ! integrate nodal density until mid-depth level of bottom
-            ! element using linearly interpolated nodal bottom density
+            ! integrate nodal density until mid-depth level of bottom 
+            ! element using linearly interpolated nodal bottom density 
             ! (interp_n_dens) to obtain bottom pressure value
             hpress_n_bottom(ni) = hpressure(nlce-1, elnodes(ni))        &
-                                    + 0.5_WP*g*                         &
+                                    + 0.5_WP*g*                         & 
                                     (density_m_rho0(nlce-1,elnodes(ni))*&
-                                              hnode(nlce-1,elnodes(ni)) &
-                                    +                                   &
+                                              hnode(nlce-1,elnodes(ni)) & 
+                                    +                                   &  
                                     interp_n_dens(ni)*dh                &
-                                    )
+                                    )                             
         end do ! --> do ni=1,3
         !_______________________________________________________________________
         pgf_x(nle,elem) = sum(gradient_sca(1:3,elem)*hpress_n_bottom)/density_0
         pgf_y(nle,elem) = sum(gradient_sca(4:6,elem)*hpress_n_bottom)/density_0
-
+        
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -798,7 +804,7 @@ end subroutine pressure_force_4_linfs_nemo
 !
 !
 !===============================================================================
-! Calculate pressure gradient force (PGF) based on "Shchepetkin
+! Calculate pressure gradient force (PGF) based on "Shchepetkin 
 ! and McWilliams," A method for computing horizontal pressure-gradient
 ! force in an oceanic model with a nonaligned vertical coordinate",
 ! Journal of geophysical research, vol 108, no C3, 3090
@@ -814,7 +820,7 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
     use g_config
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
-    type(t_partit), intent(inout),  target :: partit
+    type(t_partit), intent(inout),  target :: partit    
     integer                                :: elem, elnodes(3), nle, ule, nlz, idx(3), ni
     real(kind=WP)                          :: int_dp_dx(2), drho_dx, dz_dx, aux_sum
     real(kind=WP)                          :: dx10(3), dx20(3), dx21(3), df10(3), df21(3), drho_dz(3)
@@ -833,10 +839,10 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
         ! nle...number of mid-depth levels at elem
         nle     = nlevels(elem)-1
         ule     = ulevels(elem)
-
-        ! node indices of elem
+        
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+        
         !_______________________________________________________________________
         ! calculate mid depth element level --> Z_e
         ! nle...number of mid-depth levels at elem
@@ -849,34 +855,34 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
             Z_n(nlz-1)  = zbar_n(nlz)   + helem(nlz-1,elem)*0.5_WP
         end do
         zbar_n(ule) = zbar_n(ule+1) + helem(ule,elem)
-
+        
         !_______________________________________________________________________
-        ! Calculate pressure gradient force (PGF) based on "Shchepetkin
+        ! Calculate pressure gradient force (PGF) based on "Shchepetkin 
         ! and McWilliams," A method for computing horizontal pressure-gradient
         ! force in an oceanic model with a nonaligned vertical coordinate",
         ! Journal of geophysical research, vol 108, no C3, 3090
         ! --> based on density jacobian method ...
         ! --> equation 1.7
-        !
+        ! 
         ! -1/rho_0* dP/dx|_z = -1/rho_0*dP/dx|_z=eta - g/rho_0*int_z^eta( drho/dx|_z)*dz'
         !
         !                    = g*rho(eta)/rho_0*deta/dx
         !                      - g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
         !
         ! --> eta...free surface elevation for linfs start at zero
-
-
+        
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for surface layer
         nlz=ule
-        if (ule==1) then
+        if (ule==1) then 
             !___________________________________________________________________
             ! zonal gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(1)    = aux_sum
-
+                
             !___________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
@@ -885,12 +891,12 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
             int_dp_dx(2)    = aux_sum
         else
             !___________________________________________________________________
-            ! vertical gradient --> with average density and average
+            ! vertical gradient --> with average density and average 
             ! mid-depth level on element
             !       x0        x1               x2
             !  -----o---------o----------------o-------
             !       f0        f1               f2
-            ! --> use Newtonsche interpolation polynom of second order to calculate
+            ! --> use Newtonsche interpolation polynom of second order to calculate 
             ! central difference quotient for not equidestant distributed values
             ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
             !  | |
@@ -898,11 +904,11 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
             !  |
             !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
             !  |
-            !  +--> a0 = f0 ,
-            !       a1 = (f1-f0)/(x1-x0) ,
+            !  +--> a0 = f0 , 
+            !       a1 = (f1-f0)/(x1-x0) , 
             !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
             !
-            ! f'(x) = (f1-f0)/(x1-x0)
+            ! f'(x) = (f1-f0)/(x1-x0) 
             !         - [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x1)*(x1-x0)*(x2-x0)]
             !           *[(x-x1)+(x-x0)]
             !
@@ -912,11 +918,11 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
                 if (idx(ni)==0) then
                     ! elemental surface index nlz ist also surface index of vertice ni
                     !_______________________________________________________________
-                    ! interpolation coefficients
+                    ! interpolation coefficients 
                     dx10(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz+1,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
-
+                    
                     df10(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                     df21(ni)   = density_m_rho0(nlz+2,elnodes(ni))-density_m_rho0(nlz+1,elnodes(ni))
                     !_______________________________________________________________
@@ -927,11 +933,11 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
                 else
                     ! elemental surface index nlz ist deeper than surface index of vertices ni
                     !_______________________________________________________________
-                    ! interpolation coefficients
+                    ! interpolation coefficients 
                     dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                    
                     df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                     df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                     !_______________________________________________________________
@@ -939,27 +945,27 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
                     drho_dz(ni)= df10(ni)/dx10(ni) + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))&
                                 *((Z_n(nlz)-Z_3d_n(nlz  ,elnodes(ni))) + &
                                 (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))))
-                end if
+                end if 
             end do
-
+            
             !___________________________________________________________________
             ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
             ! zonal gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(1:3,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0               
             pgf_x(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(1)    = aux_sum
-
+            
             !___________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(4:6,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0             
             pgf_y(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(2)    = aux_sum
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for surface layer +1 until one layer above bottom
         do nlz=ule+1,nle-1
@@ -971,22 +977,22 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + aux_sum
-
+            
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + aux_sum
-
+            
         end do ! --> do nlz=1,nle-1
-
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for bottom layer
         ! vertical gradient --> with average density on element
         !       x0        x1               x2
         !  -----o---------o----------------o-------
         !       f0        f1               f2
-        ! --> use Newtonsche interpolation polynom of second order to calculate
+        ! --> use Newtonsche interpolation polynom of second order to calculate 
         ! central difference quotient for not equidestant distributed values
         ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
         !  | |
@@ -994,26 +1000,26 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
         !  |
         !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
         !  |
-        !  +--> a0 = f0 ,
-        !       a1 = (f1-f0)/(x1-x0) ,
+        !  +--> a0 = f0 , 
+        !       a1 = (f1-f0)/(x1-x0) , 
         !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
         !
-        ! f'(x) = (f1-f0)/(x1-x0)
+        ! f'(x) = (f1-f0)/(x1-x0) 
         !         - [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x1)*(x1-x0)*(x2-x0)]
         !           *[(x-x1)+(x-x0)]
         !
-        nlz = nle
+        nlz = nle 
         idx = (/1, 1, 1/)*nlz
         idx = nlevels_nod2D(elnodes)-1 - idx
         do ni=1,3
             if (idx(ni)==0) then
                 ! elemental bottom index nlz ist also bottom index of vertice ni
                 !_______________________________________________________________
-                ! interpolation coefficients
+                ! interpolation coefficients 
                 dx10(ni)   = Z_3d_n(nlz-1,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
-
+                
                 df10(ni)   = density_m_rho0(nlz-1,elnodes(ni))-density_m_rho0(nlz-2,elnodes(ni))
                 df21(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                 !_______________________________________________________________
@@ -1024,11 +1030,11 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
             else
                 ! elemental bottom index nlz ist shallower than bottom index of vertice ni
                 !_______________________________________________________________
-                ! interpolation coefficients
+                ! interpolation coefficients 
                 dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                
                 df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                 df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                 !_______________________________________________________________
@@ -1036,26 +1042,26 @@ subroutine pressure_force_4_linfs_shchepetkin(partit, mesh)
                 drho_dz(ni)= df10(ni)/dx10(ni) + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))&
                              *((Z_n(nlz)-Z_3d_n(nlz  ,elnodes(ni))) + &
                                (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))))
-            end if
+            end if 
         end do
-
+        
         !_______________________________________________________________________
         ! - g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
         ! zonal gradients
         drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
         dz_dx           = sum(gradient_sca(1:3,elem)*Z_3d_n(nlz,elnodes))
         !!PS aux_sum         = (drho_dx-drho_dz)*dz_dx)*helem(nlz,elem)*g/density_0
-        aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+        aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0               
         pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
-
+        
         !_______________________________________________________________________
         ! meridional gradients
         drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
         dz_dx           = sum(gradient_sca(4:6,elem)*Z_3d_n(nlz,elnodes))
         !!PS aux_sum         = (drho_dx-drho_dz*dz_dx)*helem(nlz,elem)*g/density_0
-        aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+        aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0             
         pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
-
+        
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -1064,7 +1070,7 @@ end subroutine pressure_force_4_linfs_shchepetkin
 !
 !
 !===============================================================================
-! Calculate pressure gradient force (PGF)
+! Calculate pressure gradient force (PGF) 
 ! First coded by P. Scholz for FESOM2.0, 08.02.2019
 subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
     use o_PARAM
@@ -1079,7 +1085,7 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
     type(t_partit), intent(inout),  target :: partit
-    type(t_tracer), intent(in),     target :: tracers
+    type(t_tracer), intent(in),     target :: tracers    
     integer                                :: elem, elnodes(3), nle, ule, nlz, idx(3),ni
     real(kind=WP)                          :: int_dp_dx(2), drho_dx, aux_sum
     real(kind=WP)                          :: dx10(3), dx20(3), dx21(3)
@@ -1108,10 +1114,10 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
         ! nle...number of mid-depth levels at elem
         nle     = nlevels(elem)-1
         ule     = ulevels(elem)
-
-        ! node indices of elem
+        
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+        
         !_______________________________________________________________________
         ! calculate mid depth element level --> Z_e
         ! nle...number of mid-depth levels at elem
@@ -1124,32 +1130,32 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
             Z_n(nlz-1)  = zbar_n(nlz)   + helem(nlz-1,elem)*0.5_WP
         end do
         zbar_n(ule) = zbar_n(ule+1) + helem(ule,elem)
-
+        
         !_______________________________________________________________________
         aux_dref     = density_0
         if (use_cavity .and. .not. use_density_ref) then
             select case(state_equation)
                 case(0)
-                    call density_linear(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot)
+                    call density_linear(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot, partit, mesh)
                 case(1)
-                    call densityJM_components(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot)
+                    call densityJM_components(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot, partit, mesh)
                 case default !unknown
                     if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                     call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
             end select
         end if
-
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for surface layer
         nlz=ule
-        if (ule==1) then
+        if (ule==1) then 
             !___________________________________________________________________
             ! zonal gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(1)    = aux_sum
-
+                
             !___________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
@@ -1158,12 +1164,12 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
             int_dp_dx(2)    = aux_sum
         else
             !___________________________________________________________________
-            ! vertical gradient --> with average density and average
+            ! vertical gradient --> with average density and average 
             ! mid-depth level on element
             !       x0        x1               x2
             !  -----o---------o----------------o-------
             !       f0        f1               f2
-            ! --> use Newtonsche interpolation polynom of second order to calculate
+            ! --> use Newtonsche interpolation polynom of second order to calculate 
             ! central difference quotient for not equidestant distributed values
             ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
             !  | |
@@ -1171,23 +1177,23 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
             !  |
             !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
             !  |
-            !  +--> a0 = f0 ,
-            !       a1 = (f1-f0)/(x1-x0) ,
+            !  +--> a0 = f0 , 
+            !       a1 = (f1-f0)/(x1-x0) , 
             !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
             !
-            ! f(x) = f0
-            !        + (f1-f0)/(x1-x0)*(x-x0)
+            ! f(x) = f0 
+            !        + (f1-f0)/(x1-x0)*(x-x0) 
             !        + [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x0)*(x2-x1)*(x1-x0)]
             !          *[(x-x1)+(x-x0)]
             !
             !
-
+            
             ! account for reference density when using cavities
             if (use_cavity .and. .not. use_density_ref) then
                 aux_dref = dref_bulk_0   + Z_n(nlz)*dref_bulk_pz   + Z_n(nlz)*dref_bulk_pz2
                 aux_dref = aux_dref*dref_rhopot/(aux_dref+0.1_WP*Z_n(nlz))
-            end if
-
+            end if 
+            
             idx = (/1, 1, 1/)*nlz
             idx = idx - ulevels_nod2D(elnodes)
             do ni=1,3
@@ -1198,38 +1204,38 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
                     dx10(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz+1,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
-
+                    
                     t0(ni)     = temp(nlz  ,elnodes(ni))
                     dt10(ni)   = temp(nlz+1,elnodes(ni))-temp(nlz  ,elnodes(ni))
                     dt21(ni)   = temp(nlz+2,elnodes(ni))-temp(nlz+1,elnodes(ni))
-
+                    
                     s0(ni)     = salt(nlz  ,elnodes(ni))
                     ds10(ni)   = salt(nlz+1,elnodes(ni))-salt(nlz  ,elnodes(ni))
                     ds21(ni)   = salt(nlz+2,elnodes(ni))-salt(nlz+1,elnodes(ni))
                     !___________________________________________________________________
                     ! interpoalte vertice temp and salinity to elemental level Z_n
                     temp_at_Zn(ni) = t0(ni) &
-                                    + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) &
+                                    + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) & 
                                     + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                     (Z_n(nlz)-Z_3d_n(nlz+1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))
                     salt_at_Zn(ni) = s0(ni) &
-                                    + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) &
+                                    + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) & 
                                     + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                     (Z_n(nlz)-Z_3d_n(nlz+1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))
                     !___________________________________________________________________
-                    ! compute density from state equation
+                    ! compute density from state equation 
                     select case(state_equation)
                         case(0)
-                            call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                            call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)   
                         case(1)
-                            call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                            call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)   
                         case default !unknown
                             if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                             call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
                     end select
                     rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
                     rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
-
+                    
                 else
                     ! elemental surface index nlz ist deeper than surface index of vertices ni
                     !___________________________________________________________________
@@ -1237,40 +1243,40 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
                     dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                    
                     t0(ni)     = temp(nlz-1,elnodes(ni))
                     dt10(ni)   = temp(nlz  ,elnodes(ni))-temp(nlz-1,elnodes(ni))
                     dt21(ni)   = temp(nlz+1,elnodes(ni))-temp(nlz  ,elnodes(ni))
-
+                    
                     s0(ni)     = salt(nlz-1,elnodes(ni))
                     ds10(ni)   = salt(nlz  ,elnodes(ni))-salt(nlz-1,elnodes(ni))
                     ds21(ni)   = salt(nlz+1,elnodes(ni))-salt(nlz  ,elnodes(ni))
-                    !___________________________________________________________________
+                    !___________________________________________________________________               
                     ! interpoalte vertice temp and salinity to elemental level Z_n
                     temp_at_Zn(ni) = t0(ni) &
-                                    + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) &
+                                    + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
                                     + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                     (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
                     salt_at_Zn(ni) = s0(ni) &
-                                    + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) &
+                                    + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
                                     + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                     (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
                     !___________________________________________________________________
-                    ! compute density from state equation
+                    ! compute density from state equation 
                     select case(state_equation)
                         case(0)
-                            call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                            call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)    
                         case(1)
-                            call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                            call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)    
                         case default !unknown
                             if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                             call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
                     end select
                     rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
                     rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
-                end if
+                end if 
             end do
-
+            
             !___________________________________________________________________
             ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
             ! zonal gradients
@@ -1278,15 +1284,15 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(1)    = aux_sum
-
+            
             !___________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*rho_at_Zn)
-            aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
+            aux_sum         = drho_dx*helem(nlz,elem)*g/density_0            
             pgf_y(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(2)    = aux_sum
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for surface layer +1 until one layer above bottom
         do nlz=ule+1,nle-1
@@ -1298,22 +1304,22 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + aux_sum
-
+            
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
             pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + aux_sum
-
+            
         end do ! --> do nlz=1,nle-1
-
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for bottom layer
         ! vertical gradient --> with average density on element
         !       x0        x1               x2
         !  -----o---------o----------------o-------
         !       f0        f1               f2
-        ! --> use Newtonsche interpolation polynom of second order to calculate
+        ! --> use Newtonsche interpolation polynom of second order to calculate 
         ! central difference quotient for not equidestant distributed values
         ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
         !  | |
@@ -1321,15 +1327,15 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
         !  |
         !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
         !  |
-        !  +--> a0 = f0 ,
-        !       a1 = (f1-f0)/(x1-x0) ,
+        !  +--> a0 = f0 , 
+        !       a1 = (f1-f0)/(x1-x0) , 
         !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
         !
-        ! f'(x) = (f1-f0)/(x1-x0)
+        ! f'(x) = (f1-f0)/(x1-x0) 
         !         - [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x1)*(x1-x0)*(x2-x0)]
         !           *[(x-x1)+(x-x0)]
         !
-        nlz = nle
+        nlz = nle 
         idx = (/1, 1, 1/)*nlz
         idx = nlevels_nod2D(elnodes)-1 - idx
         do ni=1,3
@@ -1340,38 +1346,38 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
                 dx10(ni)   = Z_3d_n(nlz-1,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
-
+                
                 t0(ni)     = temp(nlz-2,elnodes(ni))
                 dt10(ni)   = temp(nlz-1,elnodes(ni))-temp(nlz-2,elnodes(ni))
                 dt21(ni)   = temp(nlz  ,elnodes(ni))-temp(nlz-1,elnodes(ni))
-
+                
                 s0(ni)     = salt(nlz-2,elnodes(ni))
                 ds10(ni)   = salt(nlz-1,elnodes(ni))-salt(nlz-2,elnodes(ni))
                 ds21(ni)   = salt(nlz  ,elnodes(ni))-salt(nlz-1,elnodes(ni))
                 !_________________________________________________________________
                 ! interpoalte vertice temp and salinity to elemental level Z_n
                 temp_at_Zn(ni) = t0(ni) &
-                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) &
+                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) & 
                                 + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                   (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni)))
                 salt_at_Zn(ni) = s0(ni) &
-                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) &
+                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) & 
                                 + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                   (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni)))
                 !___________________________________________________________________
-                ! compute density from state equation
+                ! compute density from state equation 
                 select case(state_equation)
                     case(0)
-                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
                     case(1)
-                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
                     case default !unknown
                         if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                         call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
                 end select
                 rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
                 rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
-
+                
             else
                 ! elemental bottom index nlz ist shallower than bottom index of vertice ni
                 !___________________________________________________________________
@@ -1379,53 +1385,53 @@ subroutine pressure_force_4_linfs_easypgf(tracers, partit, mesh)
                 dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                
                 t0(ni)     = temp(nlz-1,elnodes(ni))
                 dt10(ni)   = temp(nlz  ,elnodes(ni))-temp(nlz-1,elnodes(ni))
                 dt21(ni)   = temp(nlz+1,elnodes(ni))-temp(nlz  ,elnodes(ni))
-
+                
                 s0(ni)     = salt(nlz-1,elnodes(ni))
                 ds10(ni)   = salt(nlz  ,elnodes(ni))-salt(nlz-1,elnodes(ni))
                 ds21(ni)   = salt(nlz+1,elnodes(ni))-salt(nlz  ,elnodes(ni))
-                !___________________________________________________________________
+                !___________________________________________________________________               
                 ! interpoalte vertice temp and salinity to elemental level Z_n
                 temp_at_Zn(ni) = t0(ni) &
-                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) &
+                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
                                 + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                   (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
                 salt_at_Zn(ni) = s0(ni) &
-                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) &
+                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
                                 + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
                                   (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
                 !___________________________________________________________________
-                ! compute density from state equation
+                ! compute density from state equation 
                 select case(state_equation)
                     case(0)
-                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
                     case(1)
-                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
                     case default !unknown
                         if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                         call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
                 end select
                 rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
                 rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
-            end if
+            end if 
         end do
-
+        
         !_______________________________________________________________________
         ! - g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
         ! zonal gradients
         drho_dx         = sum(gradient_sca(1:3,elem)*rho_at_Zn)
-        aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
+        aux_sum         = drho_dx*helem(nlz,elem)*g/density_0            
         pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
-
+        
         !_______________________________________________________________________
         ! meridional gradients
         drho_dx         = sum(gradient_sca(4:6,elem)*rho_at_Zn)
-        aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
+        aux_sum         = drho_dx*helem(nlz,elem)*g/density_0            
         pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
-
+        
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -1445,11 +1451,11 @@ subroutine pressure_force_4_linfs_cubicspline(partit, mesh)
     use g_config
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
-    type(t_partit), intent(inout),  target :: partit
+    type(t_partit), intent(inout),  target :: partit    
     integer                                :: elem, elnodes(3), nle, ule, nlz, nlc, ni, node, nln(3), uln(3), dd
     real(kind=WP)                          :: int_dp_dx(2), drho_dx, dz_dx, drho_dz, auxp
     real(kind=WP)                          :: dx10, dx20, dx21, df10, df21
-    real(kind=WP)                          :: interp_n_dens(3)
+    real(kind=WP)                          :: interp_n_dens(3) 
     integer                                :: s_ind(4)
     real(kind=WP)                          :: s_z(4), s_dens(4), s_H, aux1, aux2, s_dup, s_dlo
     real(kind=WP)                          :: a, b, c, d, dz
@@ -1469,10 +1475,10 @@ subroutine pressure_force_4_linfs_cubicspline(partit, mesh)
         ! nle...number of mid-depth levels at elem
         nle     = nlevels(elem)-1
         ule     = ulevels(elem)
-
-        ! node indices of elem
+        
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+        
         ! calculate mid depth element level --> Z_e
         ! nle...number of mid-depth levels at elem
         zbar_n       = 0.0_WP
@@ -1484,17 +1490,17 @@ subroutine pressure_force_4_linfs_cubicspline(partit, mesh)
             Z_n(nlz-1)  = zbar_n(nlz)   + helem(nlz-1,elem)/2.0_WP
         end do
         zbar_n(1) = zbar_n(2) + helem(1,elem)
-
-        ! nln...number of mid depth levels at each node that corresponds to
+        
+        ! nln...number of mid depth levels at each node that corresponds to 
         ! element elem
         nln=nlevels_nod2D(elnodes)-1
         uln=ulevels_nod2D(elnodes)
-
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for surface layer
         int_dp_dx     = 0.0_WP
         nlz=ule
-
+        
         if (ule==1) then ! in case no cavity
             !___________________________________________________________________
             ! zonal gradients
@@ -1502,40 +1508,40 @@ subroutine pressure_force_4_linfs_cubicspline(partit, mesh)
             auxp            = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = int_dp_dx(1) + auxp*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + auxp
-
+            
             !___________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             auxp            = drho_dx*helem(nlz,elem)*g/density_0
             pgf_y(nlz,elem) = int_dp_dx(2) + auxp*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + auxp
-
+            
         else! in case of cavity (includes pressure bnd condition under cavity)
             !___________________________________________________________________
             ! zonal gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
-
-            ! cavity-ocean interface pressure boundary condition
+            
+            ! cavity-ocean interface pressure boundary condition 
             auxp            = drho_dx*(-zbar_e_srf(elem))*g/density_0
             int_dp_dx(1)    = int_dp_dx(1) + auxp
-
+            
             auxp            = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = int_dp_dx(1) + auxp*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + auxp
-
+                
             !___________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
-
-            ! cavity-ocean interface pressure boundary condition
+            
+            ! cavity-ocean interface pressure boundary condition 
             auxp            = drho_dx*(-zbar_e_srf(elem))*g/density_0
             int_dp_dx(2)    = int_dp_dx(2) + auxp
-
+            
             auxp            = drho_dx*helem(nlz,elem)*g/density_0
             pgf_y(nlz,elem) = int_dp_dx(2) + auxp*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + auxp
-        endif
-
+        endif 
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for surface layer+1 until one layer above bottom
         !!PS do nlz=1,nle-1
@@ -1548,24 +1554,24 @@ subroutine pressure_force_4_linfs_cubicspline(partit, mesh)
             auxp            = drho_dx*helem(nlz,elem)*g/density_0
             pgf_x(nlz,elem) = int_dp_dx(1) + auxp*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + auxp
-
+            
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             auxp            = drho_dx*helem(nlz,elem)*g/density_0
             pgf_y(nlz,elem) = int_dp_dx(2) + auxp*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + auxp
-
+            
         end do ! --> do nlz=1,nle-1
-
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for bottom layer
-        nlz = nle
-
+        nlz = nle 
+        
         ! loop over the three node points that span up element
         do ni =1, 3
             ! node...
             node = elnodes(ni)
-
+            
             !___________________________________________________________________
             ! calculate vertical center index
             nlc=nln(ni)-1
@@ -1575,67 +1581,67 @@ subroutine pressure_force_4_linfs_cubicspline(partit, mesh)
                     nlc = dd-1
                     if (dd==1) nlc=1
                     exit
-                end if
+                end if 
             end do
             ! --> this is 10% slower than the above one
-            ! nlc   = minloc( Z_3d_n(1:nln(ni),elnodes(ni))-Z_n(nlz),1,&
+            ! nlc   = minloc( Z_3d_n(1:nln(ni),elnodes(ni))-Z_n(nlz),1,& 
             !                 Z_3d_n(1:nln(ni),elnodes(ni))-Z_n(nlz)>0.0_WP &  ! mask for index selection
             !                )
-            ! nlc   = min(nlc,nln(ni)-1)
-
+            ! nlc   = min(nlc,nln(ni)-1) 
+            
             !___________________________________________________________________
-            ! "Cubic spline interpolation computes a third order polynomial only
-            ! from two data points with the additional constraint that the first
-            ! and second derivative at the interpolation points are continuous.
-            ! So if you have 4 points, then you compute 3 different polynomials
-            !(between points 1-2, 2-3, and 3-4), and these polynomials are
-            ! smoothly connected in the sense that their first and second
+            ! "Cubic spline interpolation computes a third order polynomial only 
+            ! from two data points with the additional constraint that the first 
+            ! and second derivative at the interpolation points are continuous. 
+            ! So if you have 4 points, then you compute 3 different polynomials 
+            !(between points 1-2, 2-3, and 3-4), and these polynomials are 
+            ! smoothly connected in the sense that their first and second 
             ! derivatives are equal at the given data points." --> as a comment
-            ! prepare cubic spline interpolation
+            ! prepare cubic spline interpolation 
             s_ind=(/nlc-1,nlc,nlc+1,nlc+2/)
-
-            ! we are at the bottom
+            
+            ! we are at the bottom 
             s_ind(4)=nlc+1
-
+            
             s_z    = Z_3d_n(s_ind,node)
             s_dens = density_m_rho0(s_ind,node)
             s_H    = s_z(3)-s_z(2)
             aux1   = (s_dens(3)-s_dens(2))/s_H
-
+            
             !___________________________________________________________________
-            ! calculate derivatives in a way to get monotonic profile
+            ! calculate derivatives in a way to get monotonic profile 
             ! --> bottom case (see FESOM1.4)
             aux2=(s_dens(2)-s_dens(1))/(s_z(2)-s_z(1))
             s_dup=0.0_WP
             if(aux1*aux2>0._WP)  s_dup=2.0_WP*aux1*aux2/(aux1+aux2)
             s_dlo=1.5_WP*aux1-0.5_WP*s_dup
-
+            
             !___________________________________________________________________
             ! cubic polynomial coefficients
             a=s_dens(2)
             b=s_dup
             c=-(2.0_WP*s_dup+s_dlo)/s_H + 3.0_WP*(s_dens(3)-s_dens(2))/s_H**2
             d=(s_dup+s_dlo)/s_H**2 - 2.0_WP*(s_dens(3)-s_dens(2))/s_H**3
-
+            
             !___________________________________________________________________
             ! interpolate
             dz=Z_n(nlz)-s_z(2)
             interp_n_dens(ni)=a+b*dz+c*dz**2+d*dz**3
-
+            
         end do ! --> do ni=1,3
-
+        
         ! zonal gradients
         drho_dx         = sum(gradient_sca(1:3,elem)*interp_n_dens)
         auxp            = drho_dx*helem(nlz,elem)*g/density_0
         pgf_x(nlz,elem) = int_dp_dx(1) + auxp*0.5_WP
         int_dp_dx(1)    = int_dp_dx(1) + auxp
-
+            
         ! meridional gradients
         drho_dx         = sum(gradient_sca(4:6,elem)*interp_n_dens)
         auxp            = drho_dx*helem(nlz,elem)*g/density_0
         pgf_y(nlz,elem) = int_dp_dx(2) + auxp*0.5_WP
         int_dp_dx(2)    = int_dp_dx(2) + auxp
-
+        
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -1644,7 +1650,7 @@ end subroutine pressure_force_4_linfs_cubicspline
 !
 !
 !===============================================================================
-! calculate pressure gradient force for linfs in case cavities are used with
+! calculate pressure gradient force for linfs in case cavities are used with 
 ! surface partial cells or bottom partial cells
 subroutine pressure_force_4_linfs_cavity(partit, mesh)
     use o_PARAM
@@ -1655,7 +1661,7 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
     use g_config
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
-    type(t_partit), intent(inout),  target :: partit
+    type(t_partit), intent(inout),  target :: partit    
     integer                                :: elem, elnodes(3), nle, ule,  nlz, idx(3), ni
     real(kind=WP)                          :: int_dp_dx(2), drho_dx, dz_dx, aux_sum
     real(kind=WP)                          :: dx10(3), dx20(3), dx21(3), df10(3), df21(3), drho_dz(3)
@@ -1665,7 +1671,7 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
-
+    
     !___________________________________________________________________________
     ! loop over triangular elemments
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(elem, elnodes, nle, ule, nlz, idx, ni, int_dp_dx, drho_dx, dz_dx, aux_sum, &
@@ -1676,11 +1682,11 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
         ! number of levels at elem
         nle=nlevels(elem)-1
         ule=ulevels(elem)
-
+            
         !_______________________________________________________________________
-        ! node indices of elem
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+        
         !_______________________________________________________________________
         ! calculate mid depth element level --> Z_e
         ! nle...number of mid-depth levels at elem
@@ -1693,20 +1699,20 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
             Z_n(nlz-1)  = zbar_n(nlz)   + helem(nlz-1,elem)*0.5_WP
         end do
         zbar_n(ule) = zbar_n(ule+1) + helem(ule,elem)
-
+        
         !_______________________________________________________________________
-        ! calculate pressure gradient for surface layer
+        ! calculate pressure gradient for surface layer 
         nlz=ule
-        if (ule==1) then
+        if (ule==1) then 
             pgf_x(nlz,elem) = sum(gradient_sca(1:3,elem)*hpressure(nlz,elnodes)/density_0)
             pgf_y(nlz,elem) = sum(gradient_sca(4:6,elem)*hpressure(nlz,elnodes)/density_0)
         else
             !___________________________________________________________________
-            ! compute vertical density gradient
+            ! compute vertical density gradient 
             !       x0        x1               x2
             !  -----o---------o----------------o-------
             !       f0        f1               f2
-            ! --> use Newtonsche interpolation polynom of second order to calculate
+            ! --> use Newtonsche interpolation polynom of second order to calculate 
             ! central difference quotient for not equidestant distributed values
             ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
             !  | |
@@ -1714,11 +1720,11 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
             !  |
             !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
             !  |
-            !  +--> a0 = f0 ,
-            !       a1 = (f1-f0)/(x1-x0) ,
+            !  +--> a0 = f0 , 
+            !       a1 = (f1-f0)/(x1-x0) , 
             !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
             !
-            ! f'(x) = (f1-f0)/(x1-x0)
+            ! f'(x) = (f1-f0)/(x1-x0) 
             !         - [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x1)*(x1-x0)*(x2-x0)]
             !           *[(x-x1)+(x-x0)]
             !
@@ -1729,11 +1735,11 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
                 if (idx(ni)==0) then
                     ! elemental surface index nlz ist also surface index of vertice ni
                     !_______________________________________________________________
-                    ! interpolation coefficients
+                    ! interpolation coefficients 
                     dx10(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz+1,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
-
+                    
                     df10(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                     df21(ni)   = density_m_rho0(nlz+2,elnodes(ni))-density_m_rho0(nlz+1,elnodes(ni))
                     !_______________________________________________________________
@@ -1744,11 +1750,11 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
                 else
                     ! elemental surface index nlz ist deeper than surface index of vertices ni
                     !_______________________________________________________________
-                    ! interpolation coefficients
+                    ! interpolation coefficients 
                     dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                    
                     df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                     df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                     !_______________________________________________________________
@@ -1756,53 +1762,53 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
                     drho_dz(ni)= df10(ni)/dx10(ni) + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))&
                                 *((Z_n(nlz)-Z_3d_n(nlz  ,elnodes(ni))) + &
                                 (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))))
-                end if
+                end if 
             end do
-
+            
             !_______________________________________________________________________
             ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
             ! zonal gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(1:3,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0               
             pgf_x(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(1)    = aux_sum
-
+             
             !_______________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(4:6,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0             
             pgf_y(nlz,elem) = aux_sum*0.5_WP
             int_dp_dx(2)    = aux_sum
-        end if
-
+        end if 
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for the ocean bulk based on pressure
         do nlz=ule+1,nle-1
             pgf_x(nlz,elem) = sum(gradient_sca(1:3,elem)*hpressure(nlz,elnodes)/density_0)
             pgf_y(nlz,elem) = sum(gradient_sca(4:6,elem)*hpressure(nlz,elnodes)/density_0)
-        end do
-
+        end do 
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for the ocean bottom
         nlz=nle
-        if (.not. use_partial_cell) then
+        if (.not. use_partial_cell) then 
             pgf_x(nlz,elem) = sum(gradient_sca(1:3,elem)*hpressure(nlz,elnodes)/density_0)
             pgf_y(nlz,elem) = sum(gradient_sca(4:6,elem)*hpressure(nlz,elnodes)/density_0)
         else
             !___________________________________________________________________
-            ! (1) compute pgf_xy onto zbar_n(nle-1) --> needed for vertical
+            ! (1) compute pgf_xy onto zbar_n(nle-1) --> needed for vertical 
             ! integration of bottom density gradient from zbar_n(nle-1) until Z_n(nle)
-            int_dp_dx(1) = sum(gradient_sca(1:3,elem)*(hpressure(nlz-1,elnodes) &
+            int_dp_dx(1) = sum(gradient_sca(1:3,elem)*(hpressure(nlz-1,elnodes) & 
                            + 0.5_WP*g*(density_m_rho0(nlz-1,elnodes)*hnode(nlz-1,elnodes)))/density_0)
-            int_dp_dx(2) = sum(gradient_sca(4:6,elem)*(hpressure(nlz-1,elnodes) &
+            int_dp_dx(2) = sum(gradient_sca(4:6,elem)*(hpressure(nlz-1,elnodes) & 
                            + 0.5_WP*g*(density_m_rho0(nlz-1,elnodes)*hnode(nlz-1,elnodes)))/density_0)
-
+                           
             !___________________________________________________________________
-            ! vertical gradient --> with average density and average mid-depth level
+            ! vertical gradient --> with average density and average mid-depth level 
             ! on element
-            ! f'(x) = (f1-f0)/(x1-x0)
+            ! f'(x) = (f1-f0)/(x1-x0) 
             !         - [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x1)*(x1-x0)*(x2-x0)]
             !           *[(x-x1)+(x-x0)]
             idx = (/1, 1, 1/)*nlz
@@ -1811,11 +1817,11 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
                 if (idx(ni)==0) then
                     ! elemental bottom index nlz ist also bottom index of vertice ni
                     !_______________________________________________________________
-                    ! interpolation coefficients
+                    ! interpolation coefficients 
                     dx10(ni)   = Z_3d_n(nlz-1,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
-
+                    
                     df10(ni)   = density_m_rho0(nlz-1,elnodes(ni))-density_m_rho0(nlz-2,elnodes(ni))
                     df21(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                     !_______________________________________________________________
@@ -1826,11 +1832,11 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
                 else
                     ! elemental bottom index nlz ist shallower than bottom index of vertice ni
                     !_______________________________________________________________
-                    ! interpolation coefficients
+                    ! interpolation coefficients 
                     dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                     dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                     dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                    
                     df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                     df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                     !_______________________________________________________________
@@ -1838,25 +1844,25 @@ subroutine pressure_force_4_linfs_cavity(partit, mesh)
                     drho_dz(ni)= df10(ni)/dx10(ni) + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))&
                                 *((Z_n(nlz)-Z_3d_n(nlz  ,elnodes(ni))) + &
                                 (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))))
-                end if
+                end if 
             end do
-
+            
             !___________________________________________________________________
             ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
             ! zonal gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(1:3,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0               
             pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
-
+            
             !_______________________________________________________________________
             ! meridional gradients
             drho_dx         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(4:6,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0             
             pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
-        end if
-
+        end if 
+        
     end do !-->do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -1885,7 +1891,7 @@ subroutine pressure_force_4_zxxxx(tracers, partit, mesh)
     elseif (trim(which_pgf)=='cubicspline') then
         call pressure_force_4_zxxxx_cubicspline(partit, mesh)
     elseif (trim(which_pgf)=='easypgf'    ) then
-        call pressure_force_4_zxxxx_easypgf(tracers, partit, mesh)
+        call pressure_force_4_zxxxx_easypgf(tracers, partit, mesh)    
     else
         write(*,*) '________________________________________________________'
         write(*,*) ' --> ERROR: the choosen form of pressure gradient       '
@@ -1901,8 +1907,8 @@ end subroutine pressure_force_4_zxxxx
 !
 !
 !===============================================================================
-! Calculate pressure gradient force (PGF) on elements by interpolating the nodal
-! density on nodal depthlayers on the depth of the elments using a cubic-spline
+! Calculate pressure gradient force (PGF) on elements by interpolating the nodal 
+! density on nodal depthlayers on the depth of the elments using a cubic-spline 
 ! interpolation.
 ! First coded by Q. Wang for FESOM1.4, adapted by P. Scholz for FESOM2.0
 ! 26.04.2018
@@ -1915,7 +1921,7 @@ subroutine pressure_force_4_zxxxx_cubicspline(partit, mesh)
     use g_config
     implicit none
     type(t_mesh),   intent(in) ,    target :: mesh
-    type(t_partit), intent(inout),  target :: partit
+    type(t_partit), intent(inout),  target :: partit    
     integer                                :: elem, elnodes(3), nle, ule, nln(3), uln(3), nlz, nlc, dd
     integer                                :: ni, node, dens_ind, kk
     real(kind=WP)                          :: ze
@@ -1949,32 +1955,32 @@ subroutine pressure_force_4_zxxxx_cubicspline(partit, mesh)
         end do
         !!PS zbar_n(1) = zbar_n(2) + helem(1,elem)
         zbar_n(ule) = zbar_n(ule+1) + helem(ule,elem)
-
+        
         !_______________________________________________________________________
-        ! node indices of elem
+        ! node indices of elem 
         elnodes = elem2D_nodes(:,elem)
-
+        
         !_______________________________________________________________________
-        ! nln...number of mid depth levels at each node that corresponds to
+        ! nln...number of mid depth levels at each node that corresponds to 
         ! element elem
         nln=nlevels_nod2D(elnodes)-1
         uln=ulevels_nod2D(elnodes)
-
+        
         !_______________________________________________________________________
         ! loop over mid-depth levels at element elem
         p_grad=0.0_WP
         !!PS do nlz=1,nle
         do nlz=ule,nle
-
+            
             !___________________________________________________________________
             rho_n = 0.0_WP
             ! loop over the three node points that span up element elem
             do ni=1,3
                 ! node...
                 node = elnodes(ni)
-
+                
                 !_______________________________________________________________
-                ! calculate vertical center index
+                ! calculate vertical center index 
                 nlc=nln(ni)-1
                 !!PS do dd=1,nln(ni)
                 do dd=uln(ni),nln(ni)
@@ -1982,18 +1988,18 @@ subroutine pressure_force_4_zxxxx_cubicspline(partit, mesh)
                         nlc = dd-1
                         if (dd==1) nlc=1
                         exit
-                    end if
+                    end if 
                 end do
                 ! --> this is 10% slower than the above one
-                ! nlc   = minloc( Z_3d_n(1:nln(ni),elnodes(ni))-Z_n(nlz),1,&
+                ! nlc   = minloc( Z_3d_n(1:nln(ni),elnodes(ni))-Z_n(nlz),1,& 
                 !                 Z_3d_n(1:nln(ni),elnodes(ni))-Z_n(nlz)>0.0_WP)
-                ! nlc   = max(nlc,1)
-                ! nlc   = min(nlc,nln(ni)-1)
-
+                ! nlc   = max(nlc,1)                
+                ! nlc   = min(nlc,nln(ni)-1) 
+                
                 !_______________________________________________________________
-                ! prepare cubic spline interpolation
+                ! prepare cubic spline interpolation 
                 s_ind=(/nlc-1, nlc, nlc+1, nlc+2/)
-
+                
                 !_______________________________________________________________
                 ! calculate derivatives in a way to get monotonic profile
                 ! distinguish between surface, bottom and bulg
@@ -2002,42 +2008,42 @@ subroutine pressure_force_4_zxxxx_cubicspline(partit, mesh)
                     !___________________________________________________________
 !!PS                     s_ind(1)=1
                     s_ind(1)=uln(ni)
-
+                    
                     !___________________________________________________________
                     s_z    = Z_3d_n(s_ind,node)
                     s_dens = density_m_rho0(s_ind,node)
                     s_H    = s_z(3)-s_z(2)
                     aux1   = (s_dens(3)-s_dens(2))/s_H
-
+                    
                     !___________________________________________________________
                     aux2=(s_dens(4)-s_dens(3))/(s_z(4)-s_z(3))
                     s_dlo=0.0_WP
                     if(aux1*aux2>0._WP) s_dlo=2.0_WP*aux1*aux2/(aux1+aux2)
                     s_dup=1.5_WP*aux1-0.5_WP*s_dlo
-
+                    
                 elseif (nlc == nln(ni)-1) then! bottom case
                     !___________________________________________________________
                     s_ind(4)=nlc+1
-
+                    
                     !___________________________________________________________
                     s_z    = Z_3d_n(s_ind,node)
                     s_dens = density_m_rho0(s_ind,node)
                     s_H    = s_z(3)-s_z(2)
                     aux1   = (s_dens(3)-s_dens(2))/s_H
-
+                    
                     !___________________________________________________________
                     aux2=(s_dens(2)-s_dens(1))/(s_z(2)-s_z(1))
                     s_dup=0.0_WP
                     if(aux1*aux2>0._WP)  s_dup=2.0_WP*aux1*aux2/(aux1+aux2)
                     s_dlo=1.5_WP*aux1-0.5_WP*s_dup
-
+                    
                 else ! bulk, subsurface/above bottom case
                     !___________________________________________________________
                     s_z    = Z_3d_n(s_ind,node)
                     s_dens = density_m_rho0(s_ind,node)
                     s_H    = s_z(3)-s_z(2)
                     aux1   = (s_dens(3)-s_dens(2))/s_H
-
+                    
                     !___________________________________________________________
                     aux2=(s_dens(2)-s_dens(1))/(s_z(2)-s_z(1))
                     s_dup=0.0_WP
@@ -2045,42 +2051,42 @@ subroutine pressure_force_4_zxxxx_cubicspline(partit, mesh)
                     aux2=(s_dens(4)-s_dens(3))/(s_z(4)-s_z(3))
                     s_dlo=0.0_WP
                     if(aux1*aux2>0._WP) s_dlo=2.0_WP*aux1*aux2/(aux1+aux2)
-
+                    
                 end if
-
+                
                 !_______________________________________________________________
                 ! cubic polynomial coefficients
                 a=s_dens(2)
                 b=s_dup
                 c=-(2.0_WP*s_dup+s_dlo)/s_H + 3.0_WP*(s_dens(3)-s_dens(2))/s_H**2
                 d=(s_dup+s_dlo)/s_H**2 - 2.0_WP*(s_dens(3)-s_dens(2))/s_H**3
-
+                
                 !_______________________________________________________________
                 ! interpolate
                 dz=Z_n(nlz)-s_z(2)
                 rho_n(ni)=a+b*dz+c*dz**2+d*dz**3
-
+                
             end do ! --> do ni=1,3
-
+            
             !___________________________________________________________________
             ! calculate element wise density gradient
             rhograd_e(1) = sum(gradient_sca(1:3,elem)*rho_n)
             rhograd_e(2) = sum(gradient_sca(4:6,elem)*rho_n)
-
+            
             !___________________________________________________________________
-            ! calculate element wise pressure gradient force
-            ! helem ... here because of vertical integral
+            ! calculate element wise pressure gradient force 
+            ! helem ... here because of vertical integral 
             aux             = g*helem(nlz,elem)*rhograd_e/density_0
-
-            ! *0.5 because pgf_xy is calculated at mid depth levels but at
+            
+            ! *0.5 because pgf_xy is calculated at mid depth levels but at 
             ! this point p_grad is integrated pressure gradient force until
             ! full depth  layers of previouse depth layer
             pgf_x(nlz,elem) = p_grad(1) + aux(1)*0.5_WP
             pgf_y(nlz,elem) = p_grad(2) + aux(2)*0.5_WP
-
+            
             ! integration to full depth levels
             p_grad          = p_grad    + aux
-
+            
         end do ! --> do nlz=1,nle
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
@@ -2090,7 +2096,7 @@ end subroutine pressure_force_4_zxxxx_cubicspline
 !
 !
 !===============================================================================
-! Calculate pressure gradient force (PGF) based on "Shchepetkin
+! Calculate pressure gradient force (PGF) based on "Shchepetkin 
 ! and McWilliams," A method for computing horizontal pressure-gradient
 ! force in an oceanic model with a nonaligned vertical coordinate",
 ! Journal of geophysical research, vol 108, no C3, 3090
@@ -2110,7 +2116,7 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
     type(t_mesh),   intent(in) ,    target :: mesh
     type(t_partit), intent(inout),  target :: partit
     integer                                :: elem, elnodes(3), nle, ule, nlz, nln(3), ni, nlc, nlce, idx(3)
-    real(kind=WP)                          :: int_dp_dx(2), drho_dx, drho_dy, drho_dz(3), dz_dx, dz_dy, aux_sum
+    real(kind=WP)                          :: int_dp_dx(2), drho_dx, drho_dy, drho_dz(3), dz_dx, dz_dy, aux_sum 
     real(kind=WP)                          :: dx10(3), dx20(3), dx21(3), df10(3), df21(3)
     real(kind=WP)                          :: rhopot(3), bulk_0(3), bulk_pz(3), bulk_pz2(3)
     real(kind=WP)                          :: zbar_n(mesh%nl), z_n(mesh%nl-1)
@@ -2118,10 +2124,10 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
-
+    
     !___________________________________________________________________________
     ! loop over triangular elemments
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(elem, elnodes, nle, ule, nlz, nln, ni, nlc, nlce, idx, int_dp_dx, drho_dx, drho_dy, drho_dz, dz_dx, dz_dy, aux_sum, &
+!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(elem, elnodes, nle, ule, nlz, nln, ni, nlc, nlce, idx, int_dp_dx, drho_dx, drho_dy, drho_dz, dz_dx, dz_dy, aux_sum, & 
 !$OMP                                  dx10, dx20, dx21, df10, df21, rhopot, bulk_0, bulk_pz, bulk_pz2, zbar_n, z_n)
 !$OMP DO
     do elem=1, myDim_elem2D
@@ -2129,10 +2135,10 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
         ! nle...number of mid-depth levels at elem
         nle          = nlevels(elem)-1
         ule          = ulevels(elem)
-
-        ! node indices of elem
-        elnodes      = elem2D_nodes(:,elem)
-
+        
+        ! node indices of elem 
+        elnodes      = elem2D_nodes(:,elem) 
+        
         !_______________________________________________________________________
         ! calculate mid depth element level --> Z_e
         ! nle...number of mid-depth levels at elem
@@ -2145,29 +2151,29 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
             Z_n(nlz-1)  = zbar_n(nlz)   + helem(nlz-1,elem)*0.5_WP
         end do
         zbar_n(ule) = zbar_n(ule+1) + helem(ule,elem)
-
+        
         !_______________________________________________________________________
-        ! Calculate pressure gradient force (PGF) based on "Shchepetkin
+        ! Calculate pressure gradient force (PGF) based on "Shchepetkin 
         ! and McWilliams," A method for computing horizontal pressure-gradient
         ! force in an oceanic model with a nonaligned vertical coordinate",
         ! Journal of geophysical research, vol 108, no C3, 3090
         ! --> based on density jacobian method ...
         ! --> equation 1.7
-        !
+        ! 
         ! -1/rho_0* dP/dx|_z = -1/rho_0*dP/dx|_z=eta - g/rho_0*int_z^eta( drho/dx|_z)*dz'
         !
         !                    = g*rho(eta)/rho_0*deta/dx
         !                      - g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
         !
         ! --> g*rho(eta)/rho_0*deta/dx
-
+        
         !_______________________________________________________________________
-        ! vertical gradient --> with average density and average
+        ! vertical gradient --> with average density and average 
         ! mid-depth level on element
         !       x0        x1               x2
         !  -----o---------o----------------o-------
         !       f0        f1               f2
-        ! --> use Newtonsche interpolation polynom of second order to calculate
+        ! --> use Newtonsche interpolation polynom of second order to calculate 
         ! central difference quotient for not equidestant distributed values
         ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
         !  | |
@@ -2175,15 +2181,15 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
         !  |
         !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
         !  |
-        !  +--> a0 = f0 ,
-        !       a1 = (f1-f0)/(x1-x0) ,
+        !  +--> a0 = f0 , 
+        !       a1 = (f1-f0)/(x1-x0) , 
         !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
         !
-        ! f'(x) = (f1-f0)/(x1-x0)
+        ! f'(x) = (f1-f0)/(x1-x0) 
         !         - [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x1)*(x1-x0)*(x2-x0)]
         !           *[(x-x1)+(x-x0)]
         !_______________________________________________________________________
-        ! calculate pressure gradient for surface layer
+        ! calculate pressure gradient for surface layer 
         nlz = ule
         idx = (/1, 1, 1/)*nlz
         idx = idx - ulevels_nod2D(elnodes)
@@ -2191,11 +2197,11 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
             if (idx(ni)==0) then
                 ! elemental surface index nlz ist also surface index of vertice ni
                 !_______________________________________________________________
-                ! interpolation coefficients
+                ! interpolation coefficients 
                 dx10(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz+1,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
-
+                
                 df10(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                 df21(ni)   = density_m_rho0(nlz+2,elnodes(ni))-density_m_rho0(nlz+1,elnodes(ni))
                 !_______________________________________________________________
@@ -2206,11 +2212,11 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
             else
                 ! elemental surface index nlz ist deeper than surface index of vertices ni
                 !_______________________________________________________________
-                ! interpolation coefficients
+                ! interpolation coefficients 
                 dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                
                 df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                 df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                 !_______________________________________________________________
@@ -2218,36 +2224,36 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
                 drho_dz(ni)= df10(ni)/dx10(ni) + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))&
                              *((Z_n(nlz)-Z_3d_n(nlz  ,elnodes(ni))) + &
                                (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))))
-            end if
+            end if 
         end do
-
+        
         !_______________________________________________________________________
         ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
         ! zonal surface pressure gradients
         drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
         dz_dx           = sum(gradient_sca(1:3,elem)*Z_3d_n(nlz,elnodes))
-        aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+        aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0               
         pgf_x(nlz,elem) = aux_sum*0.5_WP
         int_dp_dx(1)    = aux_sum
-
+        
         !_______________________________________________________________________
         ! meridional surface pressure gradients
         drho_dy         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
         dz_dy           = sum(gradient_sca(4:6,elem)*Z_3d_n(nlz,elnodes))
-        aux_sum         = (drho_dy-sum(drho_dz)/3.0_WP*dz_dy)*helem(nlz,elem)*g/density_0
+        aux_sum         = (drho_dy-sum(drho_dz)/3.0_WP*dz_dy)*helem(nlz,elem)*g/density_0             
         pgf_y(nlz,elem) = aux_sum*0.5_WP
         int_dp_dx(2)    = aux_sum
-
+        
         !_______________________________________________________________________
-        ! calculate pressure gradient for subsurface layer until one layer above
-        ! the bottom
+        ! calculate pressure gradient for subsurface layer until one layer above 
+        ! the bottom 
         do nlz=ule+1,nle-1
             !___________________________________________________________________
             ! compute coefficients
             dx10            = Z_3d_n(nlz  ,elnodes)-Z_3d_n(nlz-1,elnodes)
             dx21            = Z_3d_n(nlz+1,elnodes)-Z_3d_n(nlz  ,elnodes)
             dx20            = Z_3d_n(nlz+1,elnodes)-Z_3d_n(nlz-1,elnodes)
-
+            
             df10            = density_m_rho0(nlz  ,elnodes)-density_m_rho0(nlz-1,elnodes)
             df21            = density_m_rho0(nlz+1,elnodes)-density_m_rho0(nlz  ,elnodes)
             !_______________________________________________________________
@@ -2255,40 +2261,40 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
             drho_dz         = df10/dx10 + (dx10*df21-dx21*df10)/(dx20*dx21*dx10)&
                               *(((/1.0,1.0,1.0/)*Z_n(nlz)-Z_3d_n(nlz,elnodes)) + &
                                 ((/1.0,1.0,1.0/)*Z_n(nlz)-Z_3d_n(nlz-1,elnodes)))
-
+            
             !___________________________________________________________________
             ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
             ! zonal bulk pressure gradients
             drho_dx         = sum(gradient_sca(1:3,elem)*density_m_rho0(nlz,elnodes))
             dz_dx           = sum(gradient_sca(1:3,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0               
             pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + aux_sum
-
+            
             !___________________________________________________________________
             ! meridional bulk pressure gradients
             drho_dy         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
             dz_dy           = sum(gradient_sca(4:6,elem)*Z_3d_n(nlz,elnodes))
-            aux_sum         = (drho_dy-sum(drho_dz)/3.0_WP*dz_dy)*helem(nlz,elem)*g/density_0
+            aux_sum         = (drho_dy-sum(drho_dz)/3.0_WP*dz_dy)*helem(nlz,elem)*g/density_0             
             pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + aux_sum
-
+            
         end do ! --> do nlz=2,nle-1
-
+        
         !_______________________________________________________________________
         ! calculate pressure gradient for bottom layer
-        nlz = nle
+        nlz = nle 
         idx = (/1, 1, 1/)*nlz
         idx = nlevels_nod2D(elnodes)-1 - idx
         do ni=1,3
             if (idx(ni)==0) then
                 ! elemental bottom index nlz ist also bottom index of vertice ni
                 !_______________________________________________________________
-                ! interpolation coefficients
+                ! interpolation coefficients 
                 dx10(ni)   = Z_3d_n(nlz-1,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
-
+                
                 df10(ni)   = density_m_rho0(nlz-1,elnodes(ni))-density_m_rho0(nlz-2,elnodes(ni))
                 df21(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                 !_______________________________________________________________
@@ -2299,11 +2305,11 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
             else
                 ! elemental bottom index nlz ist shallower than bottom index of vertice ni
                 !_______________________________________________________________
-                ! interpolation coefficients
+                ! interpolation coefficients 
                 dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
                 dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
                 dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
-
+                
                 df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
                 df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
                 !_______________________________________________________________
@@ -2311,9 +2317,9 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
                 drho_dz(ni)= df10(ni)/dx10(ni) + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))&
                              *((Z_n(nlz)-Z_3d_n(nlz  ,elnodes(ni))) + &
                                (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))))
-            end if
+            end if 
         end do
-
+        
         !_______________________________________________________________________
         ! -->...-g/rho*int_z^eta( drho/dx|_s - drho/dz'*dz'/dx|_s )*dz'
         ! zonal bottom pressure gradients
@@ -2321,14 +2327,14 @@ subroutine pressure_force_4_zxxxx_shchepetkin(partit, mesh)
         dz_dx           = sum(gradient_sca(1:3,elem)*(Z_3d_n(nlz,elnodes)))
         aux_sum         = (drho_dx-sum(drho_dz)/3.0_WP*dz_dx)*helem(nlz,elem)*g/density_0
         pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
-
+        
         !_______________________________________________________________________
         ! meridional bottom pressure gradients
         drho_dy         = sum(gradient_sca(4:6,elem)*density_m_rho0(nlz,elnodes))
         dz_dy           = sum(gradient_sca(4:6,elem)*(Z_3d_n(nlz,elnodes)))
         aux_sum         = (drho_dy-sum(drho_dz)/3.0_WP*dz_dy)*helem(nlz,elem)*g/density_0
         pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
-
+        
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -2337,7 +2343,7 @@ end subroutine pressure_force_4_zxxxx_shchepetkin
 !
 !
 !===============================================================================
-! Calculate pressure gradient force (PGF) based on "Shchepetkin
+! Calculate pressure gradient force (PGF) based on "Shchepetkin 
 ! and McWilliams," A method for computing horizontal pressure-gradient
 ! force in an oceanic model with a nonaligned vertical coordinate",
 ! Journal of geophysical research, vol 108, no C3, 3090
@@ -2357,14 +2363,14 @@ subroutine pressure_force_4_zxxxx_easypgf(tracers, partit, mesh)
     implicit none
     type(t_mesh),   intent(in) ,    target  :: mesh
     type(t_partit), intent(inout),  target  :: partit
-    type(t_tracer), intent(in),     target  :: tracers
-    integer                                 :: elem, elnodes(3), nle, ule, nlz, nln(3), ni, nlc, nlce, idx(3), layer_offset
-    real(kind=WP)                           :: int_dp_dx(2), drho_dx, dz_dx, drho_dy, dz_dy, aux_sum
+    type(t_tracer), intent(in),     target  :: tracers    
+    integer                                 :: elem, elnodes(3), nle, ule, nlz, nln(3), ni, nlc, nlce, idx(3)
+    real(kind=WP)                           :: int_dp_dx(2), drho_dx, dz_dx, drho_dy, dz_dy, aux_sum 
     real(kind=WP)                           :: dx10(3), dx20(3), dx21(3)
     real(kind=WP)                           :: f0(3), df10(3), df21(3)
     real(kind=WP)                           :: t0(3), dt10(3), dt21(3)
     real(kind=WP)                           :: s0(3), ds10(3), ds21(3)
-    real(kind=WP)                           :: rho_at_Zn(3, mesh%nl), temp_at_Zn(3), salt_at_Zn(3), drho_dz(3), aux_dref
+    real(kind=WP)                           :: rho_at_Zn(3), temp_at_Zn(3), salt_at_Zn(3), drho_dz(3), aux_dref
     real(kind=WP)                           :: rhopot(3), bulk_0(3), bulk_pz(3), bulk_pz2(3)
     real(kind=WP)                           :: dref_rhopot, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2
     real(kind=WP)                           :: zbar_n(mesh%nl), z_n(mesh%nl-1)
@@ -2381,50 +2387,49 @@ subroutine pressure_force_4_zxxxx_easypgf(tracers, partit, mesh)
 !$OMP                                  f0, df10, df21, t0, dt10, dt21, s0, ds10, ds21, rho_at_Zn, temp_at_Zn, salt_at_Zn, drho_dz, aux_dref, rhopot,                &
 !$OMP                                  bulk_0, bulk_pz, bulk_pz2, dref_rhopot, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, zbar_n, z_n                                )
 !$OMP DO
-    do elem = 1, myDim_elem2D
+    do elem=1, myDim_elem2D
         !_______________________________________________________________________
         ! nle...number of mid-depth levels at elem
-        nle = nlevels(elem) - 1
-        ule = ulevels(elem)
-
-        ! node indices of elem
-        elnodes = elem2D_nodes(:, elem)
-
+        nle          = nlevels(elem)-1
+        ule          = ulevels(elem)
+        
+        ! node indices of elem 
+        elnodes      = elem2D_nodes(:,elem) 
+        
         !_______________________________________________________________________
         ! calculate mid depth element level --> Z_e
         ! nle...number of mid-depth levels at elem
-        zbar_n          = 0.0_WP
-        Z_n             = 0.0_WP
-        zbar_n(nle + 1) = zbar_e_bot(elem)
-        Z_n(nle)        = zbar_n(nle + 1) + helem(nle, elem) * 0.5_WP
-
-        do nlz = nle, ule + 1, -1
-            zbar_n(nlz)  = zbar_n(nlz + 1) + helem(nlz    , elem)
-            Z_n(nlz - 1) = zbar_n(nlz    ) + helem(nlz - 1, elem) * 0.5_WP
+        zbar_n       = 0.0_WP
+        Z_n          = 0.0_WP
+        zbar_n(nle+1)= zbar_e_bot(elem)
+        Z_n(nle)     = zbar_n(nle+1) + helem(nle,elem)*0.5_WP
+        do nlz=nle,ule+1,-1
+            zbar_n(nlz) = zbar_n(nlz+1) + helem(nlz,elem)
+            Z_n(nlz-1)  = zbar_n(nlz)   + helem(nlz-1,elem)*0.5_WP
         end do
-        zbar_n(ule) = zbar_n(ule + 1) + helem(ule, elem)
-
+        zbar_n(ule)  = zbar_n(ule+1) + helem(ule,elem)
+        
         !_______________________________________________________________________
-        aux_dref = density_0
+        aux_dref     = density_0
         if (use_cavity .and. .not. use_density_ref) then
             select case(state_equation)
                 case(0)
-                    call density_linear(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot)
+                    call density_linear(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot, partit, mesh)
                 case(1)
-                    call densityJM_components(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot)
+                    call densityJM_components(density_ref_T, density_ref_S, dref_bulk_0, dref_bulk_pz, dref_bulk_pz2, dref_rhopot, partit, mesh)
                 case default !unknown
                     if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                     call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
             end select
-        end if
-
+        end if     
+        
         !_______________________________________________________________________
-        ! vertical gradient --> with average density and average
+        ! vertical gradient --> with average density and average 
         ! mid-depth level on element
         !       x0        x1               x2
         !  -----o---------o----------------o-------
         !       f0        f1               f2
-        ! --> use Newtonsche interpolation polynom of second order to calculate
+        ! --> use Newtonsche interpolation polynom of second order to calculate 
         ! central difference quotient for not equidestant distributed values
         ! f(x) = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1)
         !  | |
@@ -2432,125 +2437,362 @@ subroutine pressure_force_4_zxxxx_easypgf(tracers, partit, mesh)
         !  |
         !  +--> determine Coefficients a0, a1, a2 for f(x)=f1,f2,f3
         !  |
-        !  +--> a0 = f0 ,
-        !       a1 = (f1-f0)/(x1-x0) ,
+        !  +--> a0 = f0 , 
+        !       a1 = (f1-f0)/(x1-x0) , 
         !       a2 = ((x1-x0)*(f2-f1)-(x2-x1)*(f1-f0))/((x2-x0)*(x2-x1)*(x1-x0))
         !
-        ! f(x) = f0
-        !        + (f1-f0)/(x1-x0)*(x-x0)
+        ! f(x) = f0 
+        !        + (f1-f0)/(x1-x0)*(x-x0) 
         !        + [(x1-x0)*(f2-f1)-(x2-x1)*(f1-f0)]/[(x2-x0)*(x2-x1)*(x1-x0)]
         !          *[(x-x1)+(x-x0)]
         !
-
         !_______________________________________________________________________
-        ! calculate pressure gradient for all vertical layers
-        do nlz = ule, nle
-            !___________________________________________________________________
-            ! account for reference density when using cavities
-            if (use_cavity .and. .not. use_density_ref) then
-                aux_dref = dref_bulk_0 + Z_n(nlz) * dref_bulk_pz + Z_n(nlz) * dref_bulk_pz2
-                aux_dref = aux_dref * dref_rhopot / (aux_dref + 0.1_WP * Z_n(nlz))
-            end if
-
-            idx = (/1, 1, 1/) * nlz
-            if ( nlz == ule ) then
-                idx = idx - ulevels_nod2D(elnodes)
-            else
-                idx = nlevels_nod2D(elnodes) - 1 - idx
-            end if
-
-            do ni = 1, 3
-                if ( idx(ni) < 0 ) then
-                    ! would do second order Newtonian boundary extrapolation
-                    ! --> this is not wanted !!!
-                    write(*,*) ' --> would do second order bottom boundary density extrapolation'
-                    write(*,*) '     This is not wanted, model stops here'
-                    write(*,*) ' idx = ', idx
-                    write(*,*) ' nlz = ', nlz
-                    write(*,*) ' nle = ', nle
-                    write(*,*) ' ule = ', ule
-                    write(*,*) ' nln = ', nlevels_nod2D(elnodes)-1
-                    call par_ex(partit%MPI_COMM_FESOM, partit%mype, 0)
-                end if
-
-                layer_offset = 0
-                if ( nlz == ule .and. idx(ni) == 0 ) then
-                    ! elemental surface index nlz is also surface index of vertice ni
-                    ! --> do second order newtonian bottom boundary interpolation
-                    layer_offset = 1
-                else if ( nlz == nle .and. idx(ni) == 0 ) then
-                    ! elemental bottom index nlz is also bottom index of vertice ni
-                    ! --> do second order newtonian surface boundary interpolation
-                    layer_offset = -1
-                end if
+        ! calculate pressure gradient for surface layer 
+        
+        ! account for reference density when using cavities
+        if (use_cavity .and. .not. use_density_ref) then
+            aux_dref = dref_bulk_0   + Z_n(nlz)*dref_bulk_pz   + Z_n(nlz)*dref_bulk_pz2
+            aux_dref = aux_dref*dref_rhopot/(aux_dref+0.1_WP*Z_n(nlz))
+        end if 
+        
+        nlz = ule
+        idx = (/1, 1, 1/)*nlz
+        idx = idx - ulevels_nod2D(elnodes)
+        do ni=1,3
+            if (idx(ni)==0) then
+                ! elemental surface index nlz ist also surface index of vertice ni
+                ! --> do second order newtonian surface boundary interpolation
                 !___________________________________________________________________
                 ! compute interpolation coefficients
-                dx10(ni)   = Z_3d_n(nlz     + layer_offset, elnodes(ni))-Z_3d_n(nlz - 1 + layer_offset,elnodes(ni))
-                dx21(ni)   = Z_3d_n(nlz + 1 + layer_offset, elnodes(ni))-Z_3d_n(nlz     + layer_offset,elnodes(ni))
-                dx20(ni)   = Z_3d_n(nlz + 1 + layer_offset, elnodes(ni))-Z_3d_n(nlz - 1 + layer_offset,elnodes(ni))
-
-                !!PS f0     = density_m_rho0(nlz-1,elnodes)
-                !!PS df10   = density_m_rho0(nlz  ,elnodes)-density_m_rho0(nlz-1,elnodes)
-                !!PS df21   = density_m_rho0(nlz+1,elnodes)-density_m_rho0(nlz  ,elnodes)
-
-                t0(ni)     = temp(nlz - 1 + layer_offset,elnodes(ni))
-                dt10(ni)   = temp(nlz     + layer_offset,elnodes(ni))-temp(nlz - 1 + layer_offset,elnodes(ni))
-                dt21(ni)   = temp(nlz + 1 + layer_offset,elnodes(ni))-temp(nlz     + layer_offset,elnodes(ni))
-
-                s0(ni)     = salt(nlz - 1 + layer_offset,elnodes(ni))
-                ds10(ni)   = salt(nlz     + layer_offset,elnodes(ni))-salt(nlz - 1 + layer_offset,elnodes(ni))
-                ds21(ni)   = salt(nlz + 1 + layer_offset,elnodes(ni))-salt(nlz     + layer_offset,elnodes(ni))
-
+                dx10(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
+                dx21(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz+1,elnodes(ni))
+                dx20(ni)   = Z_3d_n(nlz+2,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
+                
+                !!PS f0(ni)     = density_m_rho0(nlz  ,elnodes(ni))
+                !!PS df10(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
+                !!PS df21(ni)   = density_m_rho0(nlz+2,elnodes(ni))-density_m_rho0(nlz+1,elnodes(ni))
+                
+                t0(ni)     = temp(nlz  ,elnodes(ni))
+                dt10(ni)   = temp(nlz+1,elnodes(ni))-temp(nlz  ,elnodes(ni))
+                dt21(ni)   = temp(nlz+2,elnodes(ni))-temp(nlz+1,elnodes(ni))
+                
+                s0(ni)     = salt(nlz  ,elnodes(ni))
+                ds10(ni)   = salt(nlz+1,elnodes(ni))-salt(nlz  ,elnodes(ni))
+                ds21(ni)   = salt(nlz+2,elnodes(ni))-salt(nlz+1,elnodes(ni))
                 !___________________________________________________________________
                 ! interpoalte vertice temp and salinity to elemental level Z_n
-                temp_at_Zn(ni) = t0(ni) + dt10(ni) / dx10(ni)                               &
-                                 * (Z_n(nlz) - Z_3d_n(nlz - 1 + layer_offset, elnodes(ni))) &
-                                 + (dx10(ni) * dt21(ni) - dx21(ni) * dt10(ni))              &
-                                 / (dx20(ni) * dx21(ni) * dx10(ni))                         &
-                                 * (Z_n(nlz) - Z_3d_n(nlz     + layer_offset, elnodes(ni))) &
-                                 * (Z_n(nlz) - Z_3d_n(nlz - 1 + layer_offset, elnodes(ni)))
-
-                salt_at_Zn(ni) = s0(ni) + ds10(ni) / dx10(ni)                               &
-                                 * (Z_n(nlz) - Z_3d_n(nlz - 1 + layer_offset, elnodes(ni))) &
-                                 + (dx10(ni) * ds21(ni) - dx21(ni) * ds10(ni))              &
-                                 / (dx20(ni) * dx21(ni) * dx10(ni))                         &
-                                 * (Z_n(nlz) - Z_3d_n(nlz     + layer_offset, elnodes(ni))) &
-                                 * (Z_n(nlz) - Z_3d_n(nlz - 1 + layer_offset, elnodes(ni)))
-
+                temp_at_Zn(ni) = t0(ni) &
+                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) & 
+                                + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz+1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))
+                salt_at_Zn(ni) = s0(ni) &
+                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) & 
+                                + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz+1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))
                 !___________________________________________________________________
-                ! compute density from state equation
+                ! compute density from state equation 
                 select case(state_equation)
                     case(0)
-                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
                     case(1)
-                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni))
+                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
                     case default !unknown
                         if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
                         call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
                 end select
-                rho_at_Zn(ni, nlz) = bulk_0(ni) + Z_n(nlz) * (bulk_pz(ni) + Z_n(nlz) * bulk_pz2(ni))
-                rho_at_Zn(ni, nlz) = rho_at_Zn(ni, nlz) * rhopot(ni)                                   &
-                                     / (rho_at_Zn(ni, nlz) + 0.1_WP * Z_n(nlz) * real(state_equation)) &
-                                     - aux_dref
-            end do
+                rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
+                rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
+                
+                !!PS rho_at_Zn(ni) = f0(ni) &
+                !!PS                 + df10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni))) & 
+                !!PS                 + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                !!PS                   (Z_n(nlz)-Z_3d_n(nlz+1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))
+                
+            elseif (idx(ni)>0) then
+                ! elemental surface index nlz ist deeper than surface index of vertices ni
+                ! --> do normal interpolation
+                !___________________________________________________________________
+                ! compute interpolation coefficients
+                dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
+                dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
+                dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
+                
+                !!PS f0(ni)     = density_m_rho0(nlz-1,elnodes(ni))
+                !!PS df10(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
+                !!PS df21(ni)   = density_m_rho0(nlz+1,elnodes(ni))-density_m_rho0(nlz  ,elnodes(ni))
+                
+                t0(ni)     = temp(nlz-1,elnodes(ni))
+                dt10(ni)   = temp(nlz  ,elnodes(ni))-temp(nlz-1,elnodes(ni))
+                dt21(ni)   = temp(nlz+1,elnodes(ni))-temp(nlz  ,elnodes(ni))
+                
+                s0(ni)     = salt(nlz-1,elnodes(ni))
+                ds10(ni)   = salt(nlz  ,elnodes(ni))-salt(nlz-1,elnodes(ni))
+                ds21(ni)   = salt(nlz+1,elnodes(ni))-salt(nlz  ,elnodes(ni))
+                !___________________________________________________________________               
+                ! interpoalte vertice temp and salinity to elemental level Z_n
+                temp_at_Zn(ni) = t0(ni) &
+                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
+                                + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
+                salt_at_Zn(ni) = s0(ni) &
+                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
+                                + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
+                !___________________________________________________________________
+                ! compute density from state equation 
+                select case(state_equation)
+                    case(0)
+                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
+                    case(1)
+                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
+                    case default !unknown
+                        if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
+                        call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
+                end select
+                rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
+                rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
+                
+                !!PS rho_at_Zn(ni) = f0(ni) &
+                !!PS                 + df10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
+                !!PS                 + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                !!PS                   (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
+            else
+                ! would do second order Newtonian surface boundary extrapolation 
+                ! --> this is not wanted !!!
+                write(*,*) ' --> would do second order surface boundary density extrapolation'
+                write(*,*) '     This is not wanted, model stops here'
+                call par_ex(partit%MPI_COMM_FESOM, partit%mype, 0)    
+            end if 
         end do
-
-        int_dp_dx = 0.0_WP
-        do nlz = ule, nle
-            !_______________________________________________________________________
+        !_______________________________________________________________________
+        ! zonal surface pressure gradients
+        drho_dx         = sum(gradient_sca(1:3,elem)*rho_at_Zn)
+        aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
+        pgf_x(nlz,elem) = aux_sum*0.5_WP
+        int_dp_dx(1)    = aux_sum
+        !_______________________________________________________________________
+        ! meridional surface pressure gradients
+        drho_dy         = sum(gradient_sca(4:6,elem)*rho_at_Zn)
+        aux_sum         = drho_dy*helem(nlz,elem)*g/density_0
+        pgf_y(nlz,elem) = aux_sum*0.5_WP
+        int_dp_dx(2)    = aux_sum
+        
+        !_______________________________________________________________________
+        ! calculate pressure gradient for subsurface layer until one layer above 
+        ! the bottom 
+        do nlz=ule+1,nle-1
+        
+            !___________________________________________________________________
+            ! elemental bottom index nlz ist shallower than bottom index of vertice ni
+            if (use_cavity .and. .not. use_density_ref) then
+                aux_dref = dref_bulk_0   + Z_n(nlz)*dref_bulk_pz   + Z_n(nlz)*dref_bulk_pz2
+                aux_dref = aux_dref*dref_rhopot/(aux_dref+0.1_WP*Z_n(nlz))
+            end if 
+            
+            !___________________________________________________________________
+            ! compute interpolation coefficients
+            dx10   = Z_3d_n(nlz  ,elnodes)-Z_3d_n(nlz-1,elnodes)
+            dx21   = Z_3d_n(nlz+1,elnodes)-Z_3d_n(nlz  ,elnodes)
+            dx20   = Z_3d_n(nlz+1,elnodes)-Z_3d_n(nlz-1,elnodes)
+            
+            !!PS f0     = density_m_rho0(nlz-1,elnodes)
+            !!PS df10   = density_m_rho0(nlz  ,elnodes)-density_m_rho0(nlz-1,elnodes)
+            !!PS df21   = density_m_rho0(nlz+1,elnodes)-density_m_rho0(nlz  ,elnodes)
+            
+            t0     = temp(nlz-1,elnodes)
+            dt10   = temp(nlz  ,elnodes)-temp(nlz-1,elnodes)
+            dt21   = temp(nlz+1,elnodes)-temp(nlz  ,elnodes)
+            
+            s0     = salt(nlz-1,elnodes)
+            ds10   = salt(nlz  ,elnodes)-salt(nlz-1,elnodes)
+            ds21   = salt(nlz+1,elnodes)-salt(nlz  ,elnodes)
+            !___________________________________________________________________               
+            ! interpoalte vertice temp and salinity to elemental level Z_n
+            temp_at_Zn = t0 &
+                            + dt10/dx10*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes)) & 
+                            + (dx10*dt21-dx21*dt10)/(dx20*dx21*dx10)* &
+                                ((/1.0_WP,1.0_WP,1.0_WP/)*Z_n(nlz)-Z_3d_n(nlz,elnodes))*&
+                                ((/1.0_WP,1.0_WP,1.0_WP/)*Z_n(nlz)-Z_3d_n(nlz-1,elnodes))
+            salt_at_Zn = s0 &
+                            + ds10/dx10*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes)) & 
+                            + (dx10*ds21-dx21*ds10)/(dx20*dx21*dx10)* &
+                                ((/1.0_WP,1.0_WP,1.0_WP/)*Z_n(nlz)-Z_3d_n(nlz,elnodes))*&
+                                ((/1.0_WP,1.0_WP,1.0_WP/)*Z_n(nlz)-Z_3d_n(nlz-1,elnodes))
+            !___________________________________________________________________
+            ! compute density from state equation 
+            select case(state_equation)
+                case(0)
+                    call density_linear(temp_at_Zn(1), salt_at_Zn(1), bulk_0(1), bulk_pz(1), bulk_pz2(1), rhopot(1), partit, mesh)             
+                    call density_linear(temp_at_Zn(2), salt_at_Zn(2), bulk_0(2), bulk_pz(2), bulk_pz2(2), rhopot(2), partit, mesh)             
+                    call density_linear(temp_at_Zn(3), salt_at_Zn(3), bulk_0(3), bulk_pz(3), bulk_pz2(3), rhopot(3), partit, mesh)             
+                case(1)
+                    call densityJM_components(temp_at_Zn(1), salt_at_Zn(1), bulk_0(1), bulk_pz(1), bulk_pz2(1), rhopot(1), partit, mesh)             
+                    call densityJM_components(temp_at_Zn(2), salt_at_Zn(2), bulk_0(2), bulk_pz(2), bulk_pz2(2), rhopot(2), partit, mesh)             
+                    call densityJM_components(temp_at_Zn(3), salt_at_Zn(3), bulk_0(3), bulk_pz(3), bulk_pz2(3), rhopot(3), partit, mesh)             
+                case default !unknown
+                    if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
+                    call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
+            end select
+            rho_at_Zn = bulk_0 + Z_n(nlz)*(bulk_pz + Z_n(nlz)*bulk_pz2)
+            rho_at_Zn = rho_at_Zn*rhopot/(rho_at_Zn+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
+            
+            !!PS rho_at_Zn = f0 &
+            !!PS                 + df10/dx10*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes)) & 
+            !!PS                 + (dx10*df21-dx21*df10)/(dx20*dx21*dx10)* &
+            !!PS                     (Z_n(nlz)-Z_3d_n(nlz,elnodes))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes))
+            
+            !___________________________________________________________________
             ! zonal gradients
-            drho_dx         = sum(gradient_sca(1:3,elem) * rho_at_Zn(:, nlz))
-            aux_sum         = drho_dx * helem(nlz,elem) * g/density_0
-            pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum * 0.5_WP
+            drho_dx         = sum(gradient_sca(1:3,elem)*rho_at_Zn)
+            aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
+            pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
             int_dp_dx(1)    = int_dp_dx(1) + aux_sum
-
-            !_______________________________________________________________________
+            
+            !___________________________________________________________________
             ! meridional gradients
-            drho_dy         = sum(gradient_sca(4:6,elem) * rho_at_Zn(:, nlz))
-            aux_sum         = drho_dy * helem(nlz,elem) * g / density_0
-            pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum * 0.5_WP
+            drho_dy         = sum(gradient_sca(4:6,elem)*rho_at_Zn)
+            aux_sum         = drho_dy*helem(nlz,elem)*g/density_0
+            pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
             int_dp_dx(2)    = int_dp_dx(2) + aux_sum
+        end do ! --> do nlz=2,nle-1
+        
+        !_______________________________________________________________________
+        ! calculate pressure gradient for bottom layer
+        ! vertical gradient --> with average density and average mid-depth level 
+        ! on element
+        ! f(x) = a0 + a1(x-x0) + a2*(x-x1)(x-x0)
+        ! f(x) = f0 + df10/dx10*(x-x0) + (dx10*df21-dx21*df10)/(dx20*dx21*dx10)*(x-x1)(x-x0)
+        
+        ! account for reference density when using cavities
+        if (use_cavity .and. .not. use_density_ref) then
+            aux_dref = dref_bulk_0 + Z_n(nlz)*dref_bulk_pz + Z_n(nlz)*dref_bulk_pz2
+            aux_dref = aux_dref*dref_rhopot/(aux_dref+0.1_WP*Z_n(nlz))
+        end if 
+        
+        nlz = nle 
+        idx = (/1, 1, 1/)*nlz
+        idx = nlevels_nod2D(elnodes)-1 - idx
+        do ni=1,3
+            if (idx(ni)==0) then
+                ! elemental bottom index nlz ist also bottom index of vertice ni
+                ! --> do second order newtonian bottom boundary interpolation 
+                !___________________________________________________________________
+                ! compute interpolation coefficients
+                dx10(ni)   = Z_3d_n(nlz-1,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
+                dx21(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
+                dx20(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-2,elnodes(ni))
+                
+                !!PS f0(ni)     = density_m_rho0(nlz-2,elnodes(ni))
+                !!PS df10(ni)   = density_m_rho0(nlz-1,elnodes(ni))-density_m_rho0(nlz-2,elnodes(ni))
+                !!PS df21(ni)   = density_m_rho0(nlz  ,elnodes(ni))-density_m_rho0(nlz-1,elnodes(ni))
+                
+                t0(ni)     = temp(nlz-2,elnodes(ni))
+                dt10(ni)   = temp(nlz-1,elnodes(ni))-temp(nlz-2,elnodes(ni))
+                dt21(ni)   = temp(nlz  ,elnodes(ni))-temp(nlz-1,elnodes(ni))
+                
+                s0(ni)     = salt(nlz-2,elnodes(ni))
+                ds10(ni)   = salt(nlz-1,elnodes(ni))-salt(nlz-2,elnodes(ni))
+                ds21(ni)   = salt(nlz  ,elnodes(ni))-salt(nlz-1,elnodes(ni))
+                !___________________________________________________________________
+                ! interpoalte vertice temp and salinity to elemental level Z_n
+                temp_at_Zn(ni) = t0(ni) &
+                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) & 
+                                + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni)))
+                salt_at_Zn(ni) = s0(ni) &
+                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) & 
+                                + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni)))
+                !___________________________________________________________________
+                ! compute density from state equation 
+                select case(state_equation)
+                    case(0)
+                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
+                    case(1)
+                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
+                    case default !unknown
+                        if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
+                        call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
+                end select
+                rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
+                rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
+                
+                !!PS rho_at_Zn(ni) = f0(ni) &
+                !!PS                 + df10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni))) & 
+                !!PS                 + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                !!PS                  (Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-2,elnodes(ni)))
+                
+            elseif (idx(ni)>0) then
+                ! elemental bottom index nlz ist shallower than bottom index of vertice ni
+                ! --> do normal interpolation
+                !___________________________________________________________________
+                ! compute interpolation coefficients
+                dx10(ni)   = Z_3d_n(nlz  ,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
+                dx21(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz  ,elnodes(ni))
+                dx20(ni)   = Z_3d_n(nlz+1,elnodes(ni))-Z_3d_n(nlz-1,elnodes(ni))
+                
+                !!PS f0     = density_m_rho0(nlz-1,elnodes)
+                !!PS df10   = density_m_rho0(nlz  ,elnodes)-density_m_rho0(nlz-1,elnodes)
+                !!PS df21   = density_m_rho0(nlz+1,elnodes)-density_m_rho0(nlz  ,elnodes)
+            
+                t0(ni)     = temp(nlz-1,elnodes(ni))
+                dt10(ni)   = temp(nlz  ,elnodes(ni))-temp(nlz-1,elnodes(ni))
+                dt21(ni)   = temp(nlz+1,elnodes(ni))-temp(nlz  ,elnodes(ni))
+                
+                s0(ni)     = salt(nlz-1,elnodes(ni))
+                ds10(ni)   = salt(nlz  ,elnodes(ni))-salt(nlz-1,elnodes(ni))
+                ds21(ni)   = salt(nlz+1,elnodes(ni))-salt(nlz  ,elnodes(ni))
+                !___________________________________________________________________               
+                ! interpoalte vertice temp and salinity to elemental level Z_n
+                temp_at_Zn(ni) = t0(ni) &
+                                + dt10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
+                                + (dx10(ni)*dt21(ni)-dx21(ni)*dt10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
+                salt_at_Zn(ni) = s0(ni) &
+                                + ds10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
+                                + (dx10(ni)*ds21(ni)-dx21(ni)*ds10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                                  (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
+                !___________________________________________________________________
+                ! compute density from state equation 
+                select case(state_equation)
+                    case(0)
+                        call density_linear(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
+                    case(1)
+                        call densityJM_components(temp_at_Zn(ni), salt_at_Zn(ni), bulk_0(ni), bulk_pz(ni), bulk_pz2(ni), rhopot(ni), partit, mesh)             
+                    case default !unknown
+                        if (mype==0) write(*,*) 'Wrong type of the equation of state. Check your namelists.'
+                        call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
+                end select
+                rho_at_Zn(ni) = bulk_0(ni) + Z_n(nlz)*(bulk_pz(ni) + Z_n(nlz)*bulk_pz2(ni))
+                rho_at_Zn(ni) = rho_at_Zn(ni)*rhopot(ni)/(rho_at_Zn(ni)+0.1_WP*Z_n(nlz)*real(state_equation))-aux_dref
+                
+                !!PS rho_at_Zn(ni) = f0(ni) &
+                !!PS                 + df10(ni)/dx10(ni)*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni))) & 
+                !!PS                 + (dx10(ni)*df21(ni)-dx21(ni)*df10(ni))/(dx20(ni)*dx21(ni)*dx10(ni))* &
+                !!PS                   (Z_n(nlz)-Z_3d_n(nlz,elnodes(ni)))*(Z_n(nlz)-Z_3d_n(nlz-1,elnodes(ni)))
+            else
+                ! would do second order Newtonian bottom boundary extrapolation 
+                ! --> this is not wanted !!!
+                write(*,*) ' --> would do second order bottom boundary density extrapolation'
+                write(*,*) '     This is not wanted, model stops here'
+                write(*,*) ' idx = ', idx
+                write(*,*) ' nle = ', nle
+                write(*,*) ' nln = ', nlevels_nod2D(elnodes)-1
+                call par_ex(partit%MPI_COMM_FESOM, partit%mype, 0)
+            end if 
         end do
+        !_______________________________________________________________________
+        ! zonal bottom pressure gradients
+        drho_dx         = sum(gradient_sca(1:3,elem)*rho_at_Zn)
+        aux_sum         = drho_dx*helem(nlz,elem)*g/density_0
+        pgf_x(nlz,elem) = int_dp_dx(1) + aux_sum*0.5_WP
+        
+        !_______________________________________________________________________
+        ! meridional bottom pressure gradients
+        drho_dy         = sum(gradient_sca(4:6,elem)*rho_at_Zn)
+        aux_sum         = drho_dy*helem(nlz,elem)*g/density_0
+        pgf_y(nlz,elem) = int_dp_dx(2) + aux_sum*0.5_WP
+       
     end do ! --> do elem=1, myDim_elem2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -2579,7 +2821,7 @@ IMPLICIT NONE
   type(t_mesh),   intent(in) ,   target :: mesh
   type(t_partit), intent(inout), target :: partit
   real(kind=WP),  intent(IN)            :: t,s,pz
-  real(kind=WP),  intent(OUT)           :: rho_out
+  real(kind=WP),  intent(OUT)           :: rho_out                 
   real(kind=WP)                         :: rhopot, bulk
   real(kind=WP)                         :: bulk_0, bulk_pz, bulk_pz2
 #include "associate_part_def.h"
@@ -2588,9 +2830,9 @@ IMPLICIT NONE
 #include "associate_mesh_ass.h"
   !compute secant bulk modulus
 
-  call densityJM_components(t, s, bulk_0, bulk_pz, bulk_pz2, rhopot)
+  call densityJM_components(t, s, bulk_0, bulk_pz, bulk_pz2, rhopot, partit, mesh)
 
-  bulk = bulk_0 + pz*(bulk_pz + pz*bulk_pz2)
+  bulk = bulk_0 + pz*(bulk_pz + pz*bulk_pz2) 
 
   rho_out = bulk*rhopot / (bulk + 0.1_WP*pz) - density_0
 end subroutine densityJM_local
@@ -2598,7 +2840,9 @@ end subroutine densityJM_local
 !
 !
 !===============================================================================
-SUBROUTINE densityJM_components(t, s, bulk_0, bulk_pz, bulk_pz2, rhopot)
+SUBROUTINE densityJM_components(t, s, bulk_0, bulk_pz, bulk_pz2, rhopot, partit, mesh)
+USE MOD_MESH
+USE MOD_PARTIT
 USE MOD_PARSUP !, only: par_ex,pe_status
 USE o_ARRAYS
 USE o_PARAM
@@ -2615,6 +2859,8 @@ IMPLICIT NONE
   !---------------------------------------------------------------------------
   ! N. Rakowski 2014 the split form
   !---------------------------------------------------------------------------
+  type(t_mesh),   intent(in) ,   target :: mesh
+  type(t_partit), intent(inout), target :: partit
   real(kind=WP),  intent(IN)            :: t,s
   real(kind=WP),  intent(OUT)           :: bulk_0, bulk_pz, bulk_pz2, rhopot
   real(kind=WP)                         :: s_sqrt
@@ -2623,34 +2869,39 @@ IMPLICIT NONE
   real(kind=WP), parameter   :: at2   = -3.041638,    at3  = -1.852732e-3
   real(kind=WP), parameter   :: at4   = -1.361629e-5
   real(kind=WP), parameter   :: as    = 104.4077,     ast  = -6.500517
-  real(kind=WP), parameter   :: ast2  = .1553190,     ast3 = 2.326469e-4
-  real(kind=WP), parameter   :: ass   = -5.587545,    asst = 0.7390729
+  real(kind=WP), parameter   :: ast2  = .1553190,     ast3 = 2.326469e-4 
+  real(kind=WP), parameter   :: ass   = -5.587545,    asst = 0.7390729 
   real(kind=WP), parameter   :: asst2 = -1.909078e-2
   real(kind=WP), parameter   :: ap    = -4.721788e-1, apt  = -1.028859e-2
-  real(kind=WP), parameter   :: apt2  = 2.512549e-4,  apt3 = 5.939910e-7
-  real(kind=WP), parameter   :: aps   = 1.571896e-2,  apst = 2.598241e-4
+  real(kind=WP), parameter   :: apt2  = 2.512549e-4,  apt3 = 5.939910e-7 
+  real(kind=WP), parameter   :: aps   = 1.571896e-2,  apst = 2.598241e-4 
   real(kind=WP), parameter   :: apst2 = -7.267926e-6, apss = -2.042967e-3
-  real(kind=WP), parameter   :: ap2   = 1.045941e-5,  ap2t = -5.782165e-10
+  real(kind=WP), parameter   :: ap2   = 1.045941e-5,  ap2t = -5.782165e-10 
   real(kind=WP), parameter   :: ap2t2 = 1.296821e-7
-  real(kind=WP), parameter   :: ap2s  = -2.595994e-7,ap2st = -1.248266e-9
+  real(kind=WP), parameter   :: ap2s  = -2.595994e-7,ap2st = -1.248266e-9 
   real(kind=WP), parameter   :: ap2st2= -3.508914e-9
-
+  
   real(kind=WP), parameter   :: b0 = 999.842594,    bt  = 6.793952e-2
   real(kind=WP), parameter   :: bt2 = -9.095290e-3, bt3 = 1.001685e-4
   real(kind=WP), parameter   :: bt4 = -1.120083e-6, bt5 = 6.536332e-9
   real(kind=WP), parameter   :: bs = 0.824493,      bst = -4.08990e-3
-  real(kind=WP), parameter   :: bst2 = 7.64380e-5,  bst3 = -8.24670e-7
-  real(kind=WP), parameter   :: bst4 = 5.38750e-9
+  real(kind=WP), parameter   :: bst2 = 7.64380e-5,  bst3 = -8.24670e-7		
+  real(kind=WP), parameter   :: bst4 = 5.38750e-9	
   real(kind=WP), parameter   :: bss = -5.72466e-3,  bsst = 1.02270e-4
   real(kind=WP), parameter   :: bsst2 = -1.65460e-6,bss2 = 4.8314e-4
 
+#include "associate_part_def.h"
+#include "associate_mesh_def.h"
+#include "associate_part_ass.h"
+#include "associate_mesh_ass.h"
+
   !compute secant bulk modulus
 
   s_sqrt = sqrt(s)
 
   bulk_0 =  a0      + t*(at   + t*(at2  + t*(at3 + t*at4)))      &
           + s* (as  + t*(ast  + t*(ast2 + t*ast3))               &
-               + s_sqrt*(ass  + t*(asst + t*asst2)))
+               + s_sqrt*(ass  + t*(asst + t*asst2)))                
 
   bulk_pz =  ap  + t*(apt  + t*(apt2 + t*apt3))                  &
                   + s*(aps + t*(apst + t*apst2) + s_sqrt*apss)
@@ -2685,7 +2936,7 @@ function ptheta(s,t,p,pr)
   ! Coded by ??
   ! Reviewed by ??
   !--------------------------------------------------------
-
+  
   use o_param, only: WP
   implicit none
   real(kind=WP) 			:: ptheta, s, t, p, pr
@@ -2779,8 +3030,8 @@ subroutine sw_alpha_beta(TF1,SF1, partit, mesh)
   type(t_mesh),   intent(in) ,    target :: mesh
   type(t_partit), intent(inout),  target :: partit
   integer                                :: n, nz, nzmin, nzmax
-  real(kind=WP)                          :: t1, t1_2, t1_3, t1_4, p1, p1_2, p1_3, s1, s35, s35_2
-  real(kind=WP)                          :: a_over_b
+  real(kind=WP)                          :: t1, t1_2, t1_3, t1_4, p1, p1_2, p1_3, s1, s35, s35_2 
+  real(kind=WP)                          :: a_over_b    
   real(kind=WP)                          :: TF1(mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D),SF1(mesh%nl-1, partit%myDim_nod2D+partit%eDim_nod2D)
 
 #include "associate_part_def.h"
@@ -2791,16 +3042,16 @@ subroutine sw_alpha_beta(TF1,SF1, partit, mesh)
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, nz, nzmin, nzmax, t1, t1_2, t1_3, t1_4, p1, p1_2, p1_3, s1, s35, s35_2, a_over_b)
 !$OMP DO
   do n = 1,myDim_nod2d
-     nzmin = ulevels_nod2d(n)
-     nzmax = nlevels_nod2d(n)
+     nzmin = ulevels_nod2d(n)   
+     nzmax = nlevels_nod2d(n)   
      !!PS do nz=1, nlevels_nod2d(n) -1
      do nz=nzmin, nzmax-1
-
+     
      t1 = TF1(nz,n)*1.00024_WP
      s1 = SF1(nz,n)
-    !!PS      p1 = abs(Z(nz))
-     p1 = abs(Z_3d_n(nz,n))
-
+    !!PS      p1 = abs(Z(nz)) 
+     p1 = abs(Z_3d_n(nz,n)) 
+     
      t1_2 = t1*t1
      t1_3 = t1_2*t1
      t1_4 = t1_3*t1
@@ -2814,7 +3065,7 @@ subroutine sw_alpha_beta(TF1,SF1, partit, mesh)
           + 0.555579e-7_WP*t1_2 - 0.415613e-9_WP*t1_3 &
           + s35*(-0.356603e-6_WP + 0.788212e-8_WP*t1 &
           + 0.408195e-10_WP*p1 - 0.602281e-15_WP*p1_2) &
-          + s35_2*(0.515032e-8_WP) &
+          + s35_2*(0.515032e-8_WP) & 
           + p1*(-0.121555e-7_WP + 0.192867e-9_WP*t1 - 0.213127e-11_WP*t1_2) &
           + p1_2*(0.176621e-12_WP - 0.175379e-14_WP*t1) &
           + p1_3*(0.121551e-17_WP)
@@ -2893,10 +3144,10 @@ subroutine compute_sigma_xy(TF1,SF1, partit, mesh)
         sy(uln:nln)  = 0.0_WP
         DO k=1, nod_in_elem2D_num(n)
            el=nod_in_elem2D(k, n)
-           nle = nlevels(el)-1
+           nle = nlevels(el)-1 
            ule = ulevels(el)
-           !!PS DO nz=1, nlevels(el)-1
-           DO nz=ule, nle
+           !!PS DO nz=1, nlevels(el)-1 
+           DO nz=ule, nle           
               vol(nz) = vol(nz)+elem_area(el)
 
               !NR  writing the sum over elem2D_nodes explicitly helps the compiler to vectorize the nz-loop
@@ -2922,7 +3173,7 @@ subroutine compute_sigma_xy(TF1,SF1, partit, mesh)
         !!PS sigma_xy(2,1:nl1,n) = (-sw_alpha(1:nl1,n)*ty(1:nl1)+sw_beta(1:nl1,n)*sy(1:nl1))/vol(1:nl1)*density_0
         sigma_xy(1,uln:nln,n) = (-sw_alpha(uln:nln,n)*tx(uln:nln)+sw_beta(uln:nln,n)*sx(uln:nln))/vol(uln:nln)*density_0
         sigma_xy(2,uln:nln,n) = (-sw_alpha(uln:nln,n)*ty(uln:nln)+sw_beta(uln:nln,n)*sy(uln:nln))/vol(uln:nln)*density_0
-  END DO
+  END DO 
 !$OMP END DO
 !$OMP END PARALLEL
   call exchange_nod(sigma_xy, partit)
@@ -3001,7 +3252,7 @@ subroutine insitu2pot(tracers, partit, mesh)
   real(kind=WP),  external                :: ptheta
   real(kind=WP)                           :: pp, pr, tt, ss
   integer                                 :: n, nz, nzmin, nzmax
-  real(kind=WP),  dimension(:,:), pointer :: temp, salt
+  real(kind=WP),  dimension(:,:), pointer :: temp, salt 
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
@@ -3015,40 +3266,47 @@ subroutine insitu2pot(tracers, partit, mesh)
      nzmin = ulevels_nod2D(n)
      nzmax = nlevels_nod2D(n)
      !!PS do nz=1, nlevels_nod2D(n)-1
-     do nz=nzmin, nzmax-1
+     do nz=nzmin, nzmax-1    
         tt=temp(nz,n)
         ss=salt(nz,n)
-
+        
         !!PS ___________________________________________________________________
-        !!PS using here Z_3d_n at the beginning makes the model very instable after
+        !!PS using here Z_3d_n at the beginning makes the model very instable after 
         !!PS the initialisation when using partial cell, requires smaller time step
-        !!PS when using partial cell --> i would stick here with Z --> since we
+        !!PS when using partial cell --> i would stick here with Z --> since we 
         !!PS anyway do a spinup and it its only used at initialisation time
         !!PS pp=abs(Z_3d_n(nz,n))
         pp=abs(Z(nz))
         temp(nz,n)=ptheta(ss, tt, pp, pr)
      end do
   end do
-!$OMP END PARALLEL DO
+!$OMP END PARALLEL DO 
 end subroutine insitu2pot
 !
 !
 !
 !===============================================================================
-SUBROUTINE density_linear(t, s, bulk_0, bulk_pz, bulk_pz2, rho_out)
+SUBROUTINE density_linear(t, s, bulk_0, bulk_pz, bulk_pz2, rho_out, partit, mesh)
 !coded by Margarita Smolentseva, 21.05.2020
+USE MOD_MESH
+USE MOD_PARTIT
 USE MOD_PARSUP !, only: par_ex,pe_status
 USE o_ARRAYS
 USE o_PARAM
 use g_config !, only: which_toy, toy_ocean
 IMPLICIT NONE
+  type(t_mesh),   intent(in) ,    target :: mesh
+  type(t_partit), intent(inout),  target :: partit
   real(kind=WP),  intent(IN)             :: t,s
-  real(kind=WP),  intent(OUT)            :: rho_out
+  real(kind=WP),  intent(OUT)            :: rho_out                 
   real(kind=WP)                          :: rhopot, bulk
   real(kind=WP), intent(OUT)             :: bulk_0, bulk_pz, bulk_pz2
-
+#include "associate_part_def.h"
+#include "associate_mesh_def.h"
+#include "associate_part_ass.h"
+#include "associate_mesh_ass.h"
   !compute secant bulk modulus
-
+  
   bulk_0   = 1
   bulk_pz  = 0
   bulk_pz2 = 0
@@ -3058,7 +3316,7 @@ IMPLICIT NONE
   ELSE
       rho_out  = density_0 + 0.8_WP*(s - 34.0_WP) - 0.2*(t - 20.0_WP)
   END IF
-
+  
 end subroutine density_linear
 !
 !
@@ -3076,12 +3334,12 @@ subroutine init_ref_density(partit, mesh)
     use o_ARRAYS
     use densityJM_components_interface
     implicit none
-
+    
     !___________________________________________________________________________
     type(t_mesh),   intent(in) ,    target :: mesh
     type(t_partit), intent(inout),  target :: partit
     integer                                :: node, nz, nzmin, nzmax
-    real(kind=WP)                          :: rhopot, bulk_0, bulk_pz, bulk_pz2, rho
+    real(kind=WP)                          :: rhopot, bulk_0, bulk_pz, bulk_pz2, rho 
     real(kind=8)                           :: T, S, auxz
 
 #include "associate_part_def.h"
@@ -3095,12 +3353,12 @@ subroutine init_ref_density(partit, mesh)
         nzmin = 1
         nzmax = nlevels_nod2d(node)-1
         auxz=min(0.0,Z_3d_n(nzmin,node))
-
+        
         !_______________________________________________________________________
-        call densityJM_components(density_ref_T, density_ref_S, bulk_0, bulk_pz, bulk_pz2, rhopot)
+        call densityJM_components(density_ref_T, density_ref_S, bulk_0, bulk_pz, bulk_pz2, rhopot, partit, mesh)
         rho = bulk_0   + auxz*bulk_pz   + auxz*bulk_pz2
         density_ref(nzmin, node) = rho*rhopot/(rho+0.1_WP*auxz)
-
+        
         !_______________________________________________________________________
         do nz=nzmin+1,nzmax
             auxz=Z_3d_n(nz,node)
@@ -3108,6 +3366,7 @@ subroutine init_ref_density(partit, mesh)
             density_ref(nz,node) = rho*rhopot/(rho+0.1_WP*auxz)
         end do
     end do
-!$OMP END PARALLEL DO
+!$OMP END PARALLEL DO 
     if(mype==0) write(*,*) ' --> compute reference density'
 end subroutine init_ref_density
+
diff --git a/src/oce_ale_tracer.F90 b/src/oce_ale_tracer.F90
index 1a0deaf5..c3d688f8 100644
--- a/src/oce_ale_tracer.F90
+++ b/src/oce_ale_tracer.F90
@@ -1,6 +1,6 @@
 module diff_part_hor_redi_interface
     interface
-        subroutine diff_part_hor_redi(tracer, partit, mesh)
+        subroutine diff_part_hor_redi(tracer, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
@@ -14,7 +14,7 @@ end module
 
 module diff_ver_part_expl_ale_interface
     interface
-        subroutine diff_ver_part_expl_ale(tr_num, tracer, partit, mesh)
+        subroutine diff_ver_part_expl_ale(tr_num, tracer, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
@@ -29,7 +29,7 @@ end module
 
 module diff_ver_part_redi_expl_interface
     interface
-        subroutine diff_ver_part_redi_expl(tracer, partit, mesh)
+        subroutine diff_ver_part_redi_expl(tracer, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
@@ -43,36 +43,32 @@ end module
 
 module diff_ver_part_impl_ale_interface
     interface
-        subroutine diff_ver_part_impl_ale(tr_num, dynamics,  tracer, ice, partit, mesh)
+        subroutine diff_ver_part_impl_ale(tr_num, dynamics,  tracer, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
         use mod_tracer
         use MOD_DYN
-        use mod_ice
         integer       , intent(in)   , target :: tr_num
         type(t_dyn)   , intent(inout), target :: dynamics
         type(t_tracer), intent(inout), target :: tracer
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
-        type(t_ice)   , intent(in)   , target :: ice
         end subroutine
     end interface
 end module
 
 module diff_tracers_ale_interface
     interface
-        subroutine diff_tracers_ale(tr_num, dynamics, tracer, ice, partit, mesh)
+        subroutine diff_tracers_ale(tr_num, dynamics, tracer, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
         use mod_tracer
-        use mod_ice
         use MOD_DYN
         integer       , intent(in),    target :: tr_num
         type(t_dyn)   , intent(inout), target :: dynamics
         type(t_tracer), intent(inout), target :: tracer
-        type(t_ice),    intent(in),    target :: ice
         type(t_partit), intent(inout), target :: partit
         type(t_mesh)  , intent(in)   , target :: mesh
         end subroutine
@@ -81,7 +77,7 @@ end module
 
 module bc_surface_interface
     interface
-        function bc_surface(n, id, sval, nzmin, partit)
+        function bc_surface(n, id, sval, nzmin, partit) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
@@ -93,23 +89,9 @@ module bc_surface_interface
     end interface
 end module
 
-module transit_bc_surface_interface
-    interface
-        function transit_bc_surface(n, id, sst, sss, a_ice, sval, nzmin, partit)
-        use mod_mesh
-        USE MOD_PARTIT
-        USE MOD_PARSUP
-        integer , intent(in)                  :: n, id, nzmin
-        type(t_partit), intent(inout), target :: partit
-        real(kind=WP)                         :: transit_bc_surface
-        real(kind=WP), intent(in)             :: sst, sss, a_ice, sval
-        end function
-    end interface
-end module
-
 module diff_part_bh_interface
     interface
-        subroutine diff_part_bh(tr_num, dynamics, tracer, partit, mesh)
+        subroutine diff_part_bh(tr_num, dynamics, tracer, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
@@ -126,11 +108,11 @@ end module
 
 module solve_tracers_ale_interface
     interface
-        subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
+        subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh) 
         use mod_mesh
         USE MOD_PARTIT
         USE MOD_PARSUP
-        use mod_tracer
+        use mod_tracer     
         use MOD_DYN
         USE MOD_ICE
         type(t_ice)   , intent(in),    target :: ice
@@ -145,7 +127,7 @@ end module
 !
 !===============================================================================
 ! Driving routine    Here with ALE changes!!!
-subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
+subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh) 
     use g_config
     use o_PARAM, only: SPP, Fer_GM
     use mod_mesh
@@ -160,7 +142,6 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
     use diff_tracers_ale_interface
     use oce_adv_tra_driver_interfaces    
     use g_forcing_param, only: use_age_tracer !---age-code
-    use mod_transit, only: decay14, decay39
     implicit none
     type(t_ice)   , intent(in)   , target    :: ice
     type(t_dyn)   , intent(inout), target    :: dynamics
@@ -168,7 +149,7 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
     type(t_partit), intent(inout), target    :: partit
     type(t_mesh)  , intent(in)   , target    :: mesh
     !___________________________________________________________________________
-    integer                                  :: i, tr_num, node, elem, nzmax, nzmin
+    integer                                  :: tr_num, node, elem, nzmax, nzmin
     !___________________________________________________________________________
     ! pointer on necessary derived types
     real(kind=WP), dimension(:,:,:), pointer :: UV, fer_UV
@@ -177,7 +158,7 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     UV     => dynamics%uv(:,:,:)
     Wvel   => dynamics%w(:,:)
     Wvel_e => dynamics%w_e(:,:)
@@ -185,13 +166,13 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
     if (Fer_GM) then
         fer_UV     => dynamics%fer_uv(:,:,:)
         fer_Wvel   => dynamics%fer_w(:,:)
-    end if
+    end if 
     del_ttf => tracers%work%del_ttf
 
     !___________________________________________________________________________
     if (SPP) call cal_rejected_salt(ice, partit, mesh)
     if (SPP) call app_rejected_salt(tracers%data(2)%values, partit, mesh)
-
+    
     !___________________________________________________________________________
     ! update 3D velocities with the bolus velocities:
     ! 1. bolus velocities are computed according to GM implementation after R. Ferrari et al., 2010
@@ -209,31 +190,20 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
         end do
 !$OMP END PARALLEL DO
     end if
-
+    
     !___________________________________________________________________________
-    ! loop over all tracers
-        !$ACC UPDATE DEVICE(dynamics%w, dynamics%w_e, dynamics%uv) !!! async(1) 
-!!!     !$ACC UPDATE DEVICE(tracers%work%fct_ttf_min, tracers%work%fct_ttf_max, tracers%work%fct_plus, tracers%work%fct_minus)
-        !$ACC UPDATE DEVICE (mesh%helem, mesh%hnode, mesh%hnode_new, mesh%zbar_3d_n, mesh%z_3d_n)
+    ! loop over all tracers 
     do tr_num=1, tracers%num_tracers
-        ! do tracer AB (Adams-Bashfort) interpolation only for advectiv part
+        ! do tracer AB (Adams-Bashfort) interpolation only for advectiv part 
         ! needed
         if (flag_debug .and. mype==0)  print *, achar(27)//'[37m'//'         --> call init_tracers_AB'//achar(27)//'[0m'
-        call init_tracers_AB(tr_num, tracers, partit, mesh)
-
+        call init_tracers_AB(tr_num, tracers, partit, mesh) 
+        
         ! advect tracers
         if (flag_debug .and. mype==0)  print *, achar(27)//'[37m'//'         --> call adv_tracers_ale'//achar(27)//'[0m'
-
-
-	!here update only those initialized in the init_tracers. (values, valuesAB, edge_up_dn_grad, ...)
-        !$ACC UPDATE  DEVICE(tracers%data(tr_num)%values, tracers%data(tr_num)%valuesAB) &
-        !$ACC  DEVICE(tracers%work%edge_up_dn_grad) !!&
         ! it will update del_ttf with contributions from horizontal and vertical advection parts (del_ttf_advhoriz and del_ttf_advvert)
-	!$ACC wait(1)
         call do_oce_adv_tra(dt, UV, Wvel, Wvel_i, Wvel_e, tr_num, dynamics, tracers, partit, mesh)
-
-
-        !$ACC UPDATE HOST(tracers%work%del_ttf, tracers%work%del_ttf_advhoriz, tracers%work%del_ttf_advvert)
+        
         !___________________________________________________________________________
         ! update array for total tracer flux del_ttf with the fluxes from horizontal
         ! and vertical advection
@@ -242,35 +212,37 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
            tracers%work%del_ttf(:, node)=tracers%work%del_ttf(:, node)+tracers%work%del_ttf_advhoriz(:, node)+tracers%work%del_ttf_advvert(:, node)
         end do
 !$OMP END PARALLEL DO
-        ! diffuse tracers
+           !___________________________________________________________________________
+           ! AB is not needed after the advection step. Initialize it with the current tracer before it is modified.
+           ! call init_tracers_AB at the beginning of this loop will compute AB for the next time step then.
+!$OMP PARALLEL DO
+        do node=1, myDim_nod2d+eDim_nod2D
+           tracers%data(tr_num)%valuesAB(:, node)=tracers%data(tr_num)%values(:, node) !DS: check that this is the right place!
+        end do
+!$OMP END PARALLEL DO
+        ! diffuse tracers 
         if (flag_debug .and. mype==0)  print *, achar(27)//'[37m'//'         --> call diff_tracers_ale'//achar(27)//'[0m'
-        call diff_tracers_ale(tr_num, dynamics, tracers, ice, partit, mesh)
-
-!       Radioactive decay of 14C and 39Ar
-        if (tracers%data(tr_num)%ID == 14) tracers%data(tr_num)%values(:,:) = tracers%data(tr_num)%values(:,:) * exp(-decay14 * dt)
-        if (tracers%data(tr_num)%ID == 39) tracers%data(tr_num)%values(:,:) = tracers%data(tr_num)%values(:,:) * exp(-decay39 * dt)
+        call diff_tracers_ale(tr_num, dynamics, tracers, partit, mesh) 
 
         ! relax to salt and temp climatology
         if (flag_debug .and. mype==0)  print *, achar(27)//'[37m'//'         --> call relax_to_clim'//achar(27)//'[0m'
 !       if ((toy_ocean) .AND. ((tr_num==1) .AND. (TRIM(which_toy)=="soufflet"))) then
         if ((toy_ocean) .AND. ((TRIM(which_toy)=="soufflet"))) then
-            call relax_zonal_temp(tracers%data(1), partit, mesh)
+            call relax_zonal_temp(tracers%data(1), partit, mesh) 
         else
-            call relax_to_clim(tr_num, tracers, partit, mesh)
+            call relax_to_clim(tr_num, tracers, partit, mesh) 
         end if
         call exchange_nod(tracers%data(tr_num)%values(:,:), partit)
 !$OMP BARRIER
     end do
-!!!        !$ACC UPDATE HOST (tracers%work%fct_ttf_min, tracers%work%fct_ttf_max, tracers%work%fct_plus, tracers%work%fct_minus) &
-!!!        !$ACC HOST  (tracers%work%edge_up_dn_grad)
-
+    
     !___________________________________________________________________________
     ! 3D restoring for "passive" tracers
     !!!$OMPTODO: add OpenMP later, not needed right now!
     do tr_num=1, ptracers_restore_total
        tracers%data(ptracers_restore(tr_num)%locid)%values(:, ptracers_restore(tr_num)%ind2)=1.0_WP
     end do
-
+    
     !___________________________________________________________________________
     ! subtract the the bolus velocities back from 3D velocities:
     if (Fer_GM) then
@@ -287,10 +259,9 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
 !$OMP END PARALLEL DO
     end if
     
-    ! TODO: do it only when it is coupled to atmosphere 
     !___________________________________________________________________________
     ! to avoid crash with high salinities when coupled to atmosphere
-    ! --> if we do only where (tr_arr(:,:,2) < 3._WP ) we also fill up the bottom
+    ! --> if we do only where (tr_arr(:,:,2) < 3._WP ) we also fill up the bottom 
     !     topogrpahy with values which are then writte into the output --> thats why
     !     do node=1,.... and tr_arr(node,1:nzmax,2)
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(node, nzmin, nzmax)
@@ -303,7 +274,7 @@ subroutine solve_tracers_ale(ice, dynamics, tracers, partit, mesh)
 
         where (tracers%data(2)%values(nzmin:nzmax,node) < 3._WP )
                tracers%data(2)%values(nzmin:nzmax,node) = 3._WP
-        end where
+        end where        
     end do
 !$OMP END PARALLEL DO
 
@@ -324,7 +295,7 @@ end subroutine solve_tracers_ale
 !
 !
 !===============================================================================
-subroutine diff_tracers_ale(tr_num, dynamics, tracers, ice, partit, mesh)
+subroutine diff_tracers_ale(tr_num, dynamics, tracers, partit, mesh) 
     use mod_mesh
     USE MOD_PARTIT
     USE MOD_PARSUP
@@ -337,12 +308,10 @@ subroutine diff_tracers_ale(tr_num, dynamics, tracers, ice, partit, mesh)
     use diff_ver_part_redi_expl_interface
     use diff_ver_part_impl_ale_interface
     use diff_part_bh_interface
-    use mod_ice
     implicit none
     integer       , intent(in)   , target :: tr_num
     type(t_dyn)   , intent(inout), target :: dynamics
     type(t_tracer), intent(inout), target :: tracers
-    type(t_ice)   , intent(in)   , target :: ice
     type(t_partit), intent(inout), target :: partit
     type(t_mesh)  , intent(in)   , target :: mesh
     !___________________________________________________________________________
@@ -353,35 +322,35 @@ subroutine diff_tracers_ale(tr_num, dynamics, tracers, ice, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     del_ttf => tracers%work%del_ttf
-
+    
     !___________________________________________________________________________
     ! do horizontal diffusiion
-    ! write there also horizontal diffusion rhs to del_ttf which is equal the R_T^n
+    ! write there also horizontal diffusion rhs to del_ttf which is equal the R_T^n 
     ! in danilovs srcipt
     ! includes Redi diffusivity if Redi=.true.
     call diff_part_hor_redi(tracers, partit, mesh)  ! seems to be ~9% faster than diff_part_hor
-
+    
     !___________________________________________________________________________
     ! do vertical diffusion: explicit
-    if (.not. tracers%data(tr_num)%i_vert_diff) call diff_ver_part_expl_ale(tr_num, tracers, partit, mesh)
+    if (.not. tracers%data(tr_num)%i_vert_diff) call diff_ver_part_expl_ale(tr_num, tracers, partit, mesh) 
     ! A projection of horizontal Redi diffussivity onto vertical. This par contains horizontal
     ! derivatives and has to be computed explicitly!
-    if (Redi) call diff_ver_part_redi_expl(tracers, partit, mesh)
+    if (Redi) call diff_ver_part_redi_expl(tracers, partit, mesh)     
 
     !___________________________________________________________________________
     ! Update tracers --> calculate T* see Danilov et al. (2017)
     ! T* =  (dt*R_T^n + h^(n-0.5)*T^(n-0.5))/h^(n+0.5)
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(n, nzmin, nzmax)
-    do n=1, myDim_nod2D
+    do n=1, myDim_nod2D 
         nzmax=nlevels_nod2D(n)-1
         nzmin=ulevels_nod2D(n)
         del_ttf(nzmin:nzmax,n)=del_ttf(nzmin:nzmax,n)+tracers%data(tr_num)%values(nzmin:nzmax,n)* &
                                     (hnode(nzmin:nzmax,n)-hnode_new(nzmin:nzmax,n))
         tracers%data(tr_num)%values(nzmin:nzmax,n)=tracers%data(tr_num)%values(nzmin:nzmax,n)+ &
                                     del_ttf(nzmin:nzmax,n)/hnode_new(nzmin:nzmax,n)
-        ! WHY NOT ??? --> whats advantage of above --> tested it --> the upper
+        ! WHY NOT ??? --> whats advantage of above --> tested it --> the upper 
         ! equation has a 30% smaller nummerical drift
         ! tr_arr(1:nzmax,n,tr_num)=(hnode(1:nzmax,n)*tr_arr(1:nzmax,n,tr_num)+ &
         !                           del_ttf(1:nzmax,n))/hnode_new(1:nzmax,n)
@@ -389,20 +358,20 @@ subroutine diff_tracers_ale(tr_num, dynamics, tracers, ice, partit, mesh)
 !$OMP END PARALLEL DO
     !___________________________________________________________________________
     if (tracers%data(tr_num)%i_vert_diff) then
-        ! do vertical diffusion: implicite
-        call diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
+        ! do vertical diffusion: implicite 
+        call diff_ver_part_impl_ale(tr_num, dynamics, tracers, partit, mesh) 
     end if
     !We DO not set del_ttf to zero because it will not be used in this timestep anymore
     !init_tracers_AB will set it to zero for the next timestep
     if (tracers%data(tr_num)%smooth_bh_tra) then
-       call diff_part_bh(tr_num, dynamics, tracers, partit, mesh)  ! alpply biharmonic diffusion (implemented as filter)
+       call diff_part_bh(tr_num, dynamics, tracers, partit, mesh)  ! alpply biharmonic diffusion (implemented as filter)                                                
     end if
 end subroutine diff_tracers_ale
 !
 !
 !===============================================================================
-!Vertical diffusive flux(explicit scheme):
-subroutine diff_ver_part_expl_ale(tr_num, tracers, partit, mesh)
+!Vertical diffusive flux(explicit scheme):                                                                            
+subroutine diff_ver_part_expl_ale(tr_num, tracers, partit, mesh) 
     use o_ARRAYS
     use g_forcing_arrays
     use MOD_MESH
@@ -410,7 +379,7 @@ subroutine diff_ver_part_expl_ale(tr_num, tracers, partit, mesh)
     USE MOD_PARSUP
     use MOD_TRACER
     use g_config,only: dt
-    implicit none
+    implicit none 
     integer       , intent(in)   , target :: tr_num
     type(t_tracer), intent(inout), target :: tracers
     type(t_partit), intent(inout), target :: partit
@@ -426,15 +395,15 @@ subroutine diff_ver_part_expl_ale(tr_num, tracers, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     del_ttf => tracers%work%del_ttf
-
+    
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(n, nz, nl1, ul1, vd_flux, rdata, flux, rlx, zinv1)
-    !___________________________________________________________________________
+    !___________________________________________________________________________    
     do n=1, myDim_nod2D
         nl1=nlevels_nod2D(n)-1
         ul1=ulevels_nod2D(n)
-
+        
         vd_flux=0._WP
         if (tracers%data(tr_num)%ID==1) then
             flux  = -heat_flux(n)/vcpw
@@ -446,21 +415,21 @@ subroutine diff_ver_part_expl_ale(tr_num, tracers, partit, mesh)
             flux  = 0._WP
             rdata = 0._WP
             rlx=0._WP
-        endif
+        endif        
         !_______________________________________________________________________
         !Surface forcing
-        vd_flux(ul1)= flux
+        vd_flux(ul1)= flux        
         do nz=ul1+1,nl1
             !___________________________________________________________________
-            zinv1=1.0_WP/(Z_3d_n(nz-1,n)-Z_3d_n(nz,n))
-            vd_flux(nz) = Kv(nz,n)*(tracers%data(tr_num)%values(nz-1,n)-tracers%data(tr_num)%values(nz,n))*zinv1*area(nz,n)
+            zinv1=1.0_WP/(Z_3d_n(nz-1,n)-Z_3d_n(nz,n))                       
+            vd_flux(nz) = Kv(nz,n)*(tracers%data(tr_num)%values(nz-1,n)-tracers%data(tr_num)%values(nz,n))*zinv1*area(nz,n)            
         end do
-
+        
         do nz=ul1,nl1-1
             del_ttf(nz,n) = del_ttf(nz,n) + (vd_flux(nz) - vd_flux(nz+1))/(zbar_3d_n(nz,n)-zbar_3d_n(nz+1,n))*dt/areasvol(nz,n)
         end do
         del_ttf(nl1,n) = del_ttf(nl1,n) + (vd_flux(nl1)/(zbar_3d_n(nl1,n)-zbar_3d_n(nl1+1,n)))*dt/areasvol(nl1,n)
-
+        
     end do ! --> do n=1, myDim_nod2D
 !$OMP END PARALLEL DO
 end subroutine diff_ver_part_expl_ale
@@ -468,7 +437,7 @@ end subroutine diff_ver_part_expl_ale
 !
 !===============================================================================
 ! vertical diffusivity augmented with Redi contribution [vertical flux of K(3,3)*d_zT]
-subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
+subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, partit, mesh) 
     use MOD_MESH
     USE MOD_PARTIT
     USE MOD_PARSUP
@@ -480,20 +449,16 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
     USE MOD_PARSUP
     use g_CONFIG
     use g_forcing_arrays
-    use o_mixing_KPP_mod !for ghats _GO_
+    use o_mixing_KPP_mod !for ghats _GO_   
     use g_cvmix_kpp, only: kpp_nonlcltranspT, kpp_nonlcltranspS, kpp_oblmixc
     use bc_surface_interface
-    use transit_bc_surface_interface
-    use mod_ice
+    use iceberg_params
     implicit none
     integer       , intent(in)   , target :: tr_num
     type(t_dyn)   , intent(inout), target :: dynamics
     type(t_tracer), intent(inout), target :: tracers
     type(t_partit), intent(inout), target :: partit
     type(t_mesh)  , intent(in)   , target :: mesh
-
-    type(t_ice)   , intent(in)   , target :: ice
-
     !___________________________________________________________________________
     real(kind=WP)            :: a(mesh%nl), b(mesh%nl), c(mesh%nl), tr(mesh%nl)
     real(kind=WP)            :: cp(mesh%nl), tp(mesh%nl)
@@ -507,31 +472,26 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
     ! pointer on necessary derived types
     real(kind=WP), dimension(:,:), pointer :: trarr
     real(kind=WP), dimension(:,:), pointer :: Wvel_i
-    real(kind=WP), dimension(:,:), pointer :: sst, sss ! auxiliary variables needed for transient tracers
-    real(kind=WP), dimension(:),   pointer :: a_ice
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     trarr  => tracers%data(tr_num)%values(:,:)
     Wvel_i => dynamics%w_i(:,:)
-
-    sst => tracers%data(1)%values(:,:)
-    sss => tracers%data(2)%values(:,:)
-    a_ice => ice%data(1)%values(:)
+    
     !___________________________________________________________________________
-    if ((trim(tracers%data(tr_num)%tra_adv_lim)=='FCT') .OR. (.not. dynamics%use_wsplit)) do_wimpl=.false.
+    if ((trim(tracers%data(tr_num)%tra_adv_lim)=='FCT') .OR. (.not. dynamics%use_wsplit)) do_wimpl=.false.    
     if (Redi) isredi=1._WP
     Ty    =0.0_WP
     Ty1   =0.0_WP
-
-    ! solve equation diffusion equation implicite part:
+    
+    ! solve equation diffusion equation implicite part: 
     ! -->   h^(n+0.5)* (T^(n+0.5)-Tstar) = dt*( K_33*d/dz*(T^(n+0.5)-Tstar) + K_33*d/dz*Tstar )
     ! -->   Tnew = T^(n+0.5)-Tstar
-    ! -->   h^(n+0.5)* (Tnew) = dt*(K_33*d/dz*Tnew) + K_33*dt*d/dz*Tstar
-    ! -->   h^(n+0.5)* (Tnew) = dt*(K_33*d/dz*Tnew) + RHS
-    ! -->   solve for T_new
-    ! -->   V_1 (Skalar Volume), A_1 (Area of edge),              .
+    ! -->   h^(n+0.5)* (Tnew) = dt*(K_33*d/dz*Tnew) + K_33*dt*d/dz*Tstar 
+    ! -->   h^(n+0.5)* (Tnew) = dt*(K_33*d/dz*Tnew) + RHS        
+    ! -->   solve for T_new                                      
+    ! -->   V_1 (Skalar Volume), A_1 (Area of edge),              .                 
     !       no Cavity A1==V1, yes Cavity A1 !=V1                 /I\ nvec_up (+1)
     !                                                             I
     !    ----------- zbar_1, A_1                             *----I----*
@@ -541,25 +501,25 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
     !    ----------- zbar_3, A_3                             |   |     |    --> normal vec outwards facing
     ! Z_3 o T_3, V3                                          *---|-----*
     !    ----------- zbar_4                                   \  | I ./
-    !        :                                                 \ | I/
-    !                                                           \|/I
+    !        :                                                 \ | I/ 
+    !                                                           \|/I   
     !                                                            * I
     !                                                             \I/
     !                                                              *  nvec_dwn (-1)
     ! --> 1st. solve homogenouse part:
     ! f(Tnew) = h^(n+0.5)* (Tnew) - dt*(K_33*dTnew/dz) = 0
     !
-    ! --> 2nd. Difference Quotient at Tnew_i in flux form (Gaus Theorem, dont forget normal vectors!!!):
-    ! V_i*Tnew_i *h_i = -dt * [ K_33 * (Tnew_i-1 - Tnew_i)/(Z_i-1 - Z_i) * A_i * nvec_up
+    ! --> 2nd. Difference Quotient at Tnew_i in flux form (Gaus Theorem, dont forget normal vectors!!!):  
+    ! V_i*Tnew_i *h_i = -dt * [ K_33 * (Tnew_i-1 - Tnew_i)/(Z_i-1 - Z_i) * A_i * nvec_up 
     !                          +K_33 * (Tnew_i - Tnew_i+1)/(Z_i - Z_i+1) * A_i+1 * nvec_dwn ]
-    !     Tnew_i *h_i = -dt * [ K_33 * (Tnew_i-1 - Tnew_i)/(Z_i-1 - Z_i) * A_i  /V_i * nvec_up
+    !     Tnew_i *h_i = -dt * [ K_33 * (Tnew_i-1 - Tnew_i)/(Z_i-1 - Z_i) * A_i  /V_i * nvec_up 
     !                          +K_33 * (Tnew_i - Tnew_i+1)/(Z_i - Z_i+1) * A_i+1/V_i * nvec_dwn ]
     !
     ! --> 3rd. solve for coefficents a, b, c:
-    ! f(Tnew) = [ a*dTnew_i-1 + b*dTnew_i + c*dTnew_i+1 ]
+    ! f(Tnew) = [ a*dTnew_i-1 + b*dTnew_i + c*dTnew_i+1 ] 
     !
     !     df(Tnew)/dTnew_i-1 = a = -dt*K_33/(Z_i-1 - Z_i) * A_i/V_i * (nvec_up =1)
-    !
+    ! 
     !     df(Tnew)/dTnew_i+1 = c =  dt * K_33 * 1/(Z_i - Z_i+1) * A_i+1/V_i * (nvec_dwn=-1)
     !                            = -dt * K_33 * 1/(Z_i - Z_i+1) * A_i+1/V_i
     !
@@ -582,7 +542,7 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
     !          - K_33*dt/(Z_i-1 - Z_i) * A_i/V_i   * Tstar_i
     !          - K_33*dt/(Z_i - Z_i+1) * A_i+1/V_i * Tstar_i
     !          + K_33*dt/(Z_i - Z_i+1) * A_i+1/V_i * Tstar_i+1
-    !
+    !         
     !         = -a*Tstar_i-1 + (a+c)*Tstar_i - c * Tstar_i+1
     !                            |-> b = h_i - (a+c), a+c = h_i-b
     !
@@ -592,22 +552,22 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
     !  | b_1 c_1 ...            |   |dTnew_1|
     !  | a_2 b_2 c_2 ...        |   |dTnew_2|
     !  |     a_3 b_3 c_3 ...    | * |dTnew_3| = RHS/V_i
-    !  |         a_4 b_4 c_4 ...|   |dTnew_4|
-    !  |              :         |   |   :   |
+    !  |         a_4 b_4 c_4 ...|   |dTnew_4| 
+    !  |              :         |   |   :   |   
     !
     ! --> a = -dt*K_33 / (Z_i-1 - Z_i) * A_i/V_i
-    !
+    ! 
     ! --> c = -dt*K_33 / (Z_i - Z_i+1) * A_i+1/V_i
     !
     ! --> b = h^(n+0.5) -[ dt*K_33/(Z_i-1 - Z_i)*A_i/V_i + dt*K_33/(Z_i - Z_i+1) * A_i+1/V_i ] = -(a+c) + h^(n+0.5)
-
+    
     !___________________________________________________________________________
     ! loop over local nodes
 
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, nz, nzmax, nzmin, a, b, c, tr, cp, tp, m, zinv, dz, &
 !$OMP                                         rsss, Ty, Ty1, c1, zinv1, zinv2, v_adv, zbar_n, z_n)
 !$OMP DO
-    do n=1,myDim_nod2D
+    do n=1,myDim_nod2D 
         ! initialise
         a  = 0.0_WP
         b  = 0.0_WP
@@ -619,9 +579,9 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
         nzmax=nlevels_nod2D(n)
         nzmin=ulevels_nod2D(n)
         !___________________________________________________________________________
-        ! Here can not exchange zbar_n & Z_n with zbar_3d_n & Z_3d_n because
+        ! Here can not exchange zbar_n & Z_n with zbar_3d_n & Z_3d_n because  
         ! they be calculate from the actualized mesh with hnode_new
-        ! calculate new zbar (depth of layers) and Z (mid depths of layers)
+        ! calculate new zbar (depth of layers) and Z (mid depths of layers) 
         ! depending on layer thinkness over depth at node n
         ! Be carefull here vertical operation have to be done on NEW vertical mesh !!!
         zbar_n=0.0_WP
@@ -634,44 +594,44 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
         end do
         zbar_n(nzmin) = zbar_n(nzmin+1) + hnode_new(nzmin,n)
         !_______________________________________________________________________
-        ! Regular part of coefficients: --> surface layer
+        ! Regular part of coefficients: --> surface layer 
         nz=nzmin
-
+        
         ! 1/dz(nz)
         zinv2=1.0_WP/(Z_n(nz)-Z_n(nz+1))
         zinv=1.0_WP*dt    ! no .../(zbar(1)-zbar(2)) because of  ALE
-
+        
         ! calculate isoneutral diffusivity : Kd*s^2 --> K_33 = Kv + Kd*s^2
         Ty1= (Z_n(nz)     -zbar_n(nz+1))*zinv2 *slope_tapered(3,nz  ,n)**2*Ki(nz  ,n) + &
              (zbar_n(nz+1)-Z_n(   nz+1))*zinv2 *slope_tapered(3,nz+1,n)**2*Ki(nz+1,n)
         Ty1=Ty1*isredi
-
+        
         ! layer dependent coefficients for for solving dT(1)/dt+d/dz*K_33*d/dz*T(1) = ...
         a(nz)=0.0_WP
         !!PS c(nz)=-(Kv(nz+1,n)+Ty1)*zinv2*zinv * (area(nz+1,n)/areasvol(nz,n))
-        c(nz)=-(Kv(nz+1,n)+Ty1)*zinv2*zinv * area(nz+1,n)/areasvol(nz,n)
+        c(nz)=-(Kv(nz+1,n)+Ty1)*zinv2*zinv * area(nz+1,n)/areasvol(nz,n) 
         b(nz)=-c(nz)+hnode_new(nz,n)      ! ale
-        ! update from the vertical advection --> comes from splitting of vert
+        ! update from the vertical advection --> comes from splitting of vert 
         ! velocity into explicite and implicite contribution
         if (do_wimpl) then
             !___________________________________________________________________
-            ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for
-            ! numerical reasons, to gurante that area/areasvol in case of no
+            ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for 
+            ! numerical reasons, to gurante that area/areasvol in case of no 
             ! cavity is ==1.0_WP
             v_adv =zinv * ( area(nz  ,n)/areasvol(nz,n) )
             b(nz) =b(nz)+Wvel_i(nz, n)*v_adv
-
+            
             !!PS v_adv =zinv * ( area(nz+1,n)/areasvol(nz,n) )
             v_adv =zinv * area(nz+1,n)/areasvol(nz,n)
             b(nz) =b(nz)-min(0._WP, Wvel_i(nz+1, n))*v_adv
             c(nz) =c(nz)-max(0._WP, Wvel_i(nz+1, n))*v_adv
-        end if
+        end if        
         ! backup zinv2 for next depth level
         zinv1=zinv2
         !_______________________________________________________________________
         ! Regular part of coefficients: --> 2nd...nl-2 layer
         do nz=nzmin+1, nzmax-2
-
+        
             ! 1/dz(nz)
             zinv2=1.0_WP/(Z_n(nz)-Z_n(nz+1))
             ! calculate isoneutral diffusivity : Kd*s^2 --> K_33 = Kv + Kd*s^2
@@ -681,72 +641,72 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
                  (zbar_n(nz+1)-Z_n(nz+1   ))*zinv2 *slope_tapered(3,nz+1,n)**2*Ki(nz+1,n)
             Ty =Ty *isredi
             Ty1=Ty1*isredi
-
+            
             ! layer dependent coefficients for for solving dT(nz)/dt+d/dz*K_33*d/dz*T(nz) = ...
             !___________________________________________________________________
-            ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for
-            ! numerical reasons, to gurante that area/areasvol in case of no
-            ! cavity is ==1.0_WP
-            a(nz)=-(Kv(nz,n)  +Ty )*zinv1*zinv * ( area(nz  ,n)/areasvol(nz,n) )
+            ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for 
+            ! numerical reasons, to gurante that area/areasvol in case of no 
+            ! cavity is ==1.0_WP   
+            a(nz)=-(Kv(nz,n)  +Ty )*zinv1*zinv * ( area(nz  ,n)/areasvol(nz,n) ) 
             !!PS c(nz)=-(Kv(nz+1,n)+Ty1)*zinv2*zinv * ( area(nz+1,n)/areasvol(nz,n) )
             c(nz)=-(Kv(nz+1,n)+Ty1)*zinv2*zinv * area(nz+1,n)/areasvol(nz,n)
             b(nz)=-a(nz)-c(nz)+hnode_new(nz,n)
-
+            
             ! backup zinv2 for next depth level
             zinv1=zinv2
             ! update from the vertical advection
             if (do_wimpl) then
                 !_______________________________________________________________
-                ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for
-                ! numerical reasons, to gurante that area/areasvol in case of no
-                ! cavity is ==1.0_WP
+                ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for 
+                ! numerical reasons, to gurante that area/areasvol in case of no 
+                ! cavity is ==1.0_WP   
                 v_adv=zinv * ( area(nz  ,n)/areasvol(nz,n) )
                 a(nz)=a(nz)+min(0._WP, Wvel_i(nz, n))*v_adv
                 b(nz)=b(nz)+max(0._WP, Wvel_i(nz, n))*v_adv
                 !!PS v_adv=v_adv*areasvol(nz+1,n)/areasvol(nz,n)
                 !!PS v_adv=zinv * ( area(nz+1,n)/areasvol(nz,n) )
-                v_adv=zinv * area(nz+1,n)/areasvol(nz,n)
+                v_adv=zinv * area(nz+1,n)/areasvol(nz,n) 
                 b(nz)=b(nz)-min(0._WP, Wvel_i(nz+1, n))*v_adv
                 c(nz)=c(nz)-max(0._WP, Wvel_i(nz+1, n))*v_adv
             end if
         end do ! --> do nz=nzmin+1, nzmax-2
-
+        
         !_______________________________________________________________________
         ! Regular part of coefficients: --> nl-1 layer
-        nz=nzmax-1
-
+        nz=nzmax-1 
+        
         zinv=1.0_WP*dt   ! no ... /(zbar(nzmax-1)-zbar(nzmax)) because of ale
-
+        
         ! calculate isoneutral diffusivity : Kd*s^2 --> K_33 = Kv + Kd*s^2
         Ty= (Z_n(nz-1) -zbar_n(nz)) * zinv1 * slope_tapered(3,nz-1,n)**2 * Ki(nz-1,n) + &
             (zbar_n(nz)-Z_n(nz)   ) * zinv1 * slope_tapered(3,nz  ,n)**2 * Ki(nz,n)
         Ty =Ty *isredi
         ! layer dependent coefficients for for solving dT(nz)/dt+d/dz*K_33*d/dz*T(nz) = ...
-
+        
         !___________________________________________________________________
-        ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for
-        ! numerical reasons, to gurante that area/areasvol in case of no
+        ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for 
+        ! numerical reasons, to gurante that area/areasvol in case of no 
         ! cavity is ==1.0_WP
         a(nz)=-(Kv(nz,n)+Ty)*zinv1*zinv* ( area(nz  ,n)/areasvol(nz,n) )
         c(nz)=0.0_WP
         b(nz)=-a(nz)+hnode_new(nz,n)
-
+        
         ! update from the vertical advection
         if (do_wimpl) then
             !___________________________________________________________________
-            ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for
-            ! numerical reasons, to gurante that area/areasvol in case of no
+            ! use brackets when computing ( area(nz  ,n)/areasvol(nz,n) ) for 
+            ! numerical reasons, to gurante that area/areasvol in case of no 
             ! cavity is ==1.0_WP
             v_adv=zinv* ( area(nz  ,n)/areasvol(nz,n) )
-            a(nz)=a(nz)+min(0._WP, Wvel_i(nz, n))*v_adv
+            a(nz)=a(nz)+min(0._WP, Wvel_i(nz, n))*v_adv       
             b(nz)=b(nz)+max(0._WP, Wvel_i(nz, n))*v_adv
         end if
-
+        
         !_______________________________________________________________________
         ! the rhs (inhomogene part): --> rhs = K_33*dt*d/dz*Tstar --> Tstar...trarr
         ! solve difference quotient for rhs --> tr
         !  RHS at Volume_2:
-        !
+        !  
         !  RHS*V_2 = K_33*dt*(T_1-T_2)/(Z_1-Z_2)*V_2 - K_33*dt*(T_2-T_3)/(Z_2-Z_3)*V_3
         !          = -a*T_1 + (a+c)*T_2 - c*T_3
         !
@@ -755,34 +715,34 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
         nz=nzmin
         dz=hnode_new(nz,n) ! It would be (zbar(nz)-zbar(nz+1)) if not ALE
         tr(nz)=-(b(nz)-dz)*trarr(nz,n)-c(nz)*trarr(nz+1,n)
-
+        
         do nz=nzmin+1,nzmax-2
             dz=hnode_new(nz,n)
             tr(nz)= -a(nz)     * trarr(nz-1,n) &
                     -(b(nz)-dz)* trarr(nz,n) &
                     -c(nz)     * trarr(nz+1,n)
         end do
-
+        
         nz=nzmax-1
         dz=hnode_new(nz,n)
-        tr(nz)=-a(nz)*trarr(nz-1,n)-(b(nz)-dz)*trarr(nz,n)
+        tr(nz)=-a(nz)*trarr(nz-1,n)-(b(nz)-dz)*trarr(nz,n)       
         !_______________________________________________________________________
         ! Add KPP nonlocal fluxes to the rhs (only T and S currently)
-        ! use here blmc or kpp_oblmixc instead of Kv, since Kv already contains
+        ! use here blmc or kpp_oblmixc instead of Kv, since Kv already contains 
         ! at this point the mixing enhancments from momix, instable
-        ! mixing or windmixing which are to much for nonlocal
+        ! mixing or windmixing which are to much for nonlocal 
         ! transports and lead to instability of the model
         if (use_kpp_nonlclflx) then
-            if (tracers%data(tr_num)%ID==2) then
+            if (tracers%data(tr_num)%ID==2) then 
                 rsss=ref_sss
                 if (ref_sss_local) rsss=tracers%data(tr_num)%values(1,n)
             end if
-
+            
             !___________________________________________________________________
             ! use fesom1.4 KPP
             if     (mix_scheme_nmb==1 .or. mix_scheme_nmb==17) then
                 if     (tracers%data(tr_num)%ID==1) then ! temperature
-                    ! --> no fluxes to the top out of the surface, no fluxes
+                    ! --> no fluxes to the top out of the surface, no fluxes 
                     !     downwards out of the bottom
                     !___surface_________________________________________________
                     nz = nzmin
@@ -801,16 +761,16 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
                     tr(nz)=tr(nz) &
                                +( MIN(ghats(nz  ,n)*blmc(nz  ,n,2), 1.0_WP)*(area(nz  ,n)/areasvol(nz,n)) &
                                 ) * heat_flux(n) / vcpw * dt
-
+                                
                 elseif (tracers%data(tr_num)%ID==2) then ! salinity
-                    ! --> no fluxes to the top out of the surface, no fluxes
+                    ! --> no fluxes to the top out of the surface, no fluxes 
                     !     downwards out of the bottom
                     !___surface_________________________________________________
                     nz = nzmin
                     tr(nz)=tr(nz) &
                                -(-MIN(ghats(nz+1,n)*blmc(nz+1,n,3), 1.0_WP)*(area(nz+1,n)/areasvol(nz,n)) &
                                 ) * rsss * water_flux(n) * dt
-                    !___bulk____________________________________________________
+                    !___bulk____________________________________________________            
                     do nz=nzmin+1, nzmax-2
                         tr(nz)=tr(nz) &
                                -( MIN(ghats(nz  ,n)*blmc(nz  ,n,3), 1.0_WP)*(area(nz  ,n)/areasvol(nz,n)) &
@@ -822,7 +782,7 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
                     tr(nz)=tr(nz) &
                                -( MIN(ghats(nz  ,n)*blmc(nz  ,n,3), 1.0_WP)*(area(nz  ,n)/areasvol(nz,n)) &
                                 ) * rsss * water_flux(n) * dt
-                end if
+                end if 
             !___________________________________________________________________
             ! use cvmix KPP
             elseif (mix_scheme_nmb==3 .or. mix_scheme_nmb==37) then
@@ -844,14 +804,14 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
                     tr(nz)=tr(nz) &
                                +( MIN(kpp_nonlcltranspT(nz  ,n)*kpp_oblmixc(nz  ,n,2), 1.0_WP)*(area(nz  ,n)/areasvol(nz,n)) &
                                 ) * heat_flux(n) / vcpw * dt
-
+                                
                 elseif (tracers%data(tr_num)%ID==2) then ! salinity
                     !___surface_________________________________________________
                     nz = nzmin
                     tr(nz)=tr(nz) &
                                -(-MIN(kpp_nonlcltranspS(nz+1,n)*kpp_oblmixc(nz+1,n,3), 1.0_WP)*(area(nz+1,n)/areasvol(nz,n)) &
                                 ) * rsss * water_flux(n) * dt
-                    !___bulk____________________________________________________
+                    !___bulk____________________________________________________            
                     do nz=nzmin+1, nzmax-2
                         tr(nz)=tr(nz) &
                                -( MIN(kpp_nonlcltranspS(nz  ,n)*kpp_oblmixc(nz  ,n,3), 1.0_WP)*(area(nz  ,n)/areasvol(nz,n)) &
@@ -866,7 +826,7 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
                 end if
             end if
         end if ! --> if (use_kpp_nonlclflx) then
-
+        
         !_______________________________________________________________________
         ! case of activated shortwave penetration into the ocean, ad 3d contribution
         if (use_sw_pene .and. tracers%data(tr_num)%ID==1) then
@@ -876,33 +836,42 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
                 tr(nz)=tr(nz)+(sw_3d(nz, n)-sw_3d(nz+1, n) * area(nz+1,n)/areasvol(nz,n)) * zinv
             end do
         end if
-
+        
         !_______________________________________________________________________
-        !  The first row contains also the boundary condition from heatflux,
+        ! case of activated shortwave penetration into the ocean, ad 3d contribution
+        if (use_icebergs .and. (.not. turn_off_hf) .and. tracers%data(tr_num)%ID==1) then
+            do nz=nzmin, nzmax-1
+                zinv=1.0_WP*dt  !/(zbar(nz)-zbar(nz+1)) ale!
+                !!PS tr(nz)=tr(nz)+(sw_3d(nz, n)-sw_3d(nz+1, n) * ( area(nz+1,n)/areasvol(nz,n)) ) * zinv
+                !write(*,*) "LA DEBUG: n=",n,", nz=",nz,", ibhf_n(nz, n)=",ibhf_n(nz, n)
+                tr(nz)=tr(nz)+(ibhf_n(nz, n)-ibhf_n(nz+1, n) * area(nz+1,n)/areasvol(nz,n)) * zinv / vcpw
+            end do
+        end if
+        
+        !_______________________________________________________________________
+        !  The first row contains also the boundary condition from heatflux, 
         !  freshwaterflux and relaxation terms
-        !  --> trarr(1,n)*water_flux(n) : latent heatflux contribution due to
-        !      cell volume. If Volume decreases --> temp has to raise, if volume
+        !  --> trarr(1,n)*water_flux(n) : latent heatflux contribution due to 
+        !      cell volume. If Volume decreases --> temp has to raise, if volume 
         !      expended --> temp has to decrease
-        !                           (-)   ^                        (-)   ^
-        !                            |    |                         |    |
+        !                           (-)   ^                        (-)   ^ 
+        !                            |    |                         |    | 
         !   IN MOMENT: heat_flux ~~~~|~~~~|~~~~   ,  water_flux ~~~~|~~~~|~~~~
         !  (BUT CHECK!)              |    |                         |    |
-        !                            v   (+)                        v   (+)
-        !
-        tr(nzmin)= tr(nzmin)+bc_surface(n, tracers%data(tr_num)%ID, trarr(nzmin,n), nzmin, partit)
-        if ((tracers%data(tr_num)%ID .ge. 6) .and.(tracers%data(tr_num)%ID .le. 40)) then
-          tr(nzmin)= tr(nzmin)+transit_bc_surface(n, tracers%data(tr_num)%ID, sst(nzmin,n), sss(nzmin,n), a_ice(n), trarr(nzmin,n), nzmin, partit)
-        end if
+        !                            v   (+)                        v   (+) 
+        !                            
+        tr(nzmin)= tr(nzmin)+bc_surface(n, tracers%data(tr_num)%ID, trarr(nzmin,n), nzmin, partit)  
+        
         !_______________________________________________________________________
-        ! The forward sweep algorithm to solve the three-diagonal matrix
+        ! The forward sweep algorithm to solve the three-diagonal matrix 
         ! problem
-        !
+        ! 
         !  | b_1 c_1 ...            |   |dTnew_1|
         !  | a_2 b_2 c_2 ...        |   |dTnew_2|
         !  |     a_3 b_3 c_3 ...    | * |dTnew_3| = RHS
-        !  |         a_4 b_4 c_4 ...|   |dTnew_3|
+        !  |         a_4 b_4 c_4 ...|   |dTnew_3| 
         !  |              :         |   |   :   |
-        !
+        ! 
         ! 1st: define new coefficents:
         !      --> c'_i = c_i/b_i                               ; i=1
         !          c'_i = c_i/(b_i-a_i*c'_i-1)                  ; i = 2,3,...,n-1
@@ -916,22 +885,22 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
         ! initialize c-prime and s,t-prime
         cp(nzmin) = c(nzmin)/b(nzmin)
         tp(nzmin) = tr(nzmin)/b(nzmin)
-
+        
         ! solve for vectors c-prime and t, s-prime
         do nz = nzmin+1,nzmax-1
             m = b(nz)-cp(nz-1)*a(nz)
             cp(nz) = c(nz)/m
             tp(nz) = (tr(nz)-tp(nz-1)*a(nz))/m
         end do
-
-        ! start with back substitution
+        
+        ! start with back substitution 
         tr(nzmax-1) = tp(nzmax-1)
-
+        
         ! solve for x from the vectors c-prime and d-prime
         do nz = nzmax-2, nzmin, -1
             tr(nz) = tp(nz)-cp(nz)*tr(nz+1)
         end do
-
+        
         !_______________________________________________________________________
         ! update tracer
         ! tr ... dTnew = T^(n+0.5) - T*
@@ -939,7 +908,7 @@ subroutine diff_ver_part_impl_ale(tr_num, dynamics, tracers, ice, partit, mesh)
             ! trarr - before ... T*
             trarr(nz,n)=trarr(nz,n)+tr(nz)
         end do
-
+        
     end do ! --> do n=1,myDim_nod2D
 !$OMP END DO
 !$OMP END PARALLEL
@@ -947,7 +916,7 @@ end subroutine diff_ver_part_impl_ale
 !
 !
 !===============================================================================
-subroutine diff_ver_part_redi_expl(tracers, partit, mesh)
+subroutine diff_ver_part_redi_expl(tracers, partit, mesh) 
     use o_ARRAYS
     use MOD_MESH
     USE MOD_PARTIT
@@ -972,7 +941,7 @@ subroutine diff_ver_part_redi_expl(tracers, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     del_ttf => tracers%work%del_ttf
 
 !$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, k, elem, nz, n2, nl1, ul1, nl2, Tx, Ty, vd_flux, zbar_n, z_n)
@@ -997,7 +966,7 @@ subroutine diff_ver_part_redi_expl(tracers, partit, mesh)
             tr_xynodes(2,nz,n)=ty/3.0_WP/areasvol(nz,n)
         end do
     end do
-
+    
 !$OMP END DO
     ! no halo exchange of tr_xynodes is needed !
 !$OMP DO
@@ -1005,7 +974,7 @@ subroutine diff_ver_part_redi_expl(tracers, partit, mesh)
         nl1=nlevels_nod2D(n)-1
         ul1=ulevels_nod2D(n)
         vd_flux=0._WP
-
+        
         !_______________________________________________________________________
         zbar_n(1:mesh%nl  )=0.0_WP
         z_n   (1:mesh%nl-1)=0.0_WP
@@ -1016,7 +985,7 @@ subroutine diff_ver_part_redi_expl(tracers, partit, mesh)
             z_n(nz-1)  = zbar_n(nz)   + hnode_new(nz-1,n)/2.0_WP
         end do
         zbar_n(ul1) = zbar_n(ul1+1)   + hnode_new(ul1,n)
-
+        
         !_______________________________________________________________________
         do nz=ul1+1,nl1
             vd_flux(nz)=(z_n(nz-1)-zbar_n(nz))*(slope_tapered(1,nz-1,n)*tr_xynodes(1,nz-1,n)+slope_tapered(2,nz-1,n)*tr_xynodes(2,nz-1,n))*Ki(nz-1,n)
@@ -1034,7 +1003,7 @@ end subroutine diff_ver_part_redi_expl
 !
 !
 !===============================================================================
-subroutine diff_part_hor_redi(tracers, partit, mesh)
+subroutine diff_part_hor_redi(tracers, partit, mesh) 
     use o_ARRAYS
     use MOD_MESH
     USE MOD_PARTIT
@@ -1059,7 +1028,7 @@ subroutine diff_part_hor_redi(tracers, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
     del_ttf => tracers%work%del_ttf
 
     !___________________________________________________________________________
@@ -1083,7 +1052,7 @@ subroutine diff_part_hor_redi(tracers, partit, mesh)
         !_______________________________________________________________________
         nl2=0
         ul2=0
-        if (el(2)>0) then
+        if (el(2)>0) then 
             nl2=nlevels(el(2))-1
             ul2=ulevels(el(2))
             deltaX2=edge_cross_dxdy(3,edge)
@@ -1201,8 +1170,8 @@ end subroutine diff_part_hor_redi
 !
 !
 !===============================================================================
-SUBROUTINE diff_part_bh(tr_num, dynamics, tracers, partit, mesh)
-    use o_ARRAYS, only:
+SUBROUTINE diff_part_bh(tr_num, dynamics, tracers, partit, mesh) 
+    use o_ARRAYS, only: 
     use MOD_MESH
     USE MOD_PARTIT
     USE MOD_PARSUP
@@ -1226,7 +1195,7 @@ SUBROUTINE diff_part_bh(tr_num, dynamics, tracers, partit, mesh)
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
+#include "associate_mesh_ass.h" 
 
 
     UV            => dynamics%uv(:,:,:)
@@ -1257,9 +1226,9 @@ SUBROUTINE diff_part_bh(tr_num, dynamics, tracers, partit, mesh)
            vi=u1*u1+v1*v1
            tt(nz)=ttf(nz,en(1))-ttf(nz,en(2))
            vi=sqrt(max(tracers%data(tr_num)%gamma0_tra,            &
-                   max(tracers%data(tr_num)%gamma1_tra*sqrt(vi),   &
+                   max(tracers%data(tr_num)%gamma1_tra*sqrt(vi),   & 
                        tracers%data(tr_num)%gamma2_tra*     vi)    &
-                                                         )*len)
+                                                         )*len)       
            tt(nz)=tt(nz)*vi
        END DO
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
@@ -1285,7 +1254,7 @@ SUBROUTINE diff_part_bh(tr_num, dynamics, tracers, partit, mesh)
 !$OMP END MASTER
 !$OMP BARRIER
     ! ===========
-    ! Second round:
+    ! Second round: 
     ! ===========
 !$OMP DO
     DO ed=1, myDim_edge2D!+eDim_edge2D
@@ -1303,11 +1272,11 @@ SUBROUTINE diff_part_bh(tr_num, dynamics, tracers, partit, mesh)
               vi=u1*u1+v1*v1
               tt(nz)=temporary_ttf(nz,en(1))-temporary_ttf(nz,en(2))
               vi=sqrt(max(tracers%data(tr_num)%gamma0_tra,            &
-                      max(tracers%data(tr_num)%gamma1_tra*sqrt(vi),   &
+                      max(tracers%data(tr_num)%gamma1_tra*sqrt(vi),   & 
                           tracers%data(tr_num)%gamma2_tra*     vi)    &
-                                                            )*len)
+                                                            )*len)                    
               tt(nz)=-tt(nz)*vi*dt
-          END DO
+          END DO 
 #if defined(_OPENMP)  && !defined(__openmp_reproducible)
           call omp_set_lock  (partit%plock(en(1)))
 #else
@@ -1335,33 +1304,31 @@ end subroutine diff_part_bh
 !===============================================================================
 ! this function returns a boundary conditions for a specified tracer ID and surface node
 ! ID = 0 and 1 are reserved for temperature and salinity
-FUNCTION bc_surface(n, id, sval, nzmin, partit)
+FUNCTION bc_surface(n, id, sval, nzmin, partit) 
   use MOD_MESH
   USE MOD_PARTIT
   USE MOD_PARSUP
   USE o_ARRAYS
   USE g_forcing_arrays
   USE g_config
-  use mod_transit
   implicit none
-
-  integer,       intent(in)            :: n, id, nzmin
+  
+  integer,       intent(in)            :: n, id, nzmin 
   real(kind=WP), intent(in)            :: sval
   type(t_partit),intent(inout), target :: partit
   REAL(kind=WP)                        :: bc_surface
   character(len=10)                    :: id_string
-  real(kind=WP), dimension(:), pointer :: a_ice
 
+  !  --> is_nonlinfs=1.0 for zelvel,zstar ....                            
+  !  --> is_nonlinfs=0.0 for linfs
   SELECT CASE (id)
     CASE (1)
-        bc_surface=-dt*(heat_flux(n)/vcpw + sval*water_flux(n)*is_nonlinfs)
+        bc_surface=-dt*(heat_flux(n)/vcpw + sval*water_flux(n)*is_nonlinfs)    
     CASE (2)
         ! --> real_salt_flux(:): salt flux due to containment/releasing of salt
         !     by forming/melting of sea ice
         bc_surface= dt*(virtual_salt(n) & !--> is zeros for zlevel/zstar
                     + relax_salt(n) - real_salt_flux(n)*is_nonlinfs)
-!---Transient tracers (case ##6,12,14,39) need additional input parameters 
-!   and are considered in the separate function transit_bc_surface
 !---wiso-code
     CASE (101) ! apply boundary conditions to tracer ID=101 (H218O)
         bc_surface = dt*wiso_flux_oce(n,1)
@@ -1398,132 +1365,3 @@ FUNCTION bc_surface(n, id, sval, nzmin, partit)
   END SELECT
   RETURN
 END FUNCTION
-
-
-!===============================================================================
-! This function returns a boundary conditions for a specified transient tracer ID and surface node.
-! Different to function bc_surface, SST, SSS, and sea ice concentrations are always needed as
-! auxiliary variable
-FUNCTION transit_bc_surface(n, id, sst, sss, aice, sval, nzmin, partit)
-  use MOD_MESH
-  USE MOD_PARTIT
-  USE MOD_PARSUP
-  USE o_ARRAYS
-  USE g_forcing_arrays
-  USE g_config
-  use g_clock
-  use mod_transit
-  implicit none
-
-  integer,       intent(in)            :: n, id, nzmin
-  real(kind=WP), intent(in)            :: sst, sss, aice, sval
-  type(t_partit),intent(inout), target :: partit
-  REAL(kind=WP)                        :: transit_bc_surface
-  character(len=10)                    :: id_string
-
-
-  !  --> is_nonlinfs=1.0 for zelvel,zstar ....
-  !  --> is_nonlinfs=0.0 for linfs
-
-#if defined (__oasis)
-!   SLP and wind speed in coupled setups. This is a makeshift solution
-!   as long as the true values are not provided by the AGCM / OASIS.
-    press_a = mean_slp
-    wind_2  = speed_2(stress_atmoce_x(n), stress_atmoce_y(n))
-#else
-    press_a = press_air(n)
-    wind_2  = u_wind(n)**2 + v_wind(n)**2
-#endif
-
-  SELECT CASE (id)
-
-!   Boundary conditions for additional (transient) tracers (14C, 39Ar, CFC-12, and SF6)
-    CASE (14) !   Radiocarbon (more precisely, fractionation-corrected 14C/C):
-      if (anthro_transit) then
-!       Select atmospheric input values corresponding to the latitude
-        if (y_abc > 30.)  then
-!         Northern Hemisphere
-          r14c_a = r14c_nh(ti_transit)
-        else if (y_abc <- 30.) then
-!         Southern Hemisphere
-          r14c_a = r14c_sh(ti_transit)
-        else
-!         Tropical zone
-          r14c_a = r14c_tz(ti_transit)
-        end if
-        xCO2_a = xCO2_ti(ti_transit)
-      else if (paleo_transit) then
-        r14c_a = r14c_ti(ti_transit)
-        xCO2_a = xCO2_ti(ti_transit)
-      else
-!       Constant (global-mean) namelist values are taken
-      end if
-!     Local isotopic 14CO2/CO2 air-sea exchange flux (in m / s),
-!     since F14C is normalized to atmospheric (water) values the isotopic flux has to be
-!     corrected for precipitation or evaporation fluxes with different isotopic signatures.
-      transit_bc_surface = dt * (iso_flux("co2", sst, sss, wind_2, aice, press_a, xco2_a, r14c_a, sval, dic_0)   &
-                                 - sval * water_flux(n) * is_nonlinfs)
-
-    CASE (39) ! Argon-39 (fractionationation-corrected 39Ar/Ar)
-!     Local isotopic 39Ar/Ar air-sea exchange flux (in m / s),
-!     since F39Ar is normalized to atmospheric (water) values the isotopic flux has to be
-!     corrected for precipitation or evaporation fluxes with different isotopic signatures.
-      transit_bc_surface = dt * (iso_flux("arg", sst, sss, wind_2, aice, press_a, xarg_a, r39ar_a, sval, arg_0)  &
-                                 - sval * water_flux(n) * is_nonlinfs)
-
-    CASE (12) ! CFC-12
-      if (anthro_transit) then
-!       Select atmospheric input values corresponding to the latitude
-!       Annual values are interpolated to monthly values, this is omitted in the last simulation year
-        if (y_abc > 10.)  then       ! Northern Hemisphere
-!          Northern Hemisphere
-           xf12_a = xf12_nh(ti_transit)
-           if (ti_transit < length_transit) xf12_a = xf12_a + month * (xf12_nh(ti_transit + 1) - xf12_a) / 12.
-        else if (y_abc <- 10.) then
-!          Southern Hemisphere
-           xf12_a = xf12_sh(ti_transit)
-           if (ti_transit < length_transit) xf12_a = xf12_a + month * (xf12_sh(ti_transit + 1) - xf12_a) / 12.
-        else
-!          Tropical zone, interpolate between NH and SH
-           xf12_a = (1 - yy_nh) * xf12_nh(ti_transit) + yy_nh * xf12_sh(ti_transit)
-           if (ti_transit < length_transit) &
-             xf12_a = xf12_a + month * ((1 - yy_nh) * xf12_nh(ti_transit + 1) + yy_nh * xf12_sh(ti_transit + 1) - xf12_a) / 12.
-        end if
-      else
-!       Constant (global-mean) namelist values are taken
-      end if
-
-!     Local air-sea exchange gas flux of CFC-12 (in m / s):
-      transit_bc_surface = dt * (gas_flux("f12", sst, sss, wind_2, aice, press_a, xf12_a, sval)  &
-                                 - sval * water_flux(n) * is_nonlinfs)
-
-    CASE (6) ! SF6
-      if (anthro_transit) then
-!       Select atmospheric input values corresponding to the latitude
-!       Annual values are interpolated to monthly values, this is omitted in the last simulation year
-        if (y_abc > 10.)  then       ! Northern Hemisphere
-!         Northern Hemisphere
-          xsf6_a = xsf6_nh(ti_transit)
-          if (ti_transit < length_transit) xsf6_a = xsf6_a + month * (xsf6_nh(ti_transit + 1) - xsf6_a) / 12.
-        else if (y_abc <- 10.) then
-!         Southern Hemisphere
-          xsf6_a = xsf6_sh(ti_transit)
-          if (ti_transit < length_transit) xsf6_a = xsf6_a + month * (xsf6_sh(ti_transit + 1) - xsf6_a) / 12.
-        else
-!         Tropical zone, interpolate between NH and SH
-          xsf6_a = (1 - yy_nh) * xsf6_nh(ti_transit) + yy_nh * xsf6_sh(ti_transit)
-          if (ti_transit < length_transit) &
-            xsf6_a = xsf6_a + month * ((1 - yy_nh) * xsf6_nh(ti_transit + 1) + yy_nh * xsf6_sh(ti_transit + 1) - xsf6_a) / 12.
-        end if
-      else
-!       Constant (global-mean) namelist values are taken
-      end if
-
-!     Local air-sea exchange gas flux of SF6 (in m / s):
-      transit_bc_surface = dt * (gas_flux("sf6", sst, sss, wind_2, aice, press_a, xsf6_a, sval)  &
-                                 - sval * water_flux(n) * is_nonlinfs)
-
-!   Done with boundary conditions for (transient) tracers.
-  END SELECT
-  RETURN
-END FUNCTION
diff --git a/src/oce_ale_vel_rhs.F90 b/src/oce_ale_vel_rhs.F90
index 164a7df4..b951767c 100644
--- a/src/oce_ale_vel_rhs.F90
+++ b/src/oce_ale_vel_rhs.F90
@@ -60,19 +60,17 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
     integer                  :: use_pice
     !___________________________________________________________________________
     ! pointer on necessary derived types
-    real(kind=WP), dimension(:,:,:),   pointer :: UV, UV_rhs
-    real(kind=WP), dimension(:,:,:,:), pointer :: UV_rhsAB
-    real(kind=WP), dimension(:)    ,   pointer :: eta_n
-    real(kind=WP), dimension(:)    ,   pointer :: m_ice, m_snow, a_ice
-    real(kind=WP)                  ,   pointer :: rhoice, rhosno, inv_rhowat
-    real(kind=WP)                              :: ab1, ab2, ab3 !Adams-Bashforth coefficients
+    real(kind=WP), dimension(:,:,:), pointer :: UV, UV_rhsAB, UV_rhs
+    real(kind=WP), dimension(:)    , pointer :: eta_n
+    real(kind=WP), dimension(:)    , pointer :: m_ice, m_snow, a_ice
+    real(kind=WP)                  , pointer :: rhoice, rhosno, inv_rhowat
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
     UV        => dynamics%uv(:,:,:)
     UV_rhs    => dynamics%uv_rhs(:,:,:)
-    UV_rhsAB  => dynamics%uv_rhsAB(:,:,:,:)
+    UV_rhsAB  => dynamics%uv_rhsAB(:,:,:)
     eta_n     => dynamics%eta_n(:)
     m_ice     => ice%data(2)%values(:)
     m_snow    => ice%data(3)%values(:)
@@ -85,20 +83,6 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
     if (use_floatice .and.  .not. trim(which_ale)=='linfs') use_pice=1
     if ((toy_ocean)  .and. (trim(which_toy)=="soufflet"))   use_pice=0
 
-    IF     (dynamics%AB_order==2)  THEN
-            ab1=-(0.5_WP+epsilon)
-            ab2= (1.5_WP+epsilon)
-            ab3=  0.0_WP
-    ELSEIF (dynamics%AB_order==3) THEN
-            ab1=  5.0_WP/12.0_WP
-            ab2=-16.0_WP/12.0_WP
-            ab3= 23.0_WP/12.0_WP
-    ELSE 
-       write(*,*) 'unsuppported AB scheme for momentum, use 2 or 3'
-       call par_ex(partit%MPI_COMM_FESOM, partit%mype, 1)
-       stop
-    END IF 
-
 !$OMP PARALLEL DO DEFAULT(SHARED) PRIVATE(elem, nz, nzmin, nzmax, elnodes, ff, mm, Fx, Fy, pre, p_ice, p_air, p_eta)
     do elem=1, myDim_elem2D
         nzmax = nlevels(elem)
@@ -106,29 +90,16 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
         !___________________________________________________________________________
         ! Take care of the AB part
         !!PS do nz=1,nl-1
-        IF (dynamics%AB_order==2) THEN
         do nz=nzmin,nzmax-1
-            UV_rhs(1,nz,elem)=ab1*UV_rhsAB(1,1,nz,elem)
-            UV_rhs(2,nz,elem)=ab1*UV_rhsAB(1,2,nz,elem)
+            UV_rhs(1,nz,elem)=-(0.5_WP+epsilon)*UV_rhsAB(1,nz,elem)   
+            UV_rhs(2,nz,elem)=-(0.5_WP+epsilon)*UV_rhsAB(2,nz,elem)
         end do
         if (dynamics%ldiag_ke) then
            do nz=nzmin,nzmax-1
-              dynamics%ke_adv(:,nz,elem)=ab1*dynamics%ke_adv_AB(1, :,nz,elem)
-              dynamics%ke_cor(:,nz,elem)=ab1*dynamics%ke_cor_AB(1, :,nz,elem)
+              dynamics%ke_adv(:,nz,elem)=-(0.5_WP+epsilon)*dynamics%ke_adv_AB(:,nz,elem)   
+              dynamics%ke_cor(:,nz,elem)=-(0.5_WP+epsilon)*dynamics%ke_cor_AB(:,nz,elem)   
            end do
         end if
-        ELSEIF (dynamics%AB_order==3) THEN
-        do nz=nzmin,nzmax-1
-            UV_rhs(1,nz,elem)=ab1*UV_rhsAB(2,1,nz,elem)+ab2*UV_rhsAB(1,1,nz,elem)
-            UV_rhs(2,nz,elem)=ab1*UV_rhsAB(2,2,nz,elem)+ab2*UV_rhsAB(1,2,nz,elem)
-        end do
-        if (dynamics%ldiag_ke) then
-           do nz=nzmin,nzmax-1
-              dynamics%ke_adv(:,nz,elem)=ab1*dynamics%ke_adv_AB(2,:,nz,elem)+ab2*dynamics%ke_adv_AB(1,:,nz,elem)
-              dynamics%ke_cor(:,nz,elem)=ab1*dynamics%ke_cor_AB(2,:,nz,elem)+ab2*dynamics%ke_cor_AB(1,:,nz,elem)
-           end do
-        end if      
-        END IF
         
         !___________________________________________________________________________
         ! Sea level and pressure contribution   -\nabla(\eta +hpressure/rho_0)
@@ -179,37 +150,22 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
             ! add pressure gradient terms
             UV_rhs(1,nz,elem)   = UV_rhs(1,nz,elem) + (Fx-pgf_x(nz,elem))*elem_area(elem) 
             UV_rhs(2,nz,elem)   = UV_rhs(2,nz,elem) + (Fy-pgf_y(nz,elem))*elem_area(elem)
-
-            IF (dynamics%AB_order==2) THEN
+            
             ! add coriolis force
-            UV_rhsAB(1,1,nz,elem) = UV(2,nz,elem)*ff! + mm*UV(1,nz,elem)*UV(2,nz,elem)
-            UV_rhsAB(1,2,nz,elem) =-UV(1,nz,elem)*ff! - mm*UV(1,nz,elem)*UV(2,nz,elem)
-            ELSEIF (dynamics%AB_order==3) THEN
-            UV_rhsAB(2,1,nz,elem) = UV_rhsAB(1,1,nz,elem)
-            UV_rhsAB(2,2,nz,elem) = UV_rhsAB(1,2,nz,elem)
-            UV_rhsAB(1,1,nz,elem) = UV(2,nz,elem)*ff
-            UV_rhsAB(1,2,nz,elem) =-UV(1,nz,elem)*ff
-            END IF
+            UV_rhsAB(1,nz,elem) = UV(2,nz,elem)*ff! + mm*UV(1,nz,elem)*UV(2,nz,elem)
+            UV_rhsAB(2,nz,elem) =-UV(1,nz,elem)*ff! - mm*UV(1,nz,elem)*UV(2,nz,elem)
         end do
 
         if (dynamics%ldiag_ke) then
            do nz=nzmin,nzmax-1
               dynamics%ke_pre(1,nz,elem)= (Fx-pgf_x(nz,elem))*dt!*elem_area(elem) !not to divide it aterwards (at the end of this subroutine)
-              dynamics%ke_pre(2,nz,elem)= (Fy-pgf_y(nz,elem))*dt!*elem_area(elem) !but account for DT here              
+              dynamics%ke_pre(2,nz,elem)= (Fy-pgf_y(nz,elem))*dt!*elem_area(elem) !but account for DT here
 
-              IF (dynamics%AB_order==3) THEN
-              dynamics%ke_cor_AB(2,1,nz,elem) = dynamics%ke_cor_AB(1,1,nz,elem)
-              dynamics%ke_cor_AB(2,2,nz,elem) = dynamics%ke_cor_AB(1,2,nz,elem)
+              dynamics%ke_cor_AB(1,nz,elem)= UV(2,nz,elem)*ff
+              dynamics%ke_cor_AB(2,nz,elem)=-UV(1,nz,elem)*ff
 
-              dynamics%ke_adv_AB(2,1,nz,elem)= dynamics%ke_adv_AB(1,1,nz,elem)
-              dynamics%ke_adv_AB(2,2,nz,elem)= dynamics%ke_adv_AB(1,2,nz,elem)
-              END IF
-
-              dynamics%ke_cor_AB(1,1,nz,elem)= UV(2,nz,elem)*ff
-              dynamics%ke_cor_AB(1,2,nz,elem)=-UV(1,nz,elem)*ff
-
-              dynamics%ke_adv_AB(1,1,nz,elem)= 0.0_WP
-              dynamics%ke_adv_AB(1,2,nz,elem)= 0.0_WP
+              dynamics%ke_adv_AB(1,nz,elem)= 0.0_WP
+              dynamics%ke_adv_AB(2,nz,elem)= 0.0_WP
            end do
         end if
 
@@ -225,13 +181,8 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
        call momentum_adv_scalar(dynamics, partit, mesh)
     end if
     !___________________________________________________________________________
-    ! Update the rhs
-    IF (dynamics%AB_order==2) THEN
-        ff=ab2
-    ELSEIF (dynamics%AB_order==3) THEN
-        ff=ab3
-    END IF
-    
+    ! Update the rhs   
+    ff=(1.5_WP+epsilon)
     if (lfirst.and.(.not.r_restart)) then
         ff=1.0_WP
         lfirst=.false.
@@ -241,8 +192,8 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
         nzmax = nlevels(elem)
         nzmin = ulevels(elem)
         do nz=nzmin,nzmax-1
-            UV_rhs(1,nz,elem)=dt*(UV_rhs(1,nz,elem)+UV_rhsAB(1,1,nz,elem)*ff)/elem_area(elem)
-            UV_rhs(2,nz,elem)=dt*(UV_rhs(2,nz,elem)+UV_rhsAB(1,2,nz,elem)*ff)/elem_area(elem)
+            UV_rhs(1,nz,elem)=dt*(UV_rhs(1,nz,elem)+UV_rhsAB(1,nz,elem)*ff)/elem_area(elem)
+            UV_rhs(2,nz,elem)=dt*(UV_rhs(2,nz,elem)+UV_rhsAB(2,nz,elem)*ff)/elem_area(elem)
         end do
     end do
 !$OMP END PARALLEL DO
@@ -253,8 +204,8 @@ subroutine compute_vel_rhs(ice, dynamics, partit, mesh)
         nzmax = nlevels(elem)
         nzmin = ulevels(elem)
         do nz=nzmin,nzmax-1
-            dynamics%ke_adv(:,nz,elem)=dt*(dynamics%ke_adv(:,nz,elem)+dynamics%ke_adv_AB(1,:,nz,elem)*ff)/elem_area(elem)
-            dynamics%ke_cor(:,nz,elem)=dt*(dynamics%ke_cor(:,nz,elem)+dynamics%ke_cor_AB(1,:,nz,elem)*ff)/elem_area(elem)
+            dynamics%ke_adv(:,nz,elem)=dt*(dynamics%ke_adv(:,nz,elem)+dynamics%ke_adv_AB(:,nz,elem)*ff)/elem_area(elem)
+            dynamics%ke_cor(:,nz,elem)=dt*(dynamics%ke_cor(:,nz,elem)+dynamics%ke_cor_AB(:,nz,elem)*ff)/elem_area(elem)
         end do
     end do
 !$OMP END PARALLEL DO
@@ -287,15 +238,14 @@ subroutine momentum_adv_scalar(dynamics, partit, mesh)
     real(kind=WP)            :: wu(1:mesh%nl), wv(1:mesh%nl)
     !___________________________________________________________________________
     ! pointer on necessary derived types
-    real(kind=WP), dimension(:,:,:),   pointer :: UV, UVnode_rhs
-    real(kind=WP), dimension(:,:,:,:), pointer :: UV_rhsAB
-    real(kind=WP), dimension(:,:)    , pointer :: Wvel_e
+    real(kind=WP), dimension(:,:,:), pointer :: UV, UV_rhsAB, UVnode_rhs
+    real(kind=WP), dimension(:,:)  , pointer :: Wvel_e
 #include "associate_part_def.h"
 #include "associate_mesh_def.h"
 #include "associate_part_ass.h"
 #include "associate_mesh_ass.h"
     UV        =>dynamics%uv(:,:,:)
-    UV_rhsAB  =>dynamics%uv_rhsAB(:,:,:,:)
+    UV_rhsAB  =>dynamics%uv_rhsAB(:,:,:)
     UVnode_rhs=>dynamics%work%uvnode_rhs(:,:,:)
     Wvel_e    =>dynamics%w_e(:,:)
 
@@ -502,7 +452,7 @@ subroutine momentum_adv_scalar(dynamics, partit, mesh)
     do el=1, myDim_elem2D
         nl1 = nlevels(el)-1
         ul1 = ulevels(el)
-        UV_rhsAB(1,1:2,ul1:nl1,el) = UV_rhsAB(1,1:2,ul1:nl1,el) &
+        UV_rhsAB(1:2,ul1:nl1,el) = UV_rhsAB(1:2,ul1:nl1,el) &
                 + elem_area(el)*(UVnode_rhs(1:2,ul1:nl1,elem2D_nodes(1,el)) &
                 + UVnode_rhs(1:2,ul1:nl1,elem2D_nodes(2,el)) & 
                 + UVnode_rhs(1:2,ul1:nl1,elem2D_nodes(3,el))) / 3.0_WP     
@@ -515,7 +465,7 @@ subroutine momentum_adv_scalar(dynamics, partit, mesh)
        do el=1, myDim_elem2D
           nl1 = nlevels(el)-1
           ul1 = ulevels(el)
-          dynamics%ke_adv_AB(1,1:2,ul1:nl1,el) = dynamics%ke_adv_AB(1,1:2,ul1:nl1,el) &
+          dynamics%ke_adv_AB(1:2,ul1:nl1,el) = dynamics%ke_adv_AB(1:2,ul1:nl1,el) &
                 + elem_area(el)*(UVnode_rhs(1:2,ul1:nl1,elem2D_nodes(1,el)) &
                 + UVnode_rhs(1:2,ul1:nl1,elem2D_nodes(2,el)) & 
                 + UVnode_rhs(1:2,ul1:nl1,elem2D_nodes(3,el))) / 3.0_WP     
diff --git a/src/oce_dyn.F90 b/src/oce_dyn.F90
index 01cfad89..67d34e77 100755
--- a/src/oce_dyn.F90
+++ b/src/oce_dyn.F90
@@ -54,18 +54,6 @@ module visc_filt_bidiff_interface
   end interface
 end module
 
-module check_validviscopt_interface
-    interface
-        subroutine check_validviscopt_5(partit, mesh)
-            USE MOD_MESH
-            USE MOD_PARTIT
-            USE MOD_PARSUP
-            type(t_partit), intent(inout), target :: partit
-            type(t_mesh)  , intent(in)   , target :: mesh
-        end subroutine    
-    end interface
-end module 
-
 ! 
 ! Contains routines needed for computations of dynamics.
 ! includes: update_vel, compute_vel_nodes
@@ -568,7 +556,7 @@ SUBROUTINE visc_filt_bidiff(dynamics, partit, mesh)
        V_c(:, elem) = 0.0_WP
     END DO
 !$OMP END PARALLEL DO
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(u1, v1, len, vi, ed, el, nz, nzmin, nzmax, update_u, update_v)
+!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(u1, v1, len, vi, ed, el, nz, nzmin, nzmax)
 !$OMP DO
     DO ed=1, myDim_edge2D+eDim_edge2D
         if(myList_edge2D(ed)>edge2D_in) cycle
@@ -758,151 +746,3 @@ salt   => tracers%data(2)%values(:,:)
   call exchange_nod(dynamics%ke_swA, partit)
   call exchange_nod(dynamics%ke_swB, partit)
 END SUBROUTINE compute_apegen
-
-
-!
-!
-!_______________________________________________________________________________
-! check validity of visc_opt=5 selection on basis of ratio betweem resolution and 
-! 1st baroclinic rossby radius. visc_opt=5 ("easy backscatter") in eddy 
-! resolving/partly eddy permitting setups can lead to problems in form of 
-! exedingly strong near boundary currents that can lead e.g to very weak 
-! Drake Passage throughflow (<80Sv). In this case better use visc_opt=7, 
-! which is the flow aware viscosity option 
-subroutine check_viscopt(dynamics, partit, mesh)
-    USE MOD_DYN
-    USE MOD_PARTIT
-    USE MOD_PARSUP
-    USE MOD_MESH
-    USE check_validviscopt_interface
-    IMPLICIT NONE
-    type(t_dyn)   , intent(inout), target :: dynamics
-    type(t_partit), intent(inout), target :: partit
-    type(t_mesh)  , intent(in)   , target :: mesh
-    
-    if (dynamics%check_opt_visc) then
-        select case (dynamics%opt_visc)
-            case(5) 
-                ! check validity of visc_opt=5 especially in higher resolved setups
-                ! --> there it can lead to problems
-                call check_validviscopt_5(partit, mesh)
-        end select   
-    end if 
-end subroutine check_viscopt
-
-!
-!
-!_______________________________________________________________________________
-! check if viscopt=5 is a valid and recommended option for the used configuration
-subroutine check_validviscopt_5(partit, mesh)
-    USE MOD_MESH
-    USE MOD_PARTIT
-    USE MOD_PARSUP
-    USE o_PARAM , ONLY: rad
-    USE o_ARRAYS, ONLY: bvfreq
-    USE g_CONFIG
-    USE g_comm_auto
-    IMPLICIT NONE
-    type(t_mesh),   intent(in),    target :: mesh
-    type(t_partit), intent(inout), target :: partit
-    integer                               :: node, nz, nzmax, nzmin
-    real(kind=WP)                         :: f_min=1.e-6_WP, z_min=100.0_WP, r_max=200000._WP, r_min=2000.0_WP
-    real(kind=WP)                         :: c_min=0.5_WP, c1
-    real(kind=WP)                         :: excl_EQ=30.0, thresh_ratio=1.5
-    real(kind=WP)                         :: loc_R, loc_A
-    real(kind=WP)                         :: glb_R, glb_A
-    real(kind=WP)                         :: fac_ResR1barocl, rossbyr_1barocl
-#include "associate_part_def.h"
-#include "associate_mesh_def.h"
-#include "associate_part_ass.h"
-#include "associate_mesh_ass.h"
-    
-    !___________________________________________________________________________
-    loc_R = 0.0_WP
-    loc_A = 0.0_WP
-    
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(node, nz, nzmax, nzmin, c1, rossbyr_1barocl, &
-!$OMP                                  fac_ResR1barocl) REDUCTION(+:loc_R, loc_A)
-!$OMP DO
-    do node=1, myDim_nod2D
-        !_______________________________________________________________________
-        ! Exlude the equator |lat|<30° from checking the ration between resolution
-        ! and first baroclinic rossby radius  
-        if (abs(mesh%geo_coord_nod2D(2,node)/rad)<excl_EQ) cycle
-        
-        !_______________________________________________________________________
-        !ca. first baroclinic gravity wave speed limited from below by c_min
-        nzmax=mesh%nlevels_nod2D_min(node)
-        nzmin=mesh%ulevels_nod2D_max(node)
-        c1=0.0_WP
-        do nz=nzmin, nzmax-1
-            c1=c1+hnode_new(nz,node)*(                                         &
-                                    sqrt(abs(max(bvfreq(nz  ,node), 0._WP)))+  &
-                                    sqrt(abs(max(bvfreq(nz+1,node), 0._WP)))   &
-                                  )/2._WP ! add abs() for -0 case, cray
-        end do
-        c1=max(c_min, c1/pi)
-        
-        !_______________________________________________________________________
-        ! compute 1st. baroclinic rossby radius
-        rossbyr_1barocl = max( min( c1/max( abs(mesh%coriolis_node(node)), f_min), r_max), r_min)
-        
-        !_______________________________________________________________________
-        ! compute ratio between rossby and resolution, if ratio >=1 setup is not eddy 
-        ! resolving in best case eddy permitting --> GM/Redi will still be neccessary
-        fac_ResR1barocl =min(mesh_resolution(node)/rossbyr_1barocl, 5._WP)
-        
-        !_______________________________________________________________________
-        ! compute local mean ratio but exclude equator |lat|<30°, since Rossby radius 
-        ! at equator becomes very large
-        loc_R   = loc_R + fac_ResR1barocl
-        loc_A   = loc_A + 1.0_WP
-    end do
-!$OMP END DO
-!$OMP END PARALLEL
-!$OMP BARRIER
-
-    !___________________________________________________________________________
-    ! compute global mean ratio --> core2 Ratio=4.26 (eddy parameterizted), 
-    ! dart Ratio=0.97 (eddy resolving/permitting)
-    call MPI_AllREDUCE(loc_R, glb_R, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_FESOM, MPIerr)
-    call MPI_AllREDUCE(loc_A, glb_A, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_FESOM, MPIerr)
-    glb_R  = glb_R/glb_A
-    
-    !___________________________________________________________________________
-    ! create warning message when ratio<thresh_ratio
-    if (glb_R<thresh_ratio) then 
-        if (mype==0) then 
-        print *, achar(27)//'[33m'
-        write(*,*) '____________________________________________________________________'
-        write(*,*) ' --check opt_visc--> Mean Ratio Resol/Rrossby = ',  glb_R 
-        write(*,*) '____________________________________________________________________'
-        write(*,*) ' WARNING: You want to use opt_visc=5 (easy backscatter viscosity) in'
-        write(*,*) '          an eddy resolving to eddy permitting (Resol/Rrossby<1.5)  '
-        write(*,*) '          configuration. It revealed to us that this can lead to    '
-        write(*,*) '          problems in form of unrealistically strong near boundary  '
-        write(*,*) '          currents that can ultimatively lead to an e.g very weak   '
-        write(*,*) '          Drake Passage throughflow (<80Sv in spinup up dart        '
-        write(*,*) '          configuration). For these kind of setups we recommend to  '
-        write(*,*) '          use opt_visc=7, which is the flow aware viscosity         '
-        write(*,*) '          parameterisation.                                         '
-        write(*,*) '          The easy backscatter viscosity was designed to energetise '
-        write(*,*) '          coarse resolved configurations!!!                         '
-        write(*,*) '          --> So please change the viscosity option in namelist.dyn '
-        write(*,*) '              to opt_visc=7 .                                       '
-        write(*,*) '          --> If you insist in using opt_visc=5 in your simulation, '
-        write(*,*) '              you can switch off this check in namelist.dyn by      '
-        write(*,*) '              setting check_opt_visc=.false.                        '
-        write(*,*) '                                                                    '
-        write(*,*) '____________________________________________________________________'
-        print *, achar(27)//'[0m'
-        write(*,*)
-        call par_ex(partit%MPI_COMM_FESOM, partit%mype, 0)
-        end if
-    else
-        if (mype==0) then 
-        write(*,*) ' --check opt_visc--> Mean Ratio Resol/Rrossby = ',  glb_R 
-        end if
-    end if     
-    
-end subroutine check_validviscopt_5
diff --git a/src/oce_fer_gm.F90 b/src/oce_fer_gm.F90
index dbf7d08f..f450be55 100644
--- a/src/oce_fer_gm.F90
+++ b/src/oce_fer_gm.F90
@@ -158,8 +158,8 @@ subroutine fer_solve_Gamma(partit, mesh)
     END DO   !!! cycle over nodes
 !$OMP END DO
 !$OMP END PARALLEL
-!$OMP BARRIER
     call exchange_nod(fer_gamma, partit)
+!$OMP BARRIER
 END subroutine fer_solve_Gamma
 !
 !
@@ -223,8 +223,8 @@ subroutine init_Redi_GM(partit, mesh) !fer_compute_C_K_Redi
     IMPLICIT NONE
     type(t_mesh),   intent(in),    target :: mesh
     type(t_partit), intent(inout), target :: partit
-    integer                  :: n, k, nz, nzmax, nzmin
-    real(kind=WP)            :: reso, c1, rosb, scaling, rr_ratio, aux, aux_zz(mesh%nl)
+    integer                  :: n, nz, nzmax, nzmin
+    real(kind=WP)            :: reso, c1, rosb, scaling, rr_ratio, aux_zz(mesh%nl)
     real(kind=WP)            :: x0=1.5_WP, sigma=.15_WP ! Fermi function parameters to cut off GM where Rossby radius is resolved
     real(kind=WP)            :: c_min=0.5_WP, f_min=1.e-6_WP, r_max=200000._WP
     real(kind=WP)            :: zscaling(mesh%nl)
@@ -237,18 +237,11 @@ subroutine init_Redi_GM(partit, mesh) !fer_compute_C_K_Redi
 
     ! fill arrays for 3D Redi and GM coefficients: F1(xy)*F2(z)
     !******************************* F1(x,y) ***********************************
-!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, k, nz, nzmax, nzmin, reso, c1, rosb, scaling, rr_ratio, aux, aux_zz, zscaling, bvref)
+!$OMP PARALLEL DEFAULT(SHARED) PRIVATE(n, nz, nzmax, nzmin, reso, c1, rosb, scaling, rr_ratio, aux_zz, zscaling, bvref)
 !$OMP DO
     do n=1, myDim_nod2D
-       !nzmax=minval(nlevels(nod_in_elem2D(1:nod_in_elem2D_num(n), n)), 1)
-       !nzmin=maxval(ulevels(nod_in_elem2D(1:nod_in_elem2D_num(n), n)), 1)
-       !Intel vectorisation did something strange in the above lines hence we had to code it more explicitely
-        nzmax=mesh%nl
-        nzmin=1
-        do k=1, nod_in_elem2D_num(n)
-           nzmax=min(nzmax, nlevels(nod_in_elem2D(k, n)))
-           nzmin=max(nzmin, ulevels(nod_in_elem2D(k, n)))
-        end do
+        nzmax=minval(nlevels(nod_in_elem2D(1:nod_in_elem2D_num(n), n)), 1)
+        nzmin=maxval(ulevels(nod_in_elem2D(1:nod_in_elem2D_num(n), n)), 1)
         reso=mesh_resolution(n)
         if (Fer_GM) then
             c1=0._wp
@@ -289,10 +282,7 @@ subroutine init_Redi_GM(partit, mesh) !fer_compute_C_K_Redi
 !!PS                 scaling=scaling*max((reso/10000.0_WP-3.0_WP), 0._WP) !no GM below 30km resolution
 !!PS             end if
             if (reso/1000.0_WP < K_GM_rampmax) then
-                !scaling=scaling*max((reso/1000.0_WP-K_GM_rampmin)/(K_GM_rampmax-K_GM_rampmin), 0._WP) !no GM below 30km resolution
-                !even if the condition is not met, compiling with Intel caused division by 0 here when optimization was used
-                !hence we limit the denominator by 1.e-12
-                scaling=scaling*max((reso/1000.0_WP-K_GM_rampmin)/max(K_GM_rampmax-K_GM_rampmin, 1.e-12), 0._WP) !no GM below 30km resolution
+                scaling=scaling*max((reso/1000.0_WP-K_GM_rampmin)/(K_GM_rampmax-K_GM_rampmin), 0._WP) !no GM below 30km resolution
             end if
             
             !___________________________________________________________________
@@ -412,9 +402,9 @@ subroutine init_Redi_GM(partit, mesh) !fer_compute_C_K_Redi
    end do
 !$OMP END DO
 !$OMP END PARALLEL
-!$OMP BARRIER
    if (Fer_GM) call exchange_nod(fer_c, partit)
    if (Fer_GM) call exchange_nod(fer_k, partit)
    if (Redi)   call exchange_nod(Ki, partit)
+!$OMP BARRIER
 end subroutine init_Redi_GM
 !====================================================================
diff --git a/src/oce_mesh.F90 b/src/oce_mesh.F90
index 1843e345..65378531 100755
--- a/src/oce_mesh.F90
+++ b/src/oce_mesh.F90
@@ -842,28 +842,28 @@ end if
  n=com_elem2D_full%sptr(com_elem2D_full%sPEnum+1)-1
  ALLOCATE(com_elem2D_full%slist(n))
  read(fileID,*) com_elem2D_full%slist
- close(fileID)
 
+!!$ read(fileID,*) com_edge2D%rPEnum
+!!$ ALLOCATE(com_edge2D%rPE(com_edge2D%rPEnum))
+!!$ read(fileID,*) com_edge2D%rPE
+!!$ ALLOCATE(com_edge2D%rptr(com_edge2D%rPEnum+1))
+!!$ read(fileID,*) com_edge2D%rptr
+!!$ ALLOCATE(com_edge2D%rlist(eDim_edge2D))
+!!$ read(fileID,*) com_edge2D%rlist
+!!$	 
+!!$ read(fileID,*) com_edge2D%sPEnum
+!!$ ALLOCATE(com_edge2D%sPE(com_edge2D%sPEnum))
+!!$ read(fileID,*) com_edge2D%sPE
+!!$ ALLOCATE(com_edge2D%sptr(com_edge2D%sPEnum+1))
+!!$ read(fileID,*) com_edge2D%sptr
+!!$ n=com_edge2D%sptr(com_edge2D%sPEnum+1)-1
+!!$ ALLOCATE(com_edge2D%slist(n))
+!!$ read(fileID,*) com_edge2D%slist
+ close(fileID)
  if (mype==0) write(*,*) 'communication arrays are read'
  deallocate(rbuff, ibuff)
  deallocate(mapping)
-
-! necessary for MULTIO auxuary data:
-! one element might belong to several processes hence we unify the element partition 
-! such that sum(myDim_elem2D_shrinked) over all processors will give elem2D
- partit%myDim_elem2D_shrinked=0
- DO n=1, myDim_elem2D
-    if (mesh%elem2D_nodes(1, n) > myDim_nod2D) cycle
-    partit%myDim_elem2D_shrinked=partit%myDim_elem2D_shrinked+1
- END DO
- allocate(partit%myInd_elem2D_shrinked(partit%myDim_elem2D_shrinked))
-! fill the respective indicies
- nn=1
- DO n=1, myDim_elem2D
-    if (mesh%elem2D_nodes(1, n) > myDim_nod2D) cycle
-    partit%myInd_elem2D_shrinked(nn)=n
-    nn=nn+1
- END DO
+ 
 ! no checksum for now, execute_command_line is failing too often. if you think it is important, please drop me a line and I will try to revive it: jan.hegewald@awi.de
 mesh%representative_checksum = ''
 
diff --git a/src/oce_modules.F90 b/src/oce_modules.F90
index 184c9d4f..d6ae7cee 100755
--- a/src/oce_modules.F90
+++ b/src/oce_modules.F90
@@ -2,7 +2,7 @@
 ! Modules of cell-vertex ocean model
 ! S. Danilov, 2012 (sergey.danilov@awi.de)
 ! SI units are used
-
+  
 !==========================================================
 MODULE o_PARAM
 integer, parameter            :: WP=8        ! Working precision
@@ -11,7 +11,7 @@ integer		                  :: mstep
 real(kind=WP), parameter      :: pi=3.14159265358979
 real(kind=WP), parameter      :: rad=pi/180.0_WP
 real(kind=WP), parameter      :: density_0=1030.0_WP
-real(kind=WP), parameter      :: density_0_r=1.0_WP/density_0 ! [m^3/kg]
+real(kind=WP), parameter      :: density_0_r=1.0_WP/density_0 ! [m^3/kg]         
 real(kind=WP), parameter      :: g=9.81_WP
 real(kind=WP), parameter      :: r_earth=6367500.0_WP
 real(kind=WP), parameter      :: omega=2*pi/(3600.0_WP*24.0_WP)
@@ -47,18 +47,18 @@ logical                       :: Fer_GM =.false.  !flag for Ferrari et al. (2010
 real(kind=WP)                 :: K_GM_max = 3000.
 real(kind=WP)                 :: K_GM_min = 2.0
 integer                       :: K_GM_bvref = 2 ! 0...surface, 1...bottom mixlay, 2...mean over mixlay
-real(kind=WP)                 :: K_GM_resscalorder = 2.0
+real(kind=WP)                 :: K_GM_resscalorder = 2.0 
 real(kind=WP)                 :: K_GM_rampmax = 40.0 ! Resol >K_GM_rampmax[km] GM full
 real(kind=WP)                 :: K_GM_rampmin = 30.0 ! Resol <K_GM_rampmin[km] GM off
 logical                       :: scaling_Ferreira   =.true.
-logical                       :: scaling_Rossby     =.false.
+logical                       :: scaling_Rossby     =.false. 
 logical                       :: scaling_resolution =.true.
 logical                       :: scaling_FESOM14    =.false.
 logical                       :: Redi               =.false.  !flag for Redi scheme
 
 real(kind=WP)                 :: visc_sh_limit=5.0e-3      !for KPP, max visc due to shear instability
 real(kind=WP)                 :: diff_sh_limit=5.0e-3      !for KPP, max diff due to shear instability
-logical                       :: Kv0_const=.true.		    !use Kv0 varying with depth and latitude
+logical                       :: Kv0_const=.true.		    !use Kv0 varying with depth and latitude 
 logical                       :: double_diffusion=.false.  !for KPP,dd switch
                                  ! KPP parametrization
 character(25)                 :: mix_scheme     ='KPP'	   !'KPP','PP'
@@ -68,17 +68,15 @@ real(KIND=WP)                 :: concv  = 1.6_WP  ! constant for pure convection
 
 logical                       :: hbl_diag =.false.        ! writen boundary layer depth
 logical                       :: use_global_tides=.false. ! tidal potential will be computed and used in the SSH gradient computation
-! Time stepping
+! Time stepping                               
 ! real(kind=WP)                 :: alpha=1.0_WP, theta=1.0_WP ! implicitness for
 real(kind=WP)                 :: alpha=1.0_WP, theta=1.0_WP ! implicitness for
                                                  ! elevation and divergence
-real(kind=WP)                 :: epsilon=0.1_WP  ! AB2 offset
+real(kind=WP)                 :: epsilon=0.1_WP  ! AB2 offset 
 ! Tracers
 
 logical                       :: SPP=.false.
 
-integer                       :: acc_vl = 64
-
 TYPE tracer_source3d_type
     integer                             :: locID
     integer                             :: ID
@@ -100,16 +98,16 @@ integer                        :: index_age_tracer = -1 ! water age tracer index
 
 ! Momentum
 !!PS logical                       :: free_slip=.false.
-!!PS                                 ! false=no slip
+!!PS                                 ! false=no slip 
 !!PS integer                       :: mom_adv=2
                                 ! 1 vector control volumes, p1 velocities
-				! 2 scalar control volumes
-				! 3 vector invariant
+				! 2 scalar control volumes  
+				! 3 vector invariant 
 
-logical                       :: open_b=.false.   ! Reserved
+logical                       :: open_b=.false.   ! Reserved    
 
 !_______________________________________________________________________________
-!--> mixing enhancement than can be applied via subroutine mo_convect(mesh)
+!--> mixing enhancement than can be applied via subroutine mo_convect(mesh) 
 !    additionally to every mixing scheme i.e. KPP, PP, cvmix_KPP, cvmix_PP, cvmix_TKE
 
 ! Switch for Monin-Obukov TB04 mixing --> can be additionally applied for all mixing schemes
@@ -119,7 +117,7 @@ real(kind=WP)                 :: momix_lat     = -50.0_WP ! latitudinal treshhol
 real(kind=WP)                 :: momix_kv      = 0.01   ! for PP/KPP, mixing coefficient within MO length
 
 ! Switch for enhanced vertical mixing in case of instable stratification --> enhanced
-! convection
+! convection 
 logical                       :: use_instabmix = .true.
 real(kind=WP)                 :: instabmix_kv  = 0.1
 
@@ -141,7 +139,7 @@ logical                       :: use_kpp_nonlclflx = .false.
 !_______________________________________________________________________________
 ! *** active tracer cutoff
 logical          :: limit_salinity=.true.         !set an allowed range for salinity
-real(kind=WP)    :: salinity_min=5.0              !minimal salinity
+real(kind=WP)    :: salinity_min=5.0              !minimal salinity 
 real(kind=WP)    :: coeff_limit_salinity=0.0023   !m/s, coefficient to restore s to s_min
 
   namelist /tracer_cutoff/ limit_salinity, salinity_min, coeff_limit_salinity
@@ -150,21 +148,21 @@ real(kind=WP)    :: coeff_limit_salinity=0.0023   !m/s, coefficient to restore s
  real(kind=WP)                        :: time_sum=0.0 ! for runtime estimate
 
 !___________________________________________
-! Pressure Gradient Force  calculation (pgf)
-! calculation of pgf either:
+! Pressure Gradient Force  calculation (pgf) 
+! calculation of pgf either: 
 ! only linfs:
 ! > 'nemo'         ... like NEMO (interpolate to elemental depth, inter-/extrapolation)
 ! linfs, zlevel, zstar:
 ! > 'shchepetkin'  ... based on density jacobian
 ! > 'cubicspline'  ... like in FESOM1.4
 ! > 'easypgf'      ... interpolate pressure on elemental depth
-character(20)                  :: which_pgf='shchepetkin'
+character(20)                  :: which_pgf='shchepetkin' 
 
 
  NAMELIST /oce_dyn/ state_equation, C_d, A_ver, &
                     scale_area, SPP,&
-                    Fer_GM, K_GM_max, K_GM_min, K_GM_bvref, K_GM_resscalorder, K_GM_rampmax, K_GM_rampmin, &
-                    scaling_Ferreira, scaling_Rossby, scaling_resolution, scaling_FESOM14, &
+                    Fer_GM, K_GM_max, K_GM_min, K_GM_bvref, K_GM_resscalorder, K_GM_rampmax, K_GM_rampmin, & 
+                    scaling_Ferreira, scaling_Rossby, scaling_resolution, scaling_FESOM14, & 
                     Redi, visc_sh_limit, mix_scheme, Ricr, concv, which_pgf, alpha, theta, use_density_ref, &
                     K_back, c_back, uke_scaling, uke_scaling_factor, smooth_back, smooth_dis, &
                     smooth_back_tend, rosb_dis
@@ -175,14 +173,14 @@ character(20)                  :: which_pgf='shchepetkin'
             use_instabmix, instabmix_kv, &
             use_windmix, windmix_kv, windmix_nl, &
             use_kpp_nonlclflx
-
-END MODULE o_PARAM
+            
+END MODULE o_PARAM  
 !==========================================================
 MODULE o_ARRAYS
 USE o_PARAM
 IMPLICIT NONE
-! Arrays are described in subroutine array_setup
-real(kind=WP), allocatable         :: uke(:,:), v_back(:,:), uke_back(:,:), uke_dis(:,:), uke_dif(:,:)
+! Arrays are described in subroutine array_setup  
+real(kind=WP), allocatable         :: uke(:,:), v_back(:,:), uke_back(:,:), uke_dis(:,:), uke_dif(:,:) 
 real(kind=WP), allocatable         :: uke_rhs(:,:), uke_rhs_old(:,:)
 !real(kind=WP), allocatable         :: UV_ib(:,:,:) ! kh 08.03.21 additional array for asynchronous iceberg computations
 real(kind=WP), allocatable         :: UV_dis_tend(:,:,:), UV_back_tend(:,:,:), UV_total_tend(:,:,:), UV_dis_tend_node(:,:,:)
@@ -206,14 +204,14 @@ real(kind=WP), allocatable    :: Tclim(:,:), Sclim(:,:)
 real(kind=WP), allocatable    :: Tclim_ib(:,:), Sclim_ib(:,:)
 !!PS real(kind=WP), allocatable    :: Visc(:,:)
 real(kind=WP), allocatable    :: Tsurf_t(:,:), Ssurf_t(:,:)
-real(kind=WP), allocatable    :: tau_x_t(:,:), tau_y_t(:,:)
-real(kind=WP), allocatable    :: heat_flux_t(:,:), heat_rel_t(:,:), heat_rel(:)
+real(kind=WP), allocatable    :: tau_x_t(:,:), tau_y_t(:,:) 
+real(kind=WP), allocatable    :: heat_flux_t(:,:), heat_rel_t(:,:), heat_rel(:) 
 !!PS real(kind=WP), allocatable    :: coriolis(:), coriolis_node(:)
 real(kind=WP), allocatable    :: relax2clim(:)
 real(kind=WP), allocatable    :: MLD1(:), MLD2(:), MLD3(:)
 integer,       allocatable    :: MLD1_ind(:), MLD2_ind(:), MLD3_ind(:)
 real(kind=WP), allocatable    :: ssh_gp(:)
-!Tracer gradients&RHS
+!Tracer gradients&RHS      
 real(kind=WP), allocatable :: tr_xy(:,:,:)
 real(kind=WP), allocatable :: tr_z(:,:)
 
diff --git a/src/oce_setup_step.F90 b/src/oce_setup_step.F90
index 9ec0bb0d..2ccdcb52 100755
--- a/src/oce_setup_step.F90
+++ b/src/oce_setup_step.F90
@@ -101,7 +101,7 @@ subroutine ocean_setup(dynamics, tracers, partit, mesh)
     type(t_partit), intent(inout), target :: partit
     type(t_mesh)  , intent(inout), target :: mesh
     !___________________________________________________________________________
-    integer                               :: i, n
+    integer                               :: n
     
     !___setup virt_salt_flux____________________________________________________
     ! if the ale thinkness remain unchanged (like in 'linfs' case) the vitrual 
@@ -234,9 +234,7 @@ subroutine ocean_setup(dynamics, tracers, partit, mesh)
 
     if (.not.r_restart) then
        do n=1, tracers%num_tracers
-          do i=1, tracers%data(n)%AB_order-1
-             tracers%data(n)%valuesold(i,:,:)=tracers%data(n)%values
-          end do
+          tracers%data(n)%valuesAB=tracers%data(n)%values
        end do
     end if
     
@@ -263,7 +261,6 @@ subroutine ocean_setup(dynamics, tracers, partit, mesh)
         write(*,*) 'maximum allowed CDF on explicit W is set to: ', dynamics%wsplit_maxcfl
         write(*,*) '******************************************************************************'
     end if
-
 end subroutine ocean_setup
 !
 !
@@ -276,8 +273,7 @@ SUBROUTINE tracer_init(tracers, partit, mesh)
     USE DIAGNOSTICS, only: ldiag_DVD
     USE g_ic3d
     use g_forcing_param, only: use_age_tracer !---age-code
-    use g_config, only : lwiso, use_transit   ! add lwiso switch and switch for transient tracers
-    use mod_transit, only : index_transit
+    use g_config, only : lwiso   ! add lwiso switch
     IMPLICIT NONE
     type(t_tracer), intent(inout), target               :: tracers
     type(t_partit), intent(inout), target               :: partit
@@ -293,10 +289,9 @@ SUBROUTINE tracer_init(tracers, partit, mesh)
     integer        :: num_tracers
     logical        :: i_vert_diff, smooth_bh_tra
     real(kind=WP)  :: gamma0_tra, gamma1_tra, gamma2_tra
-    integer        :: AB_order
     namelist /tracer_listsize/ num_tracers
     namelist /tracer_list    / nml_tracer_list
-    namelist /tracer_general / smooth_bh_tra, gamma0_tra, gamma1_tra, gamma2_tra, i_vert_diff, AB_order
+    namelist /tracer_general / smooth_bh_tra, gamma0_tra, gamma1_tra, gamma2_tra, i_vert_diff
     !___________________________________________________________________________
     ! pointer on necessary derived types
 #include "associate_part_def.h"
@@ -374,36 +369,6 @@ SUBROUTINE tracer_init(tracers, partit, mesh)
     endif
     !---age-code-end
 
-    ! Transient tracers
-!! UNDER CONSTRUCTION - Actually we do not want to hardwire the number of transient tracers
-    if (use_transit) then
-      ! add transient tracers to the model
-      nml_tracer_list(num_tracers+1) = nml_tracer_list(1)
-      nml_tracer_list(num_tracers+2) = nml_tracer_list(1)
-      nml_tracer_list(num_tracers+3) = nml_tracer_list(1)
-      nml_tracer_list(num_tracers+4) = nml_tracer_list(1)
-      nml_tracer_list(num_tracers+1)%id = 6
-      nml_tracer_list(num_tracers+1)%id = 12
-      nml_tracer_list(num_tracers+1)%id = 14
-      nml_tracer_list(num_tracers+1)%id = 39
-
-      index_transit(1) = num_tracers+1
-      index_transit(2) = num_tracers+2
-      index_transit(3) = num_tracers+3
-      index_transit(4) = num_tracers+4
-
-      num_tracers = num_tracers + 4
-
-      ! tracers initialised from file
-      idlist((n_ic3d+1):(n_ic3d+1)) = (/14/)
-      filelist((n_ic3d+1):(n_ic3d+1)) = (/'R14C.nc'/)
-      varlist((n_ic3d+1):(n_ic3d+1))  = (/'R14C'/)
-
-      if (mype==0) write(*,*) 'XXX Transient tracers will be used in FESOM'
-    endif
-    ! 'use_transit' end
-
-
     if (mype==0) write(*,*) 'total number of tracers is: ', num_tracers
 
     !___________________________________________________________________________
@@ -417,10 +382,8 @@ SUBROUTINE tracer_init(tracers, partit, mesh)
     ! Temperature (index=1), Salinity (index=2), etc.
     allocate(tracers%data(num_tracers))
     do n=1, tracers%num_tracers
-        allocate(tracers%data(n)%values   (                             nl-1, node_size))
-        allocate(tracers%data(n)%valuesAB (                             nl-1, node_size))
-        tracers%data(n)%AB_order      = AB_order        
-        allocate(tracers%data(n)%valuesold(tracers%data(n)%AB_order-1,  nl-1, node_size))
+        allocate(tracers%data(n)%values  (nl-1,node_size))
+        allocate(tracers%data(n)%valuesAB(nl-1,node_size))
         tracers%data(n)%ID            = nml_tracer_list(n)%id
         tracers%data(n)%tra_adv_hor   = TRIM(nml_tracer_list(n)%adv_hor)
         tracers%data(n)%tra_adv_ver   = TRIM(nml_tracer_list(n)%adv_ver)
@@ -433,7 +396,6 @@ SUBROUTINE tracer_init(tracers, partit, mesh)
         tracers%data(n)%gamma2_tra    = gamma2_tra
         tracers%data(n)%values        = 0.
         tracers%data(n)%valuesAB      = 0.
-        tracers%data(n)%valuesold     = 0.
         tracers%data(n)%i_vert_diff   = i_vert_diff
     end do
     allocate(tracers%work%del_ttf(nl-1,node_size))
@@ -474,12 +436,10 @@ SUBROUTINE dynamics_init(dynamics, partit, mesh)
     logical        :: use_freeslip =.false.
     logical        :: use_wsplit   =.false.
     logical        :: ldiag_KE     =.false.
-    integer        :: AB_order     = 2
-    logical        :: check_opt_visc=.true.
     real(kind=WP)  :: wsplit_maxcfl
-    namelist /dynamics_visc   / opt_visc, check_opt_visc, visc_gamma0, visc_gamma1, visc_gamma2,  &
+    namelist /dynamics_visc   / opt_visc, visc_gamma0, visc_gamma1, visc_gamma2,  &
                                 use_ivertvisc, visc_easybsreturn
-    namelist /dynamics_general/ momadv_opt, use_freeslip, use_wsplit, wsplit_maxcfl, ldiag_KE, AB_order
+    namelist /dynamics_general/ momadv_opt, use_freeslip, use_wsplit, wsplit_maxcfl, ldiag_KE
     !___________________________________________________________________________
     ! pointer on necessary derived types
 #include "associate_part_def.h"
@@ -504,7 +464,6 @@ SUBROUTINE dynamics_init(dynamics, partit, mesh)
     !___________________________________________________________________________
     ! set parameters in derived type
     dynamics%opt_visc          = opt_visc
-    dynamics%check_opt_visc    = check_opt_visc
     dynamics%visc_gamma0       = visc_gamma0
     dynamics%visc_gamma1       = visc_gamma1
     dynamics%visc_gamma2       = visc_gamma2
@@ -515,7 +474,6 @@ SUBROUTINE dynamics_init(dynamics, partit, mesh)
     dynamics%use_wsplit        = use_wsplit
     dynamics%wsplit_maxcfl     = wsplit_maxcfl
     dynamics%ldiag_KE          = ldiag_KE
-    dynamics%AB_order          = AB_order
     !___________________________________________________________________________
     ! define local vertice & elem array size
     elem_size=myDim_elem2D+eDim_elem2D
@@ -525,7 +483,7 @@ SUBROUTINE dynamics_init(dynamics, partit, mesh)
     ! allocate/initialise horizontal velocity arrays in derived type
     allocate(dynamics%uv(        2, nl-1, elem_size))
     allocate(dynamics%uv_rhs(    2, nl-1, elem_size))
-    allocate(dynamics%uv_rhsAB(  dynamics%AB_order-1, 2, nl-1, elem_size))
+    allocate(dynamics%uv_rhsAB(  2, nl-1, elem_size))
     allocate(dynamics%uvnode(    2, nl-1, node_size))
     dynamics%uv              = 0.0_WP
     dynamics%uv_rhs          = 0.0_WP
@@ -589,8 +547,8 @@ SUBROUTINE dynamics_init(dynamics, partit, mesh)
        allocate(dynamics%ke_umean  (2, nl-1, elem_size))
        allocate(dynamics%ke_u2mean (2, nl-1, elem_size))
        allocate(dynamics%ke_du2    (2, nl-1, elem_size))
-       allocate(dynamics%ke_adv_AB (dynamics%AB_order-1, 2, nl-1, elem_size))
-       allocate(dynamics%ke_cor_AB (dynamics%AB_order-1, 2, nl-1, elem_size))
+       allocate(dynamics%ke_adv_AB (2, nl-1, elem_size))
+       allocate(dynamics%ke_cor_AB (2, nl-1, elem_size))
        allocate(dynamics%ke_rhs_bak(2, nl-1, elem_size))
        allocate(dynamics%ke_wrho   (nl-1, node_size))
        allocate(dynamics%ke_dW     (nl-1, node_size))
@@ -881,8 +839,6 @@ SUBROUTINE oce_initial_state(tracers, partit, mesh)
     USE o_ARRAYS
     USE g_config
     USE g_ic3d
-    ! for additional (transient) tracers:
-    use mod_transit, only: id_r14c, id_r39ar, id_f12, id_sf6
     implicit none
     type(t_tracer), intent(inout), target :: tracers
     type(t_partit), intent(inout), target :: partit
@@ -906,7 +862,6 @@ SUBROUTINE oce_initial_state(tracers, partit, mesh)
     ! this must be always done! First two tracers with IDs 0 and 1 are the temperature and salinity.
     if(mype==0) write(*,*) 'read Temperature climatology from:', trim(filelist(1))
     if(mype==0) write(*,*) 'read Salinity    climatology from:', trim(filelist(2))
-    if(any(idlist == 14) .and. mype==0) write(*,*) 'read radiocarbon climatology from:', trim(filelist(3))
     call do_ic3d(tracers, partit, mesh)
     
     Tclim=tracers%data(1)%values
@@ -977,44 +932,6 @@ SUBROUTINE oce_initial_state(tracers, partit, mesh)
           end if
         !---wiso-code-end
 
-! Transient tracers
-       CASE (14)        ! initialize tracer ID=14, fractionation-corrected 14C/C
-!        this initialization can be overwritten by calling do_ic3d
-!!         if (.not. any(idlist == 14)) then ! CHECK IF THIS LINE IS STILL NECESSARY
-         tracers%data(i)%values(:,:) = 0.85
-           if (mype==0) then
-              write (i_string,  "(I3)") i
-              write (id_string, "(I3)") id
-              write(*,*) 'initializing '//trim(i_string)//'th tracer with ID='//trim(id_string)
-              write (*,*) tracers%data(i)%values(1,1)
-           end if
-!!         end if
-       CASE (39)        ! initialize tracer ID=39, fractionation-corrected 39Ar/Ar
-         tracers%data(i)%values(:,:) = 0.85
-         if (mype==0) then
-            write (i_string,  "(I3)") i
-            write (id_string, "(I3)") id
-            write(*,*) 'initializing '//trim(i_string)//'th tracer with ID='//trim(id_string)
-            write (*,*) tracers%data(i)%values(1,1)
-         end if
-       CASE (12)        ! initialize tracer ID=12, CFC-12
-         tracers%data(i)%values(:,:) = 0.
-         if (mype==0) then
-            write (i_string,  "(I3)") i
-            write (id_string, "(I3)") id
-            write(*,*) 'initializing '//trim(i_string)//'th tracer with ID='//trim(id_string)
-            write (*,*) tracers%data(i)%values(1,1)
-         end if
-       CASE (6)         ! initialize tracer ID=6, SF6
-         tracers%data(i)%values(:,:) = 0.
-         if (mype==0) then
-            write (i_string,  "(I3)") i
-            write (id_string, "(I3)") id
-            write(*,*) 'initializing '//trim(i_string)//'th tracer with ID='//trim(id_string)
-            write (*,*) tracers%data(i)%values(1,1)
-         end if
-! Transient tracers end
-
         !_______________________________________________________________________            
         CASE (301) !Fram Strait 3d restored passive tracer
             tracers%data(i)%values(:,:)=0.0_WP
diff --git a/src/oce_tracer_mod.F90 b/src/oce_tracer_mod.F90
index 944be884..f374ca5d 100755
--- a/src/oce_tracer_mod.F90
+++ b/src/oce_tracer_mod.F90
@@ -24,44 +24,17 @@ SUBROUTINE init_tracers_AB(tr_num, tracers, partit, mesh)
     type(t_partit), intent(inout), target :: partit
     type(t_tracer), intent(inout), target :: tracers
     integer                               :: n,nz 
-
-!$ACC parallel loop collapse(2) default(present) !!!async(1)
-do n=1, partit%myDim_nod2D+partit%eDim_nod2D
-       do nz=1, mesh%nl-1
-       ! del_ttf will contain all advection / diffusion contributions for this tracer. Set it to 0 at the beginning!
-       tracers%work%del_ttf          (nz, n) = 0.0_WP
-       tracers%work%del_ttf_advhoriz (nz, n) = 0.0_WP
-       tracers%work%del_ttf_advvert  (nz, n) = 0.0_WP
-       end do
-end do
-!$ACC end parallel loop
 !$OMP PARALLEL DO
     do n=1, partit%myDim_nod2D+partit%eDim_nod2D
+       ! del_ttf will contain all advection / diffusion contributions for this tracer. Set it to 0 at the beginning!
+       tracers%work%del_ttf          (:, n) = 0.0_WP
+       tracers%work%del_ttf_advhoriz (:, n) = 0.0_WP
+       tracers%work%del_ttf_advvert  (:, n) = 0.0_WP
        ! AB interpolation
-       if (tracers%data(tr_num)%AB_order==2) then
-           tracers%data(tr_num)%valuesAB(:, n)  =-(0.5_WP+epsilon)*tracers%data(tr_num)%valuesold(1, :, n)+(1.5_WP+epsilon)*tracers%data(tr_num)%values(:, n)
-       elseif (tracers%data(tr_num)%AB_order==3) then
-           tracers%data(tr_num)%valuesAB(:, n)  =5.0_WP*tracers%data(tr_num)%valuesold(2, :, n)-16.0_WP*tracers%data(tr_num)%valuesold(1, :, n)+23.0_WP*tracers%data(tr_num)%values(:, n)
-           tracers%data(tr_num)%valuesAB(:, n)  =tracers%data(tr_num)%valuesAB(:, n)/12.0_WP
-       end if
+       tracers%data(tr_num)%valuesAB(:, n)  =-(0.5_WP+epsilon)*tracers%data(tr_num)%valuesAB(:, n)+(1.5_WP+epsilon)*tracers%data(tr_num)%values(:, n)
     end do
 !$OMP END PARALLEL DO
 
-    if (tracers%data(tr_num)%AB_order==2) then
-!$OMP PARALLEL DO
-       do n=1, partit%myDim_nod2d+partit%eDim_nod2D
-          tracers%data(tr_num)%valuesold(1, :, n)=tracers%data(tr_num)%values(:, n)
-       end do
-!$OMP END PARALLEL DO
-    elseif (tracers%data(tr_num)%AB_order==3) then
-!$OMP PARALLEL DO
-       do n=1, partit%myDim_nod2d+partit%eDim_nod2D
-          tracers%data(tr_num)%valuesold(2, :, n)=tracers%data(tr_num)%valuesold(1, :, n)
-          tracers%data(tr_num)%valuesold(1, :, n)=tracers%data(tr_num)%values(:, n)
-       end do
-!$OMP END PARALLEL DO
-    end if
-
     if (flag_debug .and. partit%mype==0)  print *, achar(27)//'[38m'//'             --> call tracer_gradient_elements'//achar(27)//'[0m'
     call tracer_gradient_elements(tracers%data(tr_num)%valuesAB, partit, mesh)
     call exchange_elem_begin(tr_xy, partit)
diff --git a/src/write_step_info.F90 b/src/write_step_info.F90
index cae90ae3..3ebfb595 100644
--- a/src/write_step_info.F90
+++ b/src/write_step_info.F90
@@ -268,7 +268,7 @@ subroutine check_blowup(istep, ice, dynamics, tracers, partit, mesh)
     USE MOD_PARTIT
     USE MOD_PARSUP
     USE MOD_MESH
-    use g_config, only: logfile_outfreq, which_ALE, toy_ocean, use_ice
+    use g_config, only: logfile_outfreq, which_ALE
     use o_PARAM
     use o_ARRAYS, only: water_flux, stress_surf, &
                     heat_flux, Kv, Av
@@ -344,18 +344,14 @@ subroutine check_blowup(istep, ice, dynamics, tracers, partit, mesh)
           write(*,*)
           write(*,*) 'wflux = ',water_flux(n)
           write(*,*)
-          if (.not. toy_ocean) then
-            write(*,*) 'u_wind = ',u_wind(n),', v_wind = ',v_wind(n)
-            write(*,*)
-            do nz=1,nod_in_elem2D_num(n)
-                    write(*,*) 'stress_surf(1:2,',nz,') = ',stress_surf(:,nod_in_elem2D(nz,n))
-            end do
-          end if
-          if (use_ice) then
+          write(*,*) 'u_wind = ',u_wind(n),', v_wind = ',v_wind(n)
+          write(*,*)
+          do nz=1,nod_in_elem2D_num(n)
+                write(*,*) 'stress_surf(1:2,',nz,') = ',stress_surf(:,nod_in_elem2D(nz,n))
+          end do
           write(*,*)
           write(*,*) 'm_ice = ',m_ice(n),', m_ice_old = ',m_ice_old(n)
           write(*,*) 'a_ice = ',a_ice(n),', a_ice_old = ',a_ice_old(n)
-          end if 
           write(*,*)
           write(*,*) 'Wvel(:, n)  = ',Wvel(ulevels_nod2D(n):nlevels_nod2D(n),n)
           write(*,*)
@@ -421,7 +417,7 @@ subroutine check_blowup(istep, ice, dynamics, tracers, partit, mesh)
        end if ! --> if ( .not. trim(which_ALE)=='linfs' .and. ...
           
        
-       do nz=ulevels_nod2D(n),nlevels_nod2D(n)-1
+       do nz=1,nlevels_nod2D(n)-1
           !_______________________________________________________________
           ! check temp
           if ( (tracers%data(1)%values(nz, n) /= tracers%data(1)%values(nz, n)) .or. &
@@ -452,13 +448,11 @@ subroutine check_blowup(istep, ice, dynamics, tracers, partit, mesh)
              write(*,*) 'ssh_rhs     = ',ssh_rhs(n)
              write(*,*) 'ssh_rhs_old = ',ssh_rhs_old(n)
              write(*,*)
-             if (use_ice) then
-                write(*,*) 'm_ice    = ',m_ice(n)
-                write(*,*) 'm_ice_old   = ',m_ice_old(n)
-                write(*,*) 'm_snow      = ',m_snow(n)
-                write(*,*) 'm_snow_old  = ',m_snow_old(n)
-                write(*,*)
-             end if 
+             write(*,*) 'm_ice    = ',m_ice(n)
+             write(*,*) 'm_ice_old   = ',m_ice_old(n)
+             write(*,*) 'm_snow      = ',m_snow(n)
+             write(*,*) 'm_snow_old  = ',m_snow_old(n)
+             write(*,*)
              write(*,*) 'hnode    = ',hnode(:,n)
              write(*,*) 'hnode_new   = ',hnode_new(:,n)
              write(*,*)
@@ -475,11 +469,11 @@ subroutine check_blowup(istep, ice, dynamics, tracers, partit, mesh)
           !_______________________________________________________________
           ! check salt
           if ( (tracers%data(2)%values(nz, n) /= tracers%data(2)%values(nz, n)) .or.  &
-             tracers%data(2)%values(nz, n) <3.0_WP .or. tracers%data(2)%values(nz, n) >45.0_WP ) then
+             tracers%data(2)%values(nz, n) < 0 .or. tracers%data(2)%values(nz, n)>50 ) then
 !$OMP CRITICAL
              found_blowup_loc=1
              write(*,*) '___CHECK FOR BLOW UP___________ --> mstep=',istep
-             write(*,*) ' --STOP--> found salinity becomes NaN or <=3.0, >=45.0'
+             write(*,*) ' --STOP--> found salinity becomes NaN or <0, >50'
              write(*,*) 'mype     = ',mype
              write(*,*) 'mstep    = ',istep
              write(*,*) 'node     = ',n
